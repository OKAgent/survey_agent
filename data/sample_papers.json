{"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems": {"authors": ["Jialiang Xu", "Mengyu Zhou", "Xinyi He", "Shi Han", "Dongmei Zhang"], "title": "Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems", "url": "https://arxiv.org/pdf/2211.07455.pdf", "abstract": "Numerical Question Answering is the task of answering questions that require numerical capabilities. Previous works introduce general adversarial attacks to Numerical Question Answering, while not systematically exploring numerical capabilities specific to the topic. In this paper, we propose to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets. A series of numerical capabilities are highlighted, and corresponding dataset perturbations are designed. Empirical results indicate that existing systems are severely challenged by these perturbations. E.g., Graph2Tree experienced a 53.83% absolute accuracy drop against the ``Extra'' perturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the ``Language'' perturbation on the numerical subset of DROP. As a counteracting approach, we also investigate the effectiveness of applying perturbations as data augmentation to relieve systems' lack of robust numerical capabilities. With experiment analysis and empirical studies, it is demonstrated that Numerical Question Answering with robust numerical capabilities is still to a large extent an open question. We discuss future directions of Numerical Question Answering and summarize guidelines on future dataset collection and system design.", "arxiv_id": "2211.07455", "published_date": "2022-11-14", "year": 2022, "introduction": "Introduction Numeracy is an essential part for real-world NLP applications (Sundaram et al., 2022; Thawani et al., 2021b; Sundararaman et al., 2020; Spithourakis and Riedel, 2018). Numerical QA (Question Answering) is one representative group of such number-dependent NLP tasks. E.g., Math Word Problem Solving (Zhang et al., 2020a; Miao et al., \u2217 The contributions by Jialiang Xu and Xinyi He have been conducted and completed during their internships at Microsoft Research Asia, Beijing, China. \u2020 Corresponding author. 2020; Koncel-Kedziorski et al., 2016), Discrete Reasoning (Dua et al., 2019; Hu et al., 2019; AlNegheimish et al., 2021a), Tabular Question Answering (Zhong et al., 2017; Chen et al., 2020b; Zhu et al., 2021; Chen et al., 2021). These Numerical QA tasks require NLP systems to arrive at a numerical answer from the numbers in the question and context. By studying how existing NLP systems perform in these Numerical QA tasks, we could take a glimpse at what capabilities are required for building NLP systems in the future. In an ad-hoc manner, a line of work revealed robustness issues of handling Numerical QA in existing NLP systems. Through adversarial attacks with designed dataset perturbations, numberrelated limitations were exposed: E.g., utilizing spurious correlation in datasets (Patel et al., 2021; Kumar et al., 2021; Al-Negheimish et al., 2021b; Pi et al., 2022b), incorrectly representing numbers (Nogueira et al., 2021; Kim et al., 2021) and failing to extrapolate (Kim et al., 2021; Pal and Baral, 2021). This line of work inspires us to ask following questions: 1) What is the overall landscape of robustness issues of numerical capabilities in existing NLP systems? Can we \ufb01nd a more systematic way to investigate the number-related limitations? 2) How to diagnose each numerical capability and evaluate the severity of it not being captured in a system? Can we further develop new adversarial perturbation methods on Numerical QA for diagnosis and evaluation? 3) How to address the numerical robustness issues? How do existing solutions work and what are possible future directions? To answer the above questions, in this paper we propose the DNC (Diagnosing Numerical Capbilities) framework1 as shown in Figure 1. Most existing Numerical QA systems (see \u00a72.1) take a two-stage approach to extract and manipulate numbers. As shown in the QA Stages part of 1Our code and datasets used are available in the link: https://github.com/microsoft/NumberDiagnosis arXiv:2211.07455v1  [cs.CL]  14 Nov 2022 Systems T5 BART TagOps GPT2 Tokenization Graph2Tree GPT2pre Replacement Compare Math Word  Problem Discrete  Reasoning Tabular  Understanding ASDiv-a DROP-num TATQA-a Datasets Stages Capabilities Numerical Parsing Semantic Parsing Perturbs Language Type Noise Distribution Verbosity Extra Logic Order Noise Test Noise Train Noise Val Apply Split Dataset Compare Attack Setting Diagnose Defense Setting QA Stages E.g., A room is 12 feet long, 8 feet wide and 14 feet high. How much carpet does one need to cover the whole floor? Num1 (12) Num2 (8) Num3 (14) Stage 1: recognizing numbers from context & question Num1 (12) Num2 (8) \u00d7 96 Stage 2: translating problem to logical form & executing DNC Framework Number  Detection Number Value  Understanding Operand  Selection Operation Reasoning Noise Test Train Val To evaluate the relief by training on perturbed data. To reveal systems\u2019 lack of numerical capabilities. Figure 1: Overview of DNC Framework. The process of Numerical QA solving is divided into two logical stages. Four capabilities are required to complete the stages, each maps to two perturbations. Perturbations can be applied to appropriate train / validation / test splits of Numerical QA datasets under Attack or Defense Setting. Models of the NLP systems are trained and then evaluated on the perturbed datasets as a diagnosis of their numerical capabilities. Figure 1, systems usually \ufb01rst recognize numbers in the context and question and treat them as candidate operands. Then, with the understanding of the question semantics, they select corresponding operands, and explicitly generate logical forms or implicitly execute operations to get the \ufb01nal result. The above two stages correspond to the two groups of numerical capabilities (see \u00a74.1) covered by our DNC Framework (as shown in Figure 1). In Stage 1, we focus on a system\u2019s capabilities to recognize different forms of numbers (\u201cNumber Detection\u201d), and to parse and represent number values correctly (\u201cNumber Value Understanding\u201d). In Stage 2, we focus on the capabilities to correctly choose operands (\u201cOperand Selection\u201d) and operations (\u201cOperation Reasoning\u201d) by understanding context and question. For each of these four capabilities, two perturbations (see \u00a74.2) are proposed by us to diagnose the capability. Each perturbation is designed to be trivial to humans and thus cannot easily fool humans, but it could bring down existing NLP systems (under the \u201cAttack\u201d setting), and therefore expose the robustness issue of lacking its corresponding capability. By applying the above diagnosis to various NLP Systems and Numerical QA Datasets (as shown in Figure 1), in \u00a75 we \ufb01nd that existing systems experience signi\ufb01cant performance drops, which veri\ufb01es their lack of robust numerical capabilities. E.g., Graph2Tree experienced a 53.83% absolute accuracy drop against the \u201cExtra\u201d perturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the \u201cLanguage\u201d perturbation on the numerical subset of DROP. From another point of view, the perturbations are also applicable for data augmentation. Under the \u201cDefense\u201d setting (see \u00a74.3), perturbations are applied to all splits of the dataset. A system\u2019s performance of the same perturbation under both \u201cAttack\u201d and \u201cDefense\u201d settings are compared (in \u00a75.2) to show if the corresponding robustness issue could be relieved by augmenting the training data. Empirical results indicate that despite the recovery in most cases, the performance still fall lower than the original level. Finally, based on the \u201cAttack\u201d and \u201cDefense\u201d results in \u00a75 and additional experiments, in \u00a76 we compare some existing design choices in Numerical QA, such as: Is it better to generate logical forms (and then execute the program/expression) or predict answers directly in an end-to-end way? Shall we break numbers into subword tokens or ", "conclusion": "Conclusion In this paper we aim at diagnosing numerical capabilities in existing NLP systems. We list out a series of numerical capabilities and design corresponding dataset perturbations. Empirical results show that existing systems still lack numerical capabilities to a large extent, and this lack cannot be eliminated in a trivial manner. Analysis into the empirical results, discussion of the the existing practices, and insights for future directions of Numerical QA dataset collection and system design are also provided. Limitations Our pipeline has limitations in the following two aspects that we plan to address in the future: Dependency on ground truth equation. Currently, three of the eight DNC perturbations have strong dependency on the ground truth solving equation, which is missing in datasets such as DROP. We hope to utilize semi-supervised approaches in the future to enlarge the coverage of the DNC perturbations. Perturbing scalability. Currently our \ufb01lters cover only a portion of the whole dataset due to DNC \ufb01ltering and perturbing questions based on manual rules and templates. we hope to develop more automatic \ufb01ltering and perturbing in the future. Also, DNC can only apply perturbations to numbers provided by the problem, which limits its diagnosing power in questions where an unspeci\ufb01ed number is used, e.g., when numerical commonsense knowledge is involved. Ethical Statements The model implementation and datasets utilized in this paper are based on publication and open-source repositories. Licenses protocols are followed in the process of our experiments. No new datasets or NLP applications are presented in this paper and no violation of privacy or usage of demographic information was involved in our process of interacting with the datasets. Our experiments do not involve lots of compute time/power as reported in the paper. ", "full_text": "Towards Robust Numerical Question Answering:\nDiagnosing Numerical Capabilities of NLP Systems\nJialiang Xu1\u2217, Mengyu Zhou2\u2020, Xinyi He3*, Shi Han2, Dongmei Zhang2\n1 University of Illinois at Urbana-Champaign\n2 Microsoft Research 3 Xi\u2019an Jiaotong University\njx17@illinois.edu, hxyhxy@stu.xjtu.edu.cn,\n{mezho, shihan, dongmeiz}@microsoft.com\nAbstract\nNumerical Question Answering is the task of\nanswering questions that require numerical ca-\npabilities. Previous works introduce general\nadversarial attacks to Numerical Question An-\nswering, while not systematically exploring\nnumerical capabilities speci\ufb01c to the topic. In\nthis paper, we propose to conduct numerical\ncapability diagnosis on a series of Numeri-\ncal Question Answering systems and datasets.\nA series of numerical capabilities are high-\nlighted, and corresponding dataset perturba-\ntions are designed. Empirical results indicate\nthat existing systems are severely challenged\nby these perturbations. E.g., Graph2Tree ex-\nperienced a 53.83% absolute accuracy drop\nagainst the \u201cExtra\u201d perturbation on ASDiv-a,\nand BART experienced 13.80% accuracy drop\nagainst the \u201cLanguage\u201d perturbation on the nu-\nmerical subset of DROP. As a counteracting\napproach, we also investigate the effectiveness\nof applying perturbations as data augmenta-\ntion to relieve systems\u2019 lack of robust numeri-\ncal capabilities. With experiment analysis and\nempirical studies, it is demonstrated that Nu-\nmerical Question Answering with robust nu-\nmerical capabilities is still to a large extent an\nopen question. We discuss future directions\nof Numerical Question Answering and sum-\nmarize guidelines on future dataset collection\nand system design.\n1\nIntroduction\nNumeracy is an essential part for real-world NLP\napplications (Sundaram et al., 2022; Thawani et al.,\n2021b; Sundararaman et al., 2020; Spithourakis\nand Riedel, 2018).\nNumerical QA (Question\nAnswering) is one representative group of such\nnumber-dependent NLP tasks. E.g., Math Word\nProblem Solving (Zhang et al., 2020a; Miao et al.,\n\u2217 The contributions by Jialiang Xu and Xinyi He have\nbeen conducted and completed during their internships at\nMicrosoft Research Asia, Beijing, China.\n\u2020 Corresponding author.\n2020; Koncel-Kedziorski et al., 2016), Discrete\nReasoning (Dua et al., 2019; Hu et al., 2019; Al-\nNegheimish et al., 2021a), Tabular Question An-\nswering (Zhong et al., 2017; Chen et al., 2020b;\nZhu et al., 2021; Chen et al., 2021). These Numer-\nical QA tasks require NLP systems to arrive at a\nnumerical answer from the numbers in the ques-\ntion and context. By studying how existing NLP\nsystems perform in these Numerical QA tasks, we\ncould take a glimpse at what capabilities are re-\nquired for building NLP systems in the future.\nIn an ad-hoc manner, a line of work revealed\nrobustness issues of handling Numerical QA in\nexisting NLP systems.\nThrough adversarial at-\ntacks with designed dataset perturbations, number-\nrelated limitations were exposed: E.g., utilizing\nspurious correlation in datasets (Patel et al., 2021;\nKumar et al., 2021; Al-Negheimish et al., 2021b;\nPi et al., 2022b), incorrectly representing numbers\n(Nogueira et al., 2021; Kim et al., 2021) and fail-\ning to extrapolate (Kim et al., 2021; Pal and Baral,\n2021). This line of work inspires us to ask follow-\ning questions: 1) What is the overall landscape of\nrobustness issues of numerical capabilities in exist-\ning NLP systems? Can we \ufb01nd a more systematic\nway to investigate the number-related limitations?\n2) How to diagnose each numerical capability and\nevaluate the severity of it not being captured in a\nsystem? Can we further develop new adversarial\nperturbation methods on Numerical QA for diag-\nnosis and evaluation? 3) How to address the numer-\nical robustness issues? How do existing solutions\nwork and what are possible future directions?\nTo answer the above questions, in this pa-\nper we propose the DNC (Diagnosing Numerical\nCapbilities) framework1 as shown in Figure 1.\nMost existing Numerical QA systems (see \u00a72.1)\ntake a two-stage approach to extract and manipu-\nlate numbers. As shown in the QA Stages part of\n1Our code and datasets used are available in the link:\nhttps://github.com/microsoft/NumberDiagnosis\narXiv:2211.07455v1  [cs.CL]  14 Nov 2022\n Systems\nT5\nBART\nTagOps\nGPT2\nTokenization\nGraph2Tree\nGPT2pre\nReplacement\nCompare\nMath Word \nProblem\nDiscrete \nReasoning\nTabular \nUnderstanding\nASDiv-a\nDROP-num\nTATQA-a\nDatasets\nStages\nCapabilities\nNumerical\nParsing\nSemantic\nParsing\nPerturbs\nLanguage\nType\nNoise\nDistribution\nVerbosity\nExtra\nLogic\nOrder\nNoise\nTest\nNoise\nTrain\nNoise\nVal\nApply\nSplit Dataset\nCompare\nAttack Setting\nDiagnose\nDefense Setting\nQA Stages\nE.g., A room is 12 feet long, 8 feet wide and 14 feet high. How much carpet does one need to cover the whole floor?\nNum1 (12)\nNum2 (8)\nNum3 (14)\nStage 1: recognizing numbers from context & question\nNum1 (12)\nNum2 (8)\n\u00d7\n96\nStage 2: translating problem to logical form & executing\nDNC Framework\nNumber \nDetection\nNumber Value \nUnderstanding\nOperand \nSelection\nOperation\nReasoning\nNoise\nTest\nTrain\nVal\nTo evaluate the relief by training on perturbed data.\nTo reveal systems\u2019 lack of numerical capabilities.\nFigure 1: Overview of DNC Framework. The process of Numerical QA solving is divided into two logical stages.\nFour capabilities are required to complete the stages, each maps to two perturbations. Perturbations can be applied\nto appropriate train / validation / test splits of Numerical QA datasets under Attack or Defense Setting. Models\nof the NLP systems are trained and then evaluated on the perturbed datasets as a diagnosis of their numerical\ncapabilities.\nFigure 1, systems usually \ufb01rst recognize numbers\nin the context and question and treat them as can-\ndidate operands. Then, with the understanding of\nthe question semantics, they select corresponding\noperands, and explicitly generate logical forms or\nimplicitly execute operations to get the \ufb01nal result.\nThe above two stages correspond to the two\ngroups of numerical capabilities (see \u00a74.1) covered\nby our DNC Framework (as shown in Figure 1). In\nStage 1, we focus on a system\u2019s capabilities to\nrecognize different forms of numbers (\u201cNumber\nDetection\u201d), and to parse and represent number val-\nues correctly (\u201cNumber Value Understanding\u201d). In\nStage 2, we focus on the capabilities to correctly\nchoose operands (\u201cOperand Selection\u201d) and oper-\nations (\u201cOperation Reasoning\u201d) by understanding\ncontext and question. For each of these four capa-\nbilities, two perturbations (see \u00a74.2) are proposed\nby us to diagnose the capability. Each perturbation\nis designed to be trivial to humans and thus cannot\neasily fool humans, but it could bring down exist-\ning NLP systems (under the \u201cAttack\u201d setting), and\ntherefore expose the robustness issue of lacking its\ncorresponding capability.\nBy applying the above diagnosis to various NLP\nSystems and Numerical QA Datasets (as shown\nin Figure 1), in \u00a75 we \ufb01nd that existing systems\nexperience signi\ufb01cant performance drops, which\nveri\ufb01es their lack of robust numerical capabilities.\nE.g., Graph2Tree experienced a 53.83% absolute\naccuracy drop against the \u201cExtra\u201d perturbation on\nASDiv-a, and BART experienced 13.80% accuracy\ndrop against the \u201cLanguage\u201d perturbation on the\nnumerical subset of DROP.\nFrom another point of view, the perturbations are\nalso applicable for data augmentation. Under the\n\u201cDefense\u201d setting (see \u00a74.3), perturbations are ap-\nplied to all splits of the dataset. A system\u2019s perfor-\nmance of the same perturbation under both \u201cAttack\u201d\nand \u201cDefense\u201d settings are compared (in \u00a75.2) to\nshow if the corresponding robustness issue could\nbe relieved by augmenting the training data. Em-\npirical results indicate that despite the recovery in\nmost cases, the performance still fall lower than the\noriginal level.\nFinally, based on the \u201cAttack\u201d and \u201cDefense\u201d re-\nsults in \u00a75 and additional experiments, in \u00a76 we\ncompare some existing design choices in Numer-\nical QA, such as: Is it better to generate logical\nforms (and then execute the program/expression)\nor predict answers directly in an end-to-end way?\nShall we break numbers into subword tokens or\n substitute them with a placeholder that can be later\nre-substituted? We also discuss the open questions\nand future directions on the robust numerical capa-\nbilities of NLP systems, including recent relevant\ndevelopment such as neural program execution and\nnumerical data synthesizing.\nIn summary, our major contributions are:\n\u2022 The DNC framework is proposed by us to sys-\ntematically diagnose the robustness of NLP\nsystems on numerical capabilities. A series\nof number-related perturbation methods are\ndesigned for the capabilities.\n\u2022 Comprehensive diagnosing experiments on\nadversarial attacks and data augmentations are\nconducted by us on \ufb01ve systems over three\nNumerical QA tasks. We show the overall\npicture of numerical robustness issues of the\nsystems, and the partial effectiveness of our\nsimple defense mechanism.\n\u2022 Based on experiments and previous work, we\nprovide guidelines for existing numerically-\nrobust NLP system designs and discussions\nfor future directions on robust Numerical QA.\n2\nRelated Work\n2.1\nNumerical Question Answering\nPrevious work has proposed Numerical QA\ndatasets and systems. In this paper we consider\nas examples the domains of Math Word Problem,\nDiscrete Reasoning and Tabular QA.\nMath Word Problem (Kushman et al., 2014;\nUpadhyay and Chang, 2017; Miao et al., 2020;\nQin et al., 2020; Lan et al., 2022) concerns arith-\nmetic questions collected from lower-grade el-\nementary school coursework.\nNeural network\nare employed with different architectures such as\nSeq2Seq (Wang et al., 2017; Chiang and Chen,\n2019), Seq2Tree (Xie and Sun, 2019; Liang et al.,\n2021) and Graph2Tree (Zhang et al., 2020b; Shen\nand Jin, 2020). Recently, large end-to-end pre-\ntrained language models (Chowdhery et al., 2022;\nPi et al., 2022a) have also been showing impressive\nresults in Math Word Problem.\nDiscrete Reasoning (Dua et al., 2019; Al-\nNegheimish et al., 2021a; Hu et al., 2019) concerns\nquestions requiring logistic and arithmetic opera-\ntions on real-world paragraphs. Discrete Reason-\ning Systems are mainly based on Graph Attention\nNetworks (Chen et al., 2020a) or the Transformer\narchitecture (Ran et al., 2019).\nTabular QA and Semantic Parsing (Zhu et al.,\n2021; Chen et al., 2021; Zhong et al., 2017; Pasu-\npat and Liang, 2015) concerns question answering\nin the domain of tabular data, which often involves\na large amount of numbers and requires arithmetic\naggregations to arrive at the \ufb01nal answer. Tabular\nQA systems (Dong et al., 2022; Liu et al., 2022;\nIida et al., 2021; Herzig et al., 2020; Yin et al.,\n2020) are mainly based on Pretrained Language\nModels with Transformer backbones. Tabular QA\nsystems mainly aim at converting natural language\nutterance into executable expressions such as com-\nmands in SQL language.\n2.2\nNumeracy Limitations in NLP Systems\nEfforts have been dedicated to reveal numeracy\nlimitations in NLP systems. (Patel et al., 2021;\nKumar et al., 2021; Al-Negheimish et al., 2021b;\nPi et al., 2022b; Nogueira et al., 2021; Kim et al.,\n2021; Pal and Baral, 2021). However, previous\nwork mainly focused on borrowing adversarial at-\ntack methods from general QA such as re-ordering\nsentences (Patel et al., 2021; Al-Negheimish et al.,\n2021b; Kumar et al., 2021), substituting synonyms\n(Kumar et al., 2021; Pi et al., 2022b), or adding\nirrelevant information (Patel et al., 2021; Pi et al.,\n2022b), while having limited exploration into ca-\npabilities speci\ufb01c to Numerical QA problems such\nas understanding different number values, recog-\nnizing different number surface forms or selecting\nrelated numbers.\n3\nPreliminaries\nA Numerical Question Answering problem is de-\n\ufb01ned to consist of a problem prompt (question) P\nand a problem body (context) B. Depending on\nthe task type, the problem body takes the form of\neither a paragraph or a mixture of free-form text\nparagraphs and structured data such as tables. Let\nV be the vocabulary of the textual words, Q be\nthe set of the numerical values in P \u222a B, and Q+\nbe the numerical values that can be arithmetically\ncomputed with Q, then the problem prompt and\nbody can be formulated as P = \ufffd\ni pi, pi \u2208 V \u222aQ\nand B =\n\ufffd\ufffd\n\ufffdj bj\ni,j \u03c4i,j \u2295 \ufffd\nk bk\n, \u03c4i,j, bk \u2208 V \u222aQ.\nHere \u2295 denotes the concatenation operation, p\u00b7 and\nb\u00b7 are prompt and body textual words, and \u03c4\u00b7,\u00b7 are\nthe body tabular cells.\n The target output T of the problem is either\na numerical value Tans that is an element in Q+\nor a mathematical expression Teq that consists of\nelements in the concerned numerical values Q\nand the simple operators O = {+, \u2212, \u00d7, \u00f7}. I.e.\nT =\n\ufffd\nTans : q \u2208 Q+\nTeq : \ufffd\ni ti, ti \u2208 Q \u222a O\n. With P and B\nas input and T as output, a trained Numerical QA\nsystem can be regarded as a mapping f such that\nf : (P, B) \u2192 T\n(1)\nNote that this expression not only describes the\nNumerical QA tasks, but also generalizes to other\nnumeracy-related NLP tasks such as Tabular En-\ntailment (Chen et al., 2020b) and Timeseries-based\nFraudulent Detection (Padhi et al., 2021).\nIn this paper, we design and apply perturba-\ntions to the samples in the dataset to form per-\nturbed prompt P\u22c6, perturbed body B\u22c6 and per-\nturbed ground truth target T \u22c6. We show that ex-\nisting systems are fragile against numerical pertu-\nbation by showing that on a large portion of the\ndataset, the previous mapping fails to generate cor-\nrect perturbed target, i.e.:\nf : (P\u22c6, B\u22c6) \u0338\u2192 T \u22c6\n(2)\n4\nDNC Framework\nOur approach aims at diagnosing the numerical\nweakness of existing Numerical Question Answer-\ning models. We list out and explain a series of\nnumerical capabilities that are critical to solving\nNumerical Question Answering problems in \u00a74.1.\nWe then design numerical perturbations targeting\nthese capabilities in \u00a74.2. With the designed per-\nturbations, we examine the weaknesses under two\ndifferent perturbations settings in \u00a74.3.\nThese three sections are represented in Figure 1.\nas the \u201cCapabilities\u201d stripe, the \u201cPerturbs\u201d stripe,\nand the \u201cAttack Setting\u201d and \u201c Defense Setting\u201d.\n4.1\nNumerical Capabilities\nWe classify numerical capabilities into three major\ncategories, concerning different aspects of numeri-\ncal understanding, as below:\nNumber Detection is the capability of recog-\nnizing numbers of different surface forms. For\ninstance, the English word \"Forty-two\" and the\nArabic number \"42.0\" are regarded the same num-\nber in Numerical QA and should not affect the \ufb01nal\narithmetic answer of a question.\nNumber Value Understanding is the capabil-\nity of understanding numbers of different value\ndistributions. Systems are expected to not only\napply arithmetic calculation on a speci\ufb01c set of\nnumbers (e.g., integers of values smaller than 500\nas included in the BERT tokenizer vocabulary).\nRobust Numerical QA systems are also expected\nto handle values such as \ufb02oat-point numbers and\nnumbers larger than 500.\nOperand Selection is the capability of deciding\nwhich numbers to select as the operands in the arith-\nmetic process. One important aspect of selecting\nrelated values is to exclude numbers that are 1) ir-\nrelevant to the Numerical QA problem scenario, or\n2) relevant to the problem scenario but not essential\nto the question solving. Systems are expected to\nselect as operands the important values from the\nunimportant values.\nOperation Reasoning is the capability of infer-\nring operations from the logic pattern described in\nthe text. In an arithmetic process, the operation is\nindependent from the operands, therefore different\noperations can be applied to the same set of selected\nrelated numbers in different questions. Systems are\nexpected to decouple operation from operands and\nselect the operation in an operand-agnostic way.\n4.2\nPerturbations\nPerturbations are designed according to each nu-\nmerical capabilities. In Table 1, an example prob-\nlem is provided for each of the perturbations. The\nformal de\ufb01nition of the perturbations is provided\nin Appendix A.\nLanguage Perturbation targets the Number\nDetection capability and diagnoses how accurate\ncan systems detect numbers in different surface\nforms.\nTo perturb a number string ns, we re-\nplace it with its English form of the number with\nNum2Words.2 This perturbation changes number\nsurface forms but not their values.\nType Perturbation targets the Number Detec-\ntion capability and challenges systems to detect\nnumbers in their \ufb02oat-point forms. To perturb a\nnumber string ns, we concatenate it with the string\n\u201c.0\u201d. Similar to Language Perturbation, only the\nnumber detection capability is diagnosed with this\nperturbation. Contrary to the Noise perturbation in\nthe next paragraph, the Type perturbation does not\npropose additional calculation dif\ufb01culty by chang-\ning number values.\n2https://github.com/savoirfairelinux/num2words\n Capability\nPerturbation\nExample Problem Pair\nT5 Prediction\nNumber\nDetection\nLanguage\nOriginal: A mailman has to give out 192 pieces of junk mail. If he goes to 4 blocks, how\nmany pieces of junk mail should he give each block?\nPerturbed: A mailman has to give out one hundred and ninety-two pieces of junk mail. If\nhe goes to four blocks, how many pieces of junk mail should he give each block?\nOriginal: 192 / 4 \u2713\nPerturbed: 92 / 4 \u00d7\nExpected: 192 / 4\nType\nOriginal: There were 105 parents in the program and 698 pupils, too. How many people\nwere present in the program?\nPerturbed: There were 105.0 parents in the program and 698.0 pupils, too. How many\npeople were present in the program?\nOriginal: 105 + 698 \u2713\nPerturbed: 105 + 688 \u00d7\nExpected: 105 + 698\nNumber Value\nUnderstanding\nNoise\nOriginal: Tony had $20. He paid $8 for a ticket to a baseball game. At the game, he\nbought a hot dog for $3. What amount of money did Tony have then?\nPerturbed: Tony had $20.2. He paid $8.5 for a ticket to a baseball game. At the game, he\nbought a hot dog for $3.5. What amount of money did Tony have then?\nOriginal: 20 - 8 - 3 \u2713\nPerturbed: 208.52 - 3.5 \u00d7\nExpected: 20.2 - 8.5 - 3.5\nDistribution\nOriginal: Frank had $16. After buying some new toys he had $8 left.How much did he\nspend on toys?\nPerturbed: Frank had $1281. After buying some new toys he had $478 left.How much did\nhe spend on toys?\nOriginal: 16 - 8 \u2713\nPerturbed: 1215 - 878 \u00d7\nExpected: 1281 - 478\nOperand\nSelection\nExtra\nOriginal: John has twelve shirts. Later he bought four more shirts. How many shirts does\nJohn have in total?\nPerturbed: John has twelve shirts. Later he bought four more shirts. Frank had $16. How\nmany shirts does John have in total?\nOriginal: 12 + 4 \u2713\nPerturbed: 16 + 12 \u00d7\nExpected: 12 + 4\nVerbosity\nOriginal: The roller coaster at the state fair costs 6 tickets per ride. If 8 friends were going\nto ride the roller coaster, how many tickets would they need?\nPerturbed: The roller coaster at the state fair costs 6 (not 30) tickets per ride. If 8 (not 119)\nfriends were going to ride the roller coaster, how many tickets would they need?\nOriginal: 6 * 8 \u2713\nPerturbed: 8 * 119 \u00d7\nExpected: 6 * 8\nOperation\nReasoning\nLogic\nOriginal: Jack received 8 emails in the morning and 2 emails in the afternoon. How many\nemails did Jack receive in the day?\nPerturbed: Jack received 8 emails in the morning and 2 emails in the afternoon. How many\nmore emails did Jack receive in the morning than in the afternoon?\nOriginal: 8 + 2 \u2713\nPerturbed: 8 + 2 \u00d7\nExpected: 8 - 2\nOrder\nOriginal: A DVD book holds 126 DVDs. There are 81 DVDs already in the book. How\nmany more DVDs can be put in the book?\nPerturbed: There are 81 DVDs already in the book. A DVD book holds 126 DVDs. How\nmany more DVDs can be put in the book?\nOriginal: 126 - 81 \u2713\nPerturbed: 81 - 126 \u00d7\nExpected: 126 - 81\nTable 1: Examples of DNC Perturbations and Corresponding Predictions by T5. For each perturbation an example\noriginal and perturbed problem pair is shown. The rightmost column shows some error cases where T5 generates\ncorrect equation on the original problem but fails on the perturbed. The ground truth equation of the perturbed\nproblem is also provided after \u201cExpected\u201d.\nNoise Perturbation targets the Number Value\nUnderstanding capability and challenges systems\nto not only understand arithmetic operations of\nnot only integers but also \ufb02oat-point numbers. To\nperturb a number n, we randomly attach a one-\ndigit fractional part with uniform distribution. This\nperturbation introduces new \ufb02oat-point numbers\nand breaks the original number value distribution\nin the dataset by adding an random variable.\nDistribution Perturbation targets the Number\nValue Understanding capability and challenges sys-\ntems to conduct arithmetic with larger integers. To\nperturb a number n, we randomly offset the value\nwith a normal distribution. Based on the observa-\ntions in Wallace et al. (2019), we choose to perturb\nthe majority of the numbers to larger than 500. This\nperturbation introduces large numbers and breaks\noriginal number value distribution in the dataset.\nVerbosity Perturbation targets the Operand Se-\nlection capability and challenges systems to select\nthe correct quantity in the problem by adding ex-\nplicitly irrelevant numbers into the problem. To\nperturb a number string ns, we concatenate it with\nan irrelevant number in parentheses, the irrelevant\nnumber is preceded by \u201cnot\u201d. This perturbation in-\ntroduces numbers without breaking the distribution\nof relevant numbers in the dataset.\nExtra Perturbation targets the Operand Selec-\ntion capability and challenges systems to exclude ir-\nrelevant numbers. To perturb a problem (B, P), An\nirrelvant sentence containing numbers randomly\nsampled from the corpus is added to the body B.\nThis perturbation breaks the number distribution\nby introducing extra instances of different numbers\nfor the same problem.\nLogic Perturbation targets the Operation Rea-\nsoning capability and challenges systems to choose\ncorrect operations for the same set of numbers.\nIn this paper, for two datasets described in \u00a75.1,\nTATQA and ASDiv-a, the Operation perturbation\ndemands additional attention. On TATQA it is\n based on template matching via SpaCy3 and auto-\nmatic conversions, while on ASDiv-a it is based\non manual annotation due to the diversity of pat-\nterns in the ASDiv-a dataset. This perturbation\nintroduces extra problems of different operations.\nOrder Perturbation targets the Operation Rea-\nsoning capability and challenges systems to choose\ncorrect operations for the same set of numbers. On\nASDiv-a, the order of sentences in the problem\nbody is manually altered in a manner that changes\nthe order of number occurrence but not the prob-\nlem logic. This perturbation does not break the\noperation distribution within the dataset.\n4.3\nPerturbing Settings\nWith the aforementioned perturbations, we con-\nstruct perturbed datasets under different settings\nto investigate systems\u2019 numerical capabilities and\nthe effectiveness of the perturbations from different\nperspectives. For a speci\ufb01c dataset with a training\n/ validation / testing split, different splits are per-\nturbed under different settings. In this paper we\nconsider the following two settings of Attack and\nDefense, as compared in Table 2:\nAttack. By applying the perturbations to the\ntesting split of the dataset, we construct a challenge\nset to evaluate the corresponding numerical capa-\nbility of existing systems. Systems are trained on\nthe original datasets and evaluated on the perturbed\nchallenge set.\nDefense. Under the defense setting, perturba-\ntions are applied to all of training, validation, and\ntesting split of the dataset.\nBy comparing sys-\ntems\u2019 performance under the Defense with Attack\nsettings, we investigate to what extent the perfor-\nmance drop can be alleviated by using the pertur-\nbations as a data augmentation approach.\nSetting\nAttack\nDefense\nTrain on\ntrain\ntrain\u22c6\nValidate on\nval\nval\u22c6\nTest on\ntest\u22c6\ntest\u22c6\nTable 2: The Comparison between Two Settings in\nDNC. Perturbations (denoted by \u201c\u22c6\u201d) are applied to dif-\nferent dataset splits (train / val / test) under each setting.\nTo perturb under Attack or Defense setting, suit-\nable samples are \ufb01rst \ufb01ltered according to a series\nof conditions. The perturbations are applied only\nto these \ufb01ltered samples. The \ufb01ltered samples in\n3https://spacy.io/\nthe dataset split(s) are replaced with their perturbed\nversion to form the perturbed dataset. The \ufb01lter-\ning conditions and the formalized algorithm are\nprovided in Appendix B.\n5\nExperiments\n5.1\nExperiment Setup\nDatasets. In this paper, we used ASDiv-a (Miao\net al., 2020), DROP (Dua et al., 2019), and TATQA\n(Zhu et al., 2021) as our Numerical Question An-\nswering datasets. For DROP and TATQA, we \ufb01l-\ntered out DROP-num and TATQA-a, the numerical\nsubsets of them. The statistics of these datasets are\nshown in Table 4.\nSystems. We selected representative systems\non each dataset and test their performance against\nperturbations. For the ASDiv-a dataset, we use\nGraph2Tree (Patel et al., 2021). For the DROP\ndataset, we use BART-base and T5-base from Hug-\ngingface.4\nFor the TATQA dataset, we utilize\nTagOps with the RoBERTa backbone as described\nin the original paper.\nCompute Environment. All experiments are\ndone on a Linux machine equipped with 4 NVIDIA\nTesla V100 16GB GPUs. The average runtime of\nour experiments ranges from one to three hours.\nHyperparameters.\nIn our experiments, we\nadopt a general setting of hyperparameters of epoch\nnumber = 40, learning rate = 1e \u2212 5 and batch size\n= 32. It is observed in our exploratory experiments\nthat while the hyperparameters such as learning rate\nand batchsize do affect the absolute performance\nof the models, they have a modest effect on the gen-\neral trend of the models\u2019 strengths and weaknesses\nagainst the numerical perturbations. The details\nand analysis are provided in Appendix C.\n5.2\nExperiment Results and Analysis\nThe experiment results are provided in Table 3.\nThe metric we report is 1) the metric on original\ndatasets (Original), and 2) the absolute change of\nthe metric on perturbed datasets, denoted by \u201c\u2206\u201d.\nWe additionally provide the raw metric and relative\ndrop in Table 9 and Table 10 in the Appendix. The\ncalculation details of the observation can be found\nin Appendix D.2.\nAttack. As can be observed in Table 3 and Ta-\nble 10, most systems were severely challenged un-\nder the Attack setting and experienced signi\ufb01cant\n4https://github.com/huggingface/transformers\n ASDiv-a\nDROP-num\nTATQA-a\nCon\ufb01guration\nT5\nBART\nGPT2\nGraph2Tree\nT5\nBART\nTagOps\nSetting\nPerturbation\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcc\nAcc\nAcc\nLanguage\n-18.85%\n-18.85%\n-23.77%\n-27.05%\n-12.30%\n-12.30%\n-7.65%\n-7.38%\n-10.62%\n-14.73%\n-18.62%\nType\n-37.70%\n-11.48%\n-32.79%\n-15.57%\n-17.21%\n-10.66%\n0.27%\n1.09%\n-7.70%\n-11.06%\n-5.34%\nNoise\n-36.89%\n-36.89%\n-18.85%\n-21.31%\n-9.84%\n-9.02%\n0.27%\n0.55%\n-\n-\n-\nDistribution\n-16.39%\n-14.75%\n-29.51%\n-18.03%\n-13.11%\n-13.11%\n-6.56%\n-6.56%\n-\n-\n-\nVerbosity\n-41.80%\n-44.26%\n-25.41%\n-29.51%\n-10.66%\n-11.48%\n-33.33%\n-33.88%\n-9.58%\n-13.31%\n-1.90%\nExtra\n-25.41%\n-27.87%\n-41.80%\n-45.90%\n-28.69%\n-28.69%\n-53.83%\n-54.64%\n-11.79%\n-11.67%\n-1.21%\nLogic\n-29.51%\n-27.87%\n-36.89%\n-35.25%\n-25.41%\n-23.77%\n-28.42%\n-21.86%\n-\n-\n-14.29%\nAttack (\u2206)\nOrder\n-34.43%\n-5.74%\n-33.61%\n-4.10%\n-27.87%\n-7.38%\n-33.33%\n-7.10%\n-\n-\n1.12%\nLanguage\n-12.30%\n-13.93%\n-19.67%\n-24.59%\n2.46%\n2.46%\n-7.65%\n-7.38%\n0.07%\n-1.84%\n-7.59%\nType\n-11.48%\n-12.30%\n-4.92%\n-6.56%\n3.28%\n4.10%\n1.64%\n1.91%\n0.46%\n-0.95%\n2.93%\nNoise\n-14.75%\n-14.75%\n-3.28%\n-4.92%\n3.28%\n4.10%\n0.55%\n0.27%\n-\n-\n-\nDistribution\n-20.49%\n-20.49%\n-8.20%\n-9.84%\n-8.20%\n-9.02%\n-6.83%\n-6.01%\n-\n-\n-\nVerbosity\n-15.57%\n-16.39%\n-5.74%\n-7.38%\n-0.82%\n0.00%\n-0.27%\n1.09%\n-5.13%\n-1.84%\n2.25%\nExtra\n0.00%\n1.64%\n-2.46%\n-4.10%\n-17.21%\n-18.03%\n-20.22%\n-17.76%\n-11.32%\n-10.44%\n-9.14%\nLogic\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n13.64%\nDefense (\u2206)\nOrder\n-25.41%\n-4.10%\n-27.87%\n-7.38%\n-1.64%\n23.77%\n-29.23%\n-7.92%\n-\n-\n19.47%\nOriginal\nNone\n68.03%\n72.95%\n67.21%\n72.95%\n44.26%\n45.08%\n66.94%\n68.58%\n49.42%\n50.36%\n42.41%\nTable 3: The Results of DNC Framework. Five NLP systems are evaluated with three Numerical QA tasks under\nboth Attack and Defense settings. The symbol \u201c\u2206\u201d stands for the absolute metric difference between the current\nsetting and the original setting. The color scale represents the distance from the original setting, deeper means\nfurther from the original setting. For ASDiv-a, Acceq and Accans refer to the prediction accuracy of ground truth\nequations and denotation accuracy of answers, respectively. For DROP-num and TATQA-a, Acc refers to the\ndenotation accuracy of the answers. We provide the raw performance and relative change of the metrics w.r.t. the\noriginal setting in Appendix D.1. \u201c-\u201d denotes that automatic perturbation and automatic data augmentation as\ndescribed by \u00a74.3 is not applicable here. We provide detailed explanation of the reason why they are not applicable\nin Appendix E.\nDataset\n# Training\n# Validation\n# Testing\nASDiv-a\n974\n122\n122\nDROP-num\n42258\n5282\n5283\nTATQA-a\n1971\n245\n247\nTable 4: The Statistics of the Datasets Used.\nperformance drop, ranging from 5% to 50% abso-\nlute drop and 5% to 80% relative drop in answer\ndenotation accuracy. Between the two DNC goals,\nSemantic Parsing causes a more severe challenge,\naveraging 19.66% absolute drop and 31.79% rela-\ntive drop, as compared to the 13.15% absolute drop\nand 19.66% relative drop by Numerical Parsing.\nAmong the considered systems, Transformer-\nbased Seq2Seq systems (T5, BART, GPT2) are\nmore sensitive than the tasks-speci\ufb01c Graph2Tree\nsystem against the perturbations stemming from\nthe Numerical Parsing goal. The former resulted\nin 17.42% absolute drop and 27.06% relative drop,\nwhile Graph2Tree only experienced 3.07% abso-\nlute drop and 4.48% relative drop. The masking of\nnumbers used by Graph2Tree allows it to remain\nunaffected against a portion of the perturbations\ntargeting the Numerical Parsing goal.\nDefense. As a counteracting approach, the de-\nfense mechanism helps alleviate systems\u2019 lack of\ncorresponding numerical capabilities by applying\nautomatic perturbations to the training and valida-\ntion set. Via Defense, the lack according to the Se-\nmantic Parsing gets more recovery of (17.96% ab-\nsolute improvement and 26.95% relative improve-\nment vs. 6.52% absolute improvement and 11.42%\nrelative improvement).\nAmong the considered systems, Transformer-\nbased Seq2Seq systems bene\ufb01ts more from De-\nfense than the Graph2Tree system (12.53% abso-\nlute improvement and 20.52% relative improve-\nment vs.\n11.58% absolute improvement and\n16.88% relative improvement).\nDespite the recovery from Defense, the chal-\nlenge is still not solved. As the majority of the\ndefense performance is still more than 10% below\nthe original performance. This observation indi-\ncates that the lack of Numerical Capabilities is still\nan open question.\nSummary. Our DNC framework provides in-\nsights on two major aspects of the diagnosis to\nNumerical QA systems:\n1) It is demonstrated that severe numerical weak-\nnesses exist in current Numerical QA systems (\u201cAt-\ntack\u201d), and they can not be trivially eliminated via,\n although bene\ufb01ting from, an automatic data aug-\nmentation process (\u201cDefense\u201d).\n2) The systems\u2019 weaknesses are explicitly pro-\n\ufb01led in a quantitative and interpretable manner\nthrough the models\u2019 susceptibility difference to\na diversity of perturbations.\n6\nGuidelines and Open Directions\nIn this section, phenomena observed on different\nsystems and datasets were summarized to provide\ncomparison for existing methods. Also, recent re-\nlated efforts corresponding to these phenomena\nwere discussed to point open directions in the do-\nmain of Numerical QA.\n6.1\nTarget: Logical Form Generation vs.\nAnswer Predicting\nOne attribute speci\ufb01c to Numerical QA is the rea-\nsoning processes leading to the numerical answers,\nwhich is usually described by logical forms. On\ndatasets where the ground truth logical forms are\nprovided as an additional supervision (e.g., ASDiv-\na and TATQA), the systems have two options for\nthe target: 1) Logical Form Generation, where\nsystems generate the logical form which is later in-\nput to external symbolic executing systems such as\nPython scripts or SQL engines, and 2) Answer Pre-\ndicting, where systems directly predict the output\nanswer in an end-to-end manner. On datasets where\nground truth logical forms are not provided (e.g.,\nDROP), the latter is the most frequently adopted\napproach. Logical Form Generation and Answer\nPredicting differ in the actual object to conduct the\nexecuting step of the logical form insinuated by\nthe question (external symbolic systems vs. neural\nsystems). With Answer Predicting, systems are\nexpected to possess the capability of executing the\nlogical forms internally.\nWe investigate to what extent do existing sys-\ntems possess this execution capability, by compar-\ning the impact of the problem target T in Numer-\nical QA on ASDiv-a. The systems are trained to\npredict two different targets: 1) the logical form\n(i.e., the MWP equation), and 2) the logical form\nand the execution result. Since most MWP-speci\ufb01c\nsystems are incapable of predicting answers di-\nrectly, we choose the Transformer-based systems\nGPT2, BART and T5. Results in Table 5 indicate\nthat: 1) on existing systems, Logical Form Genera-\ntion is bene\ufb01cial for higher accuracy, and 2) even\nthough models managed to compose equations with\nModel\nAccans\nAcceq\nGPT2ans\n6.56%\n-\nGPT2eq\n45.08%\n44.26%\nBARTans\n9.02%\n-\nBARTeq\n72.95%\n67.21%\nT5ans\n2.46%\n-\nT5eq\n72.95%\n68.03%\nTable 5: Comparing Models with Different Prediction\nTargets on ASDiv-a. For a model M, Meq / Mans\npredicts equation / equation and answer, respectively.\nAcceq and Accans stand for the denotation accuracy of\nthe generated equation and the accuracy of the directly\npredicted answer, respectively.\nhigh accuracy, they struggle to faithfully execute\nan equation to get the correct answer.\nRecent work also pays increasing attention to\nthe execution capability. Systems such as TAPEX\n(Liu et al., 2022) and POET (Pi et al., 2022a) have\nbeen leveraging data synthesizing and intermedi-\nate pretraining to learn neural program executors\nand achieved state-of-the-art results over systems\nleveraging Logical Form Generation. This recent\ndevelopment shows the potential of neural systems\nwith enhanced execution capability on the Numeri-\ncal QA task.\n6.2\nNumbers: Tokenization vs. Replacement\nWe also investigate the impact of different ways of\nmanipulating numbers. There are two mainstream\nexisting methods to process and represent num-\nbers, herein referred to as the Tokenization and\nReplacement methods.\nTokenization methods such as WordPiece (Wu\net al., 2016) and BPE (Sennrich et al., 2016)\nadopted by existing Numerical QA systems divides\nnumbers into potentially multiple sub-word level\ntokens. E.g., The number 768 will be divided into\ntokens 7 and 68 by T5\u2019s tokenizer. This approach\nstems from the fundamental fact that existing sys-\ntems\u2019 vocabularies are \ufb01nite while the occurrences\nof numbers in a Numerical QA dataset can be too\ndiverse to include in a \ufb01nite vocabulary. Tokeniza-\ntion causes extra representation cost and erases the\ndigit integrity by potentially introducing multiple\ntokens for a single number.\nReplacement substitutes numbers with special\ntokens in the input ([NUM1], [NUM2], etc.), which\nare later re-substituted with the original number\nin the output logical forms. This approach avoids\nmultiple tokens by providing exactly one represen-\ntation for each number, but has its own limitations\n Model\nPerturbation\nAcceq\nAccans\n\u2206 Acceq\n\u2206 Accans\nLanguage\n31.97%\n32.79%\n-12.30%\n-12.30%\nType\n27.05%\n34.43%\n-17.21%\n-10.66%\nNoise\n34.43%\n36.07%\n-9.84%\n-9.02%\nDistribution\n31.15%\n31.97%\n-13.11%\n-13.11%\nVerbosity\n33.61%\n33.61%\n-10.66%\n-11.48%\nExtra\n15.57%\n16.39%\n-28.69%\n-28.69%\nLogic\n18.85%\n21.31%\n-25.41%\n-23.77%\nOrder\n16.39%\n37.70%\n-27.87%\n-7.38%\nGPT2token\nOriginal\n44.26%\n45.08%\n0.00%\n0.00%\nLanguage\n46.72%\n47.54%\n-9.84%\n-9.84%\nType\n56.56%\n57.38%\n0.00%\n0.00%\nNoise\n56.56%\n57.38%\n0.00%\n0.00%\nDistribution\n56.56%\n57.38%\n0.00%\n0.00%\nVerbosity\n22.13%\n22.13%\n-34.43%\n-35.25%\nExtra\n10.66%\n11.48%\n-45.90%\n-45.90%\nLogic\n32.79%\n40.16%\n-23.77%\n-17.21%\nOrder\n18.03%\n36.07%\n-38.52%\n-21.31%\nGPT2replace\nOriginal\n56.56%\n57.38%\n0.00%\n0.00%\nTable 6: The Results of Tokenization and Replacement\non GPT2. GPT2token adopts the Tokenization method\nand GPT2replace adopts the Replacement method.\nhandling number diversity since the recognition\nof numbers are usually performed with rule-based\nmatching, which is often non-exhaustive.\nIn this paper, T5, BART, GPT2 and TagOps\nadopts Tokenization, while Graph2Tree adopts Re-\nplacement. We implement two variations of GPT2:\nGPT2token and GPT2replace to compare their ro-\nbustness against different perturbations on the\nASDiv-a dataset.Results in Table 6 indicate that Re-\nplacement has an advantage when no perturbation\nis present or when the perturbation only involves\nchanges in number value. However, when the per-\nturbation changes number values, the Replacement-\nbased system is more severely challenged.\nWe hypothesize that the Replacement method re-\nmoves all numerical information such as the format\nand value of numbers in the problem and lost nu-\nmeracy capabilities, therefore the system receives\nonly textual signals such as number order or word\nfrequency, which further encouraged systems to\nlearn from spurious correlations as stated in Patel\net al. (2021). This hypothesis is consistent with\nthe observations of a recent study (Thawani et al.,\n2021a) that investigates of the mutual-enhancement\nbetween numeracy and literacy.\nThe respective limitations of Tokenization and\nReplacement are calling for more numeracy-\npreserving number representation methods. Some\nstudies have suggested changing number surface\nforms (Kim et al., 2021) or using dataset-agnostic\nrepresentation (Sundararaman et al., 2020), how-\never they either create extra token loads or could\nnot generalize well on large-scale real-world\ndataset. The numeracy-preserving number repre-\nsentation is another bottleneck for Numerical QA.\n7\nConclusion\nIn this paper we aim at diagnosing numerical ca-\npabilities in existing NLP systems. We list out a\nseries of numerical capabilities and design corre-\nsponding dataset perturbations. Empirical results\nshow that existing systems still lack numerical ca-\npabilities to a large extent, and this lack cannot\nbe eliminated in a trivial manner. Analysis into\nthe empirical results, discussion of the the existing\npractices, and insights for future directions of Nu-\nmerical QA dataset collection and system design\nare also provided.\nLimitations\nOur pipeline has limitations in the following two\naspects that we plan to address in the future:\nDependency on ground truth equation. Cur-\nrently, three of the eight DNC perturbations have\nstrong dependency on the ground truth solving\nequation, which is missing in datasets such as\nDROP. We hope to utilize semi-supervised ap-\nproaches in the future to enlarge the coverage of\nthe DNC perturbations.\nPerturbing scalability.\nCurrently our \ufb01lters\ncover only a portion of the whole dataset due to\nDNC \ufb01ltering and perturbing questions based on\nmanual rules and templates. we hope to develop\nmore automatic \ufb01ltering and perturbing in the fu-\nture. Also, DNC can only apply perturbations to\nnumbers provided by the problem, which limits its\ndiagnosing power in questions where an unspeci-\n\ufb01ed number is used, e.g., when numerical common-\nsense knowledge is involved.\nEthical Statements\nThe model implementation and datasets utilized in\nthis paper are based on publication and open-source\nrepositories. Licenses protocols are followed in the\nprocess of our experiments. No new datasets or\nNLP applications are presented in this paper and\nno violation of privacy or usage of demographic\ninformation was involved in our process of inter-\nacting with the datasets. Our experiments do not\ninvolve lots of compute time/power as reported in\nthe paper.\n References\nHadeel Al-Negheimish, Pranava Madhyastha, and\nAlessandra Russo. 2021a. Discrete reasoning tem-\nplates for natural language understanding. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Student Research Workshop, pages 80\u201387,\nOnline. Association for Computational Linguistics.\nHadeel Al-Negheimish, Pranava Madhyastha, and\nAlessandra Russo. 2021b. Numerical reasoning in\nmachine reading comprehension tasks: are we there\nyet?\nIn Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9643\u20139649, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nKunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xi-\naochuan, Yuyu Zhang, Le Song, Taifeng Wang,\nYuan Qi, and Wei Chu. 2020a. Question directed\ngraph attention network for numerical reasoning\nover text. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 6759\u20136768, Online. Associa-\ntion for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020b. Tabfact : A large-scale\ndataset for table-based fact veri\ufb01cation.\nIn Inter-\nnational Conference on Learning Representations\n(ICLR), Addis Ababa, Ethiopia.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge,\nand William Yang Wang. 2021. FinQA: A dataset of\nnumerical reasoning over \ufb01nancial data. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 3697\u20133711,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nTing-Rui\nChiang\nand\nYun-Nung\nChen.\n2019.\nSemantically-aligned\nequation\ngeneration\nfor\nsolving and reasoning math word problems.\nIn\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 2656\u2013\n2668, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek B\nRao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben-\nton C. Hutchinson, Reiner Pope, James Bradbury, Ja-\ncob Austin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarc\u00eda, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Er-\nica Oliveira Moreira, Rewon Child, Oleksandr Polo-\nzov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark D\u00edaz, Orhan Firat, Michele\nCatasta, Jason Wei, Kathleen S. Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah\nFiedel. 2022.\nPalm: Scaling language modeling\nwith pathways. ArXiv, abs/2204.02311.\nHaoyu Dong, Zhoujun Cheng, Xinyi He, Mengyu\nZhou, Anda Zhou, Fan Zhou, Ao Liu, Shi Han, and\nDongmei Zhang. 2022. Table pre-training: A survey\non model architectures, pre-training objectives, and\ndownstream tasks. In IJCAI\u20192022 SURVEY TRACK.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2368\u20132378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nM\u00fcller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training.\nIn Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4320\u20134333, Online. Association for\nComputational Linguistics.\nMinghao Hu, Yuxing Peng, Zhen Huang, and Dong-\nsheng Li. 2019. A multi-type multi-span network\nfor reading comprehension that requires discrete rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1596\u20131606, Hong Kong, China. Association for\nComputational Linguistics.\nHiroshi Iida, Dung Thai, Varun Manjunatha, and Mo-\nhit Iyyer. 2021. TABBIE: Pretrained representations\nof tabular data. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3446\u20133456, Online. As-\nsociation for Computational Linguistics.\nJeonghwan Kim,\nGiwon Hong,\nKyung-min Kim,\nJunmo Kang, and Sung-Hyon Myaeng. 2021. Have\nyou seen that number? investigating extrapolation\nin question answering models.\nIn Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7031\u20137037, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\n Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1152\u20131157, San Diego, California. Association for\nComputational Linguistics.\nVivek Kumar, Rishabh Maheshwary, and Vikram Pudi.\n2021.\nAdversarial examples for evaluating math\nword problem solvers. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2021,\npages 2705\u20132712, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nNate Kushman, Yoav Artzi, Luke Zettlemoyer, and\nRegina Barzilay. 2014.\nLearning to automatically\nsolve algebra word problems.\nIn Proceedings of\nthe 52nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 271\u2013281, Baltimore, Maryland. Association\nfor Computational Linguistics.\nYihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan,\nBing Tian Dai, Yan Wang, Dongxiang Zhang, and\nEe-Peng Lim. 2022. Mwptoolkit: An open-source\nframework for deep learning-based math word prob-\nlem solvers. Proceedings of the AAAI Conference\non Arti\ufb01cial Intelligence, 36(11):13188\u201313190.\nZhenwen Liang, Jipeng Zhang, Jie Shao, and Xi-\nangliang Zhang. 2021.\nMWP-BERT: A strong\nbaseline\nfor\nmath\nword\nproblems.\nCoRR,\nabs/2107.13435.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: Table pre-training via learning a neural\nSQL executor.\nIn International Conference on\nLearning Representations.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and develop-\ning English math word problem solvers. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 975\u2013984, On-\nline. Association for Computational Linguistics.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin.\n2021.\nInvestigating the limitations of the trans-\nformers with simple arithmetic tasks.\nCoRR,\nabs/2102.13019.\nInkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rig-\notti, Youssef Mroueh, Pierre Dognin, Jerret Ross,\nRavi Nair, and Erik Altman. 2021.\nTabular\ntransformers for modeling multivariate time series.\nIn ICASSP 2021-2021 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 3565\u20133569. IEEE.\nKuntal Kumar Pal and Chitta Baral. 2021. Investigat-\ning numeracy learning ability of a text-to-text trans-\nfer model. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, pages 3095\u2013\n3101, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015.\nCompo-\nsitional semantic parsing on semi-structured tables.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1470\u20131480, Beijing, China. Association for Compu-\ntational Linguistics.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems?\nIn Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi\nLin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\nWeizhu Chen. 2022a. Reasoning like program ex-\necutors. ArXiv, abs/2201.11473.\nXinyu Pi, Bing Wang, Yan Gao, Jiaqi Guo, Zhoujun Li,\nand Jian-Guang Lou. 2022b. Towards robustness of\ntext-to-SQL models against natural and realistic ad-\nversarial table perturbation. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2007\u20132022, Dublin, Ireland. Association for Com-\nputational Linguistics.\nJinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang,\nand Liang Lin. 2020. Semantically-aligned univer-\nsal tree-structured solver for math word problems.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3780\u20133789, Online. Association for Computa-\ntional Linguistics.\nQiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan\nLiu. 2019. NumNet: Machine reading comprehen-\nsion with numerical reasoning. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2474\u20132484, Hong Kong,\nChina. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016.\nNeural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715\u2013\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nYibin Shen and Cheqing Jin. 2020. Solving math word\nproblems with multi-encoders and multi-decoders.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 2924\u20132934,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\n Georgios Spithourakis and Sebastian Riedel. 2018. Nu-\nmeracy for language models: Evaluating and im-\nproving their ability to predict numbers. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2104\u20132115, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nSowmya S. Sundaram,\nSairam Gurajada,\nMarco\nFisichella, Deepak P, and Savitha Sam Abraham.\n2022. Why are nlp models fumbling at elementary\nmath? a survey of deep learning based word prob-\nlem solvers. ArXiv, abs/2205.15683.\nDhanasekar Sundararaman, Shijing Si, Vivek Subra-\nmanian, Guoyin Wang, Devamanyu Hazarika, and\nLawrence Carin. 2020.\nMethods for numeracy-\npreserving word embeddings. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4742\u20134753,\nOnline. Association for Computational Linguistics.\nAvijit Thawani, Jay Pujara, and Filip Ilievski. 2021a.\nNumeracy enhances the literacy of language models.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6960\u20136967, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021b. Representing numbers in NLP: a\nsurvey and a vision.\nIn Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644\u2013656, Online. As-\nsociation for Computational Linguistics.\nShyam Upadhyay and Ming-Wei Chang. 2017. Anno-\ntating derivations: A new evaluation strategy and\ndataset for algebra word problems.\nIn Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 1, Long Papers, pages 494\u2013504, Valencia,\nSpain. Association for Computational Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019.\nDo NLP models know\nnumbers?\nprobing numeracy in embeddings.\nIn\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5307\u2013\n5315, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 845\u2013\n854, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Z. Chen, Quoc V. Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaob-\ning Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo\nKato, Taku Kudo, Hideto Kazawa, Keith Stevens,\nGeorge Kurian, Nishant Patil, Wei Wang, Cliff\nYoung, Jason R. Smith, Jason Riesa, Alex Rudnick,\nOriol Vinyals, Gregory S. Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google\u2019s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. ArXiv, abs/1609.08144.\nZhipeng Xie and Shichao Sun. 2019. A goal-driven\ntree-structured neural model for math word prob-\nlems.\nIn Proceedings of the Twenty-Eighth In-\nternational Joint Conference on Arti\ufb01cial Intelli-\ngence, IJCAI-19, pages 5299\u20135305. International\nJoint Conferences on Arti\ufb01cial Intelligence Organi-\nzation.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8413\u2013\n8426, Online. Association for Computational Lin-\nguistics.\nDongxiang\nZhang,\nLei\nWang,\nLuming\nZhang,\nBing Tian Dai, and Heng Tao Shen. 2020a.\nThe\ngap of semantic parsing: A survey on automatic\nmath word problem solvers.\nIEEE Transactions\non Pattern Analysis and Machine Intelligence,\n42(9):2287\u20132305.\nJipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan\nWang, Jie Shao, and Ee-Peng Lim. 2020b. Graph-\nto-tree learning for solving math word problems. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3928\u2013\n3937, Online. Association for Computational Lin-\nguistics.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017.\nSeq2sql:\nGenerating structured queries\nfrom natural language using reinforcement learning.\nCoRR, abs/1709.00103.\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\nTat-Seng Chua. 2021. TAT-QA: A question answer-\ning benchmark on a hybrid of tabular and textual\ncontent in \ufb01nance. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3277\u20133287, Online. Associa-\ntion for Computational Linguistics.\n A\nFormal De\ufb01nition of Perturbations\nWe provide the formalized de\ufb01nition of the pertur-\nbations as follows. In all de\ufb01nitions, \u201c\u22c6\u201d denotes\nperturbed version.\nNoise Perturbation. To apply noise perturba-\ntion to an number n, an variable X is uniformly\nsampled on the interval (1, 10). Then a fractional\npart corresponding to X is added the concerned\nnumber n, i.e.,\nX \u223c U(1, 10)\nn\u22c6 = n + 0.1 \u00d7 \u230aX\u230b\nDistribution Perturbation. The Distribution\nPerturbation changes the number distribution in the\ndataset by adding an normally distributed random\nvariable X to the concerned number n. I.e.,\nX \u223c N(\u00b5d, \u03b42\nd)\nn\u22c6 = n + \u230aX\u230b\nIn this paper we adopt \u00b5d = 1000 and \u03b4d = 300.\nLanguage Perturbation. The concerned num-\nber string ns is replaced by the English word de-\nscribing the same quantity, i.e.,\nn\u22c6\ns = Num2Words(ns)\nType Perturbation. To apply the Type Pertur-\nbation, the concerned number is expected to be an\nintegral number. The number string ns is concate-\nnated with an extra \u201c.0\u201d string to change the type of\nthe concerned number from integer to \ufb02oat-point,\ni.e.,\nn\u22c6\ns = Concat(ns, Stringfy(.0))\nVerbosity Perturbation. The Verbosity Pertur-\nbation aims to introduce irrelevant numbers without\nchanging the semantics of the problem. To perturb\na number string ns, we concatenate it with an irrel-\nevant number in parentheses, the irrelevant number\nis preceded by \"not\", i.e.,\nX \u223c N(\u00b5v, \u03b42\nv)\nn\u22c6\ns = Concat(ns, (not, Stringfy(X)))\nIn this paper we adopt \u00b5v = 100 and \u03b4v = 30.\nExtra Perturbation To apply the Extra Pertur-\nbation to a problem (B, P), an irrelevant sentence\ncontaining numbers from the corpus is added to the\nbody B, i.e.,\nP\u2206 = SampleOtherQs()\nP\u22c6 = P \u2295 P\u2206\nB\u22c6 = B\nLogic Perturbation To apply the Logic Pertur-\nbation to a problem (B, P), the prompt is altered\nto convert the problem logic used in the problem,\ni.e.,\nP\u22c6 = ConvertLogic(P)\nB\u22c6 = B\nOrder Perturbation For the Order Perturbation,\nthe sentence order in the problem body is manu-\nally altered in a manner that changes the order of\nnumber occurrence but not the problem logic, i.e.,\nP\u22c6 = ChangeOrder(P)\nB\u22c6 = B\nB\nDetails of the Perturbing Process\nB.1\nThe Filtering Conditions\nThe \ufb01ltering conditions for Perturbing Algorithms\nis different across perturbations. The perturbations\ncan be divided into two major categories: 1) pertur-\nbations that do not change the solving equation or\n\ufb01nal results (Language, Type, Verbosity, Extra, Or-\nder), and 2) perturbations that changes the solving\nequation or \ufb01nal results (Noise, Distri, Operation).\nFor perturbations in category 1), there is no limi-\ntation on the perturbing process, thus all questions\nnaturally pass the the \ufb01ltering condition.\nFor perturbations in category 2), the \ufb01ltering\nconditions follow the principles of Unambiguity,\nSuitability and Visibility.\nUnambiguity. The \ufb01ltered question should have\nan unambiguous mapping between the number to\nbe perturbed and the their location in the context.\nOne example that violates this principle is when\nthere are duplicated numbers in the problem body,\nthen it cannot be determined which occurrence of\nthe number affects the \ufb01nal result.\nSuitability. The number to be perturbed should\nbe suitable for the perturbation to be conducted.\nE.g. A \ufb02oat-point number should not be used as the\ntarget of the Noise perturbation which adds frac-\ntional part to integral numbers. In DNC, the Noise\nand Type perturbations requires the concerned num-\nber to be integral, and the Operation perturbation\n requires the question to match a manually created\ntemplate.\nVisibility. The concerned number should be\noccur in the the problem since the perturbations\ncan only be applied to known input numbers.\nB.2\nThe Formalized Perturbing Process\nAlgorithm 1: The Perturbing Process\nData: D = {Dtrain, Dval, Dtest}\nAllPert = {Noise, ...} as in \u00a74.2\nAllSet \u2208 {Attack, ...} as in \u00a74.3\nFilter = {Integral, ...} as in \u00a7B\nResult: D\u22c6 = {D\u22c6\ntrain, D\u22c6\nval, D\u22c6\ntest}\n/* Decide perturbations to use\n*/\nPerturbs \u2190 SelectBy(AllPert, D);\n/* Create perturbed dataset for\neach perturbation and setting */\nfor setting \u2208 AllSet do\n/* Decide split to perturb\n*/\nD\u2206, Dremain \u2190\nSelectBy(D, setting);\nfor perturb \u2208 Perturbs do\nD\u22c6\n\u2206 \u2190 {};\nfor d \u2208 D\u2206 do\nif Filter(d) then\nD\u22c6\n\u2206 \u2190 D\u22c6\n\u2206 + perturb(d);\nend\nelse\nD\u22c6\n\u2206 \u2190 D\u22c6\n\u2206 + d;\nend\nend\nD\u22c6\nperturb = {D\u22c6\n\u2206, Dremain}\nend\nend\nD\u22c6 = {D\u22c6\nperturb}| perturb \u2208 Perturbs\nC\nHyperparameters\nIn our exploratory experiment, it it observed that\nwhile the hyperparameters such as learning rate\nand batchsize do affect the absolute performance\nof the models, they have a modest effect on the gen-\neral trend of the models\u2019 strengths and weaknesses\nagainst the numerical perturbations. We hypothe-\nsize that this is due to the numerical capabilities\nof a model being contributed mostly by the model\narchitecture instead of hyperparameters.\nFor example, when the hyperparameters are vary-\ning from the default setting (1e-5 for learning rate,\n32 for batch size), the following results are ob-\nserved:\nOn Graph2Tree, the results of changing the learn-\ning rate and batch size are shown in Table 7, the\ntrend of results with the varied hyperparameters\nalign with the default result as shown in Table 3.\nPerturbation\nAcceq\nlr=1e-2\nAcceq\nbatchsize=128\nDefault\nLanguage\n-4.10%\n-9.84%\n-7.65%\nType\n3.28%\n3.28%\n0.27%\nNoise\n4.10%\n1.64%\n0.27%\nDistribution\n-4.92%\n-10.66%\n-6.56%\nVerbosity\n-33.61%\n-38.52%\n-33.33%\nExtra\n-44.26%\n-57.38%\n-53.83%\nLogic\n-18.03%\n-27.87%\n-28.42%\nOrder\n-33.61%\n-13.11%\n-33.33%\nOriginal\n62.30%\n71.31%\n66.94%\nTable 7: The results of the attack Acceq for Graph2Tree\non ASDiv-a\nSimilar behavior can also be observed on large\ntransformer-based model such as T5, as shown in\nTable 8:\nPerturbation\nAcceq\nlr=1e-4\nAcceq\nbatchsize=16\nDefault\nLanguage\n-17.21%\n-17.21%\n-18.85%\nType\n-20.49%\n-32.79%\n-37.70%\nNoise\n-23.77%\n-32.79%\n-36.89%\nDistribution\n-18.03%\n-13.11%\n-16.39%\nVerbosity\n-39.34%\n-38.52%\n-41.80%\nExtra\n-40.16%\n-21.31%\n-25.41%\nLogic\n-27.05%\n-28.69%\n-29.51%\nOrder\n-31.15%\n-33.61%\n-34.43%\nOriginal\n61.48%\n63.11%\n68.03%\nTable 8: The results of the attack Acceq for T5 on\nASDiv-a\nConsidering this observation, and the fact that\nthe number of our experiment is large due to the\ncombination of different models, datasets, DNC\nsettings, and DNC perturbations, we chose one\ngeneral setting to reduce search space. We chose\nthe setting as close as possible to the reported set-\nting in the original papers of Graph2Tree and T5.\nWe veri\ufb01ed that this setting provides suf\ufb01ciently\ngood performance to demonstrate the performance\ngap corresponding to the perturbations, since our\nexperiment focused more on the performance of a\n same model checkpoint against the datasets before\nand after the perturbations.\nD\nDNC Results\nD.1\nRaw Performance And Relative\nPerformance Drop\nWe provide the original result in Table 9 and the\nrelative performance drop in Table 10.\nD.2\nObservation Calculation Details\nWe denote the experiment results table in Table 11.\nThe values, observation explanation, and the for-\nmula used are provided in Table 12.\nE\nDNC Experiments that Are Not\nApplicable\nThe following types of experiments are not appli-\ncable in current DNC framework:\nE.1\nThe Defense of the Logic perturbation on\nASDiv-a\nThe Logic perturbation requires the problem to be\nperturbed in a way that the logic is changed while\nthe semantics of the problem is still cohesive. This\nrequirement proposes challenge on the scalability\nof the perturbation. For the Attack setting, we\nutilized manually annotated labels. However, under\nthe Defense setting the perturbations are expected\nto automatically augment the dataset. Thus, the\nDefense setting results of Logic perturbation on\nASDiv-a is not applicable.\nE.2\nNoise and Type perturbations on\nDROP-num and TATQA-a\nDROP-num and TATQA-a do not provide supervi-\nsion of the operand origins, therefore a mapping\nfrom the operands in equation to the context quan-\ntities cannot be built, which results in the Noise\nand Distribution perturbation not applicable on the\nDROP-num and TATQA-a datasets.\nE.3\nLogic Perturbation on DROP-num\nDROP-num does not provide ground truth reason-\ning steps or logical forms, thus Logic perturbations\nthat has dependency on the provided supervision is\nnot applicable on DROP-num.\nE.4\nOrder Perturbation on DROP-num\nDROP-num is a reasonding dataset based on real-\nworld paragraphs that usually have logical or tem-\nporal order information. Order perturbation breaks\nthe semantic of the paragraph and will also confuse\nhumans. Thus Order perturbation is not valid on\nDROP-num and the results are not applicable.\n Con\ufb01guration\nASDiv-a\nDROP-num\nTATQA-a\nT5\nBART\nGPT2\nGraph2Tree\nT5\nBART\nTagOps\nSetting\nPerturbation\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcc\nAcc\nAcc\nAttack\nLanguage\n49.18%\n54.10%\n43.44%\n45.90%\n31.97%\n32.79%\n59.29%\n61.20%\n38.80%\n35.62%\n23.79%\nType\n30.33%\n61.48%\n34.43%\n57.38%\n27.05%\n34.43%\n67.21%\n69.67%\n41.72%\n39.30%\n37.07%\nNoise\n31.15%\n36.07%\n48.36%\n51.64%\n34.43%\n36.07%\n67.21%\n69.13%\n-\n-\n-\nDistribution\n51.64%\n58.20%\n37.70%\n54.92%\n31.15%\n31.97%\n60.38%\n62.02%\n-\n-\n-\nVerbosity\n26.23%\n28.69%\n41.80%\n43.44%\n33.61%\n33.61%\n33.61%\n34.70%\n39.84%\n37.04%\n40.52%\nExtra\n42.62%\n45.08%\n25.41%\n27.05%\n15.57%\n16.39%\n13.11%\n13.93%\n37.63%\n38.69%\n41.21%\nLogic\n38.52%\n45.08%\n30.33%\n37.70%\n18.85%\n21.31%\n38.52%\n46.72%\n-\n-\n28.12%\nOrder\n33.61%\n67.21%\n33.61%\n68.85%\n16.39%\n37.70%\n33.61%\n61.48%\n-\n-\n43.53%\nDefense\nLanguage\n55.74%\n59.02%\n47.54%\n48.36%\n46.72%\n47.54%\n59.29%\n61.20%\n49.49%\n48.52%\n34.83%\nType\n56.56%\n60.66%\n62.30%\n66.39%\n47.54%\n49.18%\n68.58%\n70.49%\n49.88%\n49.41%\n45.34%\nNoise\n53.28%\n58.20%\n63.93%\n68.03%\n47.54%\n49.18%\n67.49%\n68.85%\n-\n-\n-\nDistribution\n47.54%\n52.46%\n59.02%\n63.11%\n36.07%\n36.07%\n60.11%\n62.57%\n-\n-\n-\nVerbosity\n52.46%\n56.56%\n61.48%\n65.57%\n43.44%\n45.08%\n66.67%\n69.67%\n44.29%\n48.52%\n44.67%\nExtra\n68.03%\n74.59%\n64.75%\n68.85%\n27.05%\n27.05%\n46.72%\n50.82%\n38.10%\n39.92%\n33.28%\nLogic\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n56.05%\nOrder\n42.62%\n68.85%\n39.34%\n65.57%\n42.62%\n68.85%\n37.70%\n60.66%\n-\n-\n61.89%\nOriginal\nNone\n68.03%\n72.95%\n67.21%\n72.95%\n44.26%\n45.08%\n66.94%\n68.58%\n49.42%\n50.36%\n42.41%\nTable 9: The raw results of the DNC Framework. Notations here follow the ones in the main experiment results\nASDiv-a\nDROP-num\nTATQA-a\nCon\ufb01guration\nT5\nBART\nGPT2\nGraph2Tree\nT5\nBART\nTagOps\nSetting\nPerturbation\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcc\nAcc\nAcc\nLanguage\n-27.71%\n-25.84%\n-35.37%\n-37.08%\n-27.78%\n-27.27%\n-11.43%\n-10.76%\n-21.49%\n-29.26%\n-43.90%\nType\n-55.42%\n-15.73%\n-48.78%\n-21.35%\n-38.89%\n-23.64%\n0.41%\n1.59%\n-15.59%\n-21.96%\n-12.60%\nNoise\n-54.22%\n-50.56%\n-28.05%\n-29.21%\n-22.22%\n-20.00%\n0.41%\n0.80%\n-\n-\n-\nDistribution\n-24.10%\n-20.22%\n-43.90%\n-24.72%\n-29.63%\n-29.09%\n-9.80%\n-9.56%\n-\n-\n-\nVerbosity\n-61.45%\n-60.67%\n-37.80%\n-40.45%\n-24.07%\n-25.45%\n-49.80%\n-49.40%\n-19.38%\n-26.44%\n-4.47%\nExtra\n-37.35%\n-38.20%\n-62.20%\n-62.92%\n-64.81%\n-63.64%\n-80.41%\n-79.68%\n-23.86%\n-23.17%\n-2.85%\nLogic\n-43.37%\n-38.20%\n-54.88%\n-48.31%\n-57.41%\n-52.73%\n-42.45%\n-31.87%\n-\n-\n-33.69%\nAttack (\u2206%)\nOrder\n-50.60%\n-7.87%\n-50.00%\n-5.62%\n-62.96%\n-16.36%\n-49.80%\n-10.36%\n-\n-\n2.64%\nLanguage\n-18.07%\n-19.10%\n-29.27%\n-33.71%\n5.56%\n5.45%\n-11.43%\n-10.76%\n0.14%\n-3.65%\n-17.89%\nType\n-16.87%\n-16.85%\n-7.32%\n-8.99%\n7.41%\n9.09%\n2.45%\n2.79%\n0.93%\n-1.88%\n6.91%\nNoise\n-21.69%\n-20.22%\n-4.88%\n-6.74%\n7.41%\n9.09%\n0.82%\n0.40%\n-\n-\n-\nDistribution\n-30.12%\n-28.09%\n-12.20%\n-13.48%\n-18.52%\n-20.00%\n-10.20%\n-8.76%\n-\n-\n-\nVerbosity\n-22.89%\n-22.47%\n-8.54%\n-10.11%\n-1.85%\n0.00%\n-0.41%\n1.59%\n-10.38%\n-3.65%\n5.31%\nExtra\n0.00%\n2.25%\n-3.66%\n-5.62%\n-38.89%\n-40.00%\n-30.20%\n-25.90%\n-22.90%\n-20.72%\n-21.54%\nLogic\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n32.16%\nDefense (\u2206%)\nOrder\n-37.35%\n-5.62%\n-41.46%\n-10.11%\n-3.70%\n52.73%\n-43.67%\n-11.55%\n-\n-\n45.92%\nOriginal\nNone\n68.03%\n72.95%\n67.21%\n72.95%\n44.26%\n45.08%\n66.94%\n68.58%\n49.42%\n50.36%\n42.41%\nTable 10: The relative drop results of the DNC Framework. Notations here follow the ones in the main experiment\nresults\n A\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nASDiv-a\nDROP-num\nTATQA-a\nCon\ufb01guration\nT5\nBART\nGPT2\nGraph2Tree\nT5\nBART\nTagOps\nSetting\nPerturbation\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcceq\nAccans\nAcc\nAcc\nAcc\n1\nLanguage\n-18.85%\n-18.85%\n-23.77%\n-27.05%\n-12.30%\n-12.30%\n-7.65%\n-7.38%\n-10.62%\n-14.73%\n-18.62%\n2\nType\n-37.70%\n-11.48%\n-32.79%\n-15.57%\n-17.21%\n-10.66%\n0.27%\n1.09%\n-7.70%\n-11.06%\n-5.34%\n3\nNoise\n-36.89%\n-36.89%\n-18.85%\n-21.31%\n-9.84%\n-9.02%\n0.27%\n0.55%\n-\n-\n-\n4\nDistribution\n-16.39%\n-14.75%\n-29.51%\n-18.03%\n-13.11%\n-13.11%\n-6.56%\n-6.56%\n-\n-\n-\n5\nVerbosity\n-41.80%\n-44.26%\n-25.41%\n-29.51%\n-10.66%\n-11.48%\n-33.33%\n-33.88%\n-9.58%\n-13.31%\n-1.90%\n6\nExtra\n-25.41%\n-27.87%\n-41.80%\n-45.90%\n-28.69%\n-28.69%\n-53.83%\n-54.64%\n-11.79%\n-11.67%\n-1.21%\n7\nLogic\n-29.51%\n-27.87%\n-36.89%\n-35.25%\n-25.41%\n-23.77%\n-28.42%\n-21.86%\n-\n-\n-14.29%\n8\nAttack (\u2206)\nOrder\n-34.43%\n-5.74%\n-33.61%\n-4.10%\n-27.87%\n-7.38%\n-33.33%\n-7.10%\n-\n-\n1.12%\n9\nLanguage\n-12.30%\n-13.93%\n-19.67%\n-24.59%\n2.46%\n2.46%\n-7.65%\n-7.38%\n0.07%\n-1.84%\n-7.59%\n10\nType\n-11.48%\n-12.30%\n-4.92%\n-6.56%\n3.28%\n4.10%\n1.64%\n1.91%\n0.46%\n-0.95%\n2.93%\n11\nNoise\n-14.75%\n-14.75%\n-3.28%\n-4.92%\n3.28%\n4.10%\n0.55%\n0.27%\n-\n-\n-\n12\nDistribution\n-20.49%\n-20.49%\n-8.20%\n-9.84%\n-8.20%\n-9.02%\n-6.83%\n-6.01%\n-\n-\n-\n13\nVerbosity\n-15.57%\n-16.39%\n-5.74%\n-7.38%\n-0.82%\n0.00%\n-0.27%\n1.09%\n-5.13%\n-1.84%\n2.25%\n14\nExtra\n0.00%\n1.64%\n-2.46%\n-4.10%\n-17.21%\n-18.03%\n-20.22%\n-17.76%\n-11.32%\n-10.44%\n-9.14%\n15\nLogic\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n13.64%\n16\nDefense (\u2206)\nOrder\n-25.41%\n-4.10%\n-27.87%\n-7.38%\n-1.64%\n23.77%\n-29.23%\n-7.92%\n-\n-\n19.47%\nOriginal\nNone\n68.03%\n72.95%\n67.21%\n72.95%\n44.26%\n45.08%\n66.94%\n68.58%\n49.42%\n50.36%\n42.41%\nTable 11: The main experiment result table with cell coordinates\nValue\nExplanation\nFormula\n19.66%\nAverage performance drop of all models\ncaused by perturbations according to the\nSemantic Parsing stage\nAVERAGE(B5:B8,D5:D8,F5:F8,H5:H8,I5:I8,J5:J8,K5:K8)\n13.15%\nAverage performance drop of all models\ncaused by perturbations according to the\nNumerical Parsing stage\nAVERAGE(B1:B4,D1:D4,F1:F4,H1:H4,I1:I4,J1:J4,K1:K4)\n17.42%\nAverage performance drop of all Transformer-\nbased models caused by perturbations according\nto the Semantic Parsing stage\nAVERAGE(B5:B8,D5:D8,F5:F8,I5:I8,J5:J8,K5:K8)\n3.07%\nAverage performance drop of the Graph2Tree\nsystem caused by perturbations according to\nthe Semantic Parsing stage\nAVERAGE(B1:B4,D1:D4,F1:F4,I1:I4,J1:J4,K1:K4)\n17.96%\nAverage performance recovery of all models\ncaused by perturbations according to the\nSemantic Parsing stage\nAVERAGE(B13:B16,D13:D16,F13:F16,H13:H16,I13:I16,J13:J16,K13:K16)\n6.52%\nAverage performance recovery of all models\ncaused by perturbations according to the\nNumerical Parsing stage\nAVERAGE(B9:B12,D9:D12,F9:F12,H9:H12,I9:I12,J9:J12,K9:K12)\n12.53%\nAverage performance recovery of all Transformer-\nbased models caused by perturbations according\nto the Semantic Parsing stage\nAVERAGE(B13:B16,D13:D16,F13:F16,I13:I16,J13:J16,K13:K16)\n11.58%\nAverage performance recovery of the Graph2Tree\nsystem caused by perturbations according to the\nSemantic Parsing stage\nAVERAGE(B9:B12,D9:D12,F9:F12,I9:I12,J9:J12,K9:K12)\nTable 12: The Value, Explanation, And Formula Used of The Experiment Observations.\n"}, "A Survey On Neural Word Embeddings": {"authors": ["Erhan Sezerer", "Selma Tekir"], "title": "A Survey On Neural Word Embeddings", "url": "https://arxiv.org/pdf/2110.01804.pdf", "abstract": "Understanding human language has been a sub-challenge on the way of intelligent machines. The study of meaning in natural language processing (NLP) relies on the distributional hypothesis where language elements get meaning from the words that co-occur within contexts. The revolutionary idea of distributed representation for a concept is close to the working of a human mind in that the meaning of a word is spread across several neurons, and a loss of activation will only slightly affect the memory retrieval process.   Neural word embeddings transformed the whole field of NLP by introducing substantial improvements in all NLP tasks. In this survey, we provide a comprehensive literature review on neural word embeddings. We give theoretical foundations and describe existing work by an interplay between word embeddings and language modelling. We provide broad coverage on neural word embeddings, including early word embeddings, embeddings targeting specific semantic relations, sense embeddings, morpheme embeddings, and finally, contextual representations. Finally, we describe benchmark datasets in word embeddings' performance evaluation and downstream tasks along with the performance results of/due to word embeddings.", "arxiv_id": "2110.01804", "published_date": "2021-10-05", "year": 2021, "introduction": "INTRODUCTION The recent decade has witnessed a transformation in natural language processing (NLP). This transformation can be attributed to neural language models, their success in representation learning, and the transfer of this knowledge into complex NLP tasks. Before neural representation learning, representations of words or documents have been computed using the vector space model (VSM) of semantics. Turney and Pantel [131] provide a comprehensive survey on the use of VSM for semantics. In VSM [120], frequencies of words in documents are considered to form a term-document matrix, and global co-occurrences of words in context lead to word-context matrices [35, 67, 83]. Although these count-based representations are proved helpful in addressing semantics, they are the bag of words approaches and are not able to capture both syntactical and semantic features at the same time, which is required for performing well in NLP tasks. Neural word embeddings are due to neural language models. Neural network architecture is constructed to predict the next word given the set of neighboring words in the sequence in neural language modeling. In the iterative processing of this prediction over a large corpus, the learned weights in the hidden layers serve as neural embeddings for words. Neural word embeddings have experienced an evolution. Early word embeddings had some problems. Although they can learn syntactic and semantic regularities, they are not so good at capturing their mixture. Moreover, they provide just one representation that is shared among the different senses of a word. State-of-the-art contextual embeddings are responsive to these problems. They lead to a significant performance improvement and find their application throughout all NLP tasks and in many other fields [60, 64, 137]. In this article, we describe this transition by first providing the theoretical foundations. Then, preliminary realizations of these ideas by some seminal papers are explained. In the remaining part, generally accepted and efficiently computable early word embeddings are introduced. Afterward, extensions to early word embeddings are given with respect to some criteria such as the use of knowledge base, having morphological features, and addressing specific semantic relations (synonym, antonym, hypernym, hyponym, etc.). Succeedingly, separate sections are devoted to sense, morphological, and contextual embeddings. We also include performance evaluation of word embeddings on the benchmark datasets. Authors\u2019 address: Erhan Sezerer, erhansezerer@iyte.edu.tr; Selma Tekir, selmatekir@iyte.edu.tr, Izmir Institute of Technology, Izmir, Turkey, 35430. 1 arXiv:2110.01804v1  [cs.CL]  5 Oct 2021 ", "conclusion": "CONCLUSION Human-level language understanding is one of the oldest challenges in computer science. Many scientific works have been dedicated to finding good representations for semantic units (words, morphemes, characters) in languages since it is preliminary for all downstream tasks in NLP. Most of these studies use the distributional hypothesis, where the meaning of a word is measured from its neighboring words. ", "full_text": "A Survey On Neural Word Embeddings\nERHAN SEZERER and SELMA TEKIR, Izmir Institute of Technology\nUnderstanding human language has been a sub-challenge on the way of intelligent machines. The study of meaning in natural\nlanguage processing (NLP) relies on the distributional hypothesis where language elements get meaning from the words that co-occur\nwithin contexts. The revolutionary idea of distributed representation for a concept is close to the working of a human mind in that the\nmeaning of a word is spread across several neurons, and a loss of activation will only slightly affect the memory retrieval process.\nNeural word embeddings transformed the whole field of NLP by introducing substantial improvements in all NLP tasks. In this\nsurvey, we provide a comprehensive literature review on neural word embeddings. We give theoretical foundations and describe\nexisting work by an interplay between word embeddings and language modeling. We provide broad coverage on neural word\nembeddings, including early word embeddings, embeddings targeting specific semantic relations, sense embeddings, morpheme\nembeddings, and finally, contextual representations. Finally, we describe benchmark datasets in word embeddings\u2019 performance\nevaluation and downstream tasks along with the performance results of/due to word embeddings.\n1\nINTRODUCTION\nThe recent decade has witnessed a transformation in natural language processing (NLP). This transformation can be\nattributed to neural language models, their success in representation learning, and the transfer of this knowledge into\ncomplex NLP tasks.\nBefore neural representation learning, representations of words or documents have been computed using the vector\nspace model (VSM) of semantics. Turney and Pantel [131] provide a comprehensive survey on the use of VSM for\nsemantics. In VSM [120], frequencies of words in documents are considered to form a term-document matrix, and\nglobal co-occurrences of words in context lead to word-context matrices [35, 67, 83]. Although these count-based\nrepresentations are proved helpful in addressing semantics, they are the bag of words approaches and are not able to\ncapture both syntactical and semantic features at the same time, which is required for performing well in NLP tasks.\nNeural word embeddings are due to neural language models. Neural network architecture is constructed to predict the\nnext word given the set of neighboring words in the sequence in neural language modeling. In the iterative processing\nof this prediction over a large corpus, the learned weights in the hidden layers serve as neural embeddings for words.\nNeural word embeddings have experienced an evolution. Early word embeddings had some problems. Although they\ncan learn syntactic and semantic regularities, they are not so good at capturing their mixture. Moreover, they provide\njust one representation that is shared among the different senses of a word. State-of-the-art contextual embeddings are\nresponsive to these problems. They lead to a significant performance improvement and find their application throughout\nall NLP tasks and in many other fields [60, 64, 137].\nIn this article, we describe this transition by first providing the theoretical foundations. Then, preliminary realizations\nof these ideas by some seminal papers are explained. In the remaining part, generally accepted and efficiently computable\nearly word embeddings are introduced. Afterward, extensions to early word embeddings are given with respect to some\ncriteria such as the use of knowledge base, having morphological features, and addressing specific semantic relations\n(synonym, antonym, hypernym, hyponym, etc.). Succeedingly, separate sections are devoted to sense, morphological,\nand contextual embeddings. We also include performance evaluation of word embeddings on the benchmark datasets.\nAuthors\u2019 address: Erhan Sezerer, erhansezerer@iyte.edu.tr; Selma Tekir, selmatekir@iyte.edu.tr, Izmir Institute of Technology, Izmir, Turkey, 35430.\n1\narXiv:2110.01804v1  [cs.CL]  5 Oct 2021\n 2\nErhan Sezerer and Selma Tekir\nFinally, we conclude the article with some historical reflections and future remarks. We have also included a diagram\nshowing the milestone papers and summarizing the flow of ideas in the field in Appendix A.\nMultilingual information requirements and parallel/comparable corpora in different languages pave the way for\ncross-lingual representations of words in a joint embedding space. In this survey, we exclude those techniques that\nspecialize in learning word representations in a multilingual setting. The reader can refer to Ruder et al. [119] for a\ncomprehensive overview of cross-lingual word embedding models.\n2\nBACKGROUND\n2.1\nDistributional Hypothesis\nTogether with Wittgenstein [141], Harris [50] were one of the first authors to propose that languages have a distributional\nstructure. He argues that language elements are dispersed to environments that are composed of an existing array\nof their co-occurrents. An element\u2019s distribution is the sum of all these environments. Harris\u2019 second contribution is\nthat we can relate an element\u2019s distribution with its meaning. He states that at least certain aspects of meaning are\ndue to distributional relations. For instance, synonymy between two words can be defined as having almost identical\nenvironments except chiefly for glosses where they co-occur e.g. oculist and eye-doctor. The author also suggests that\nsentences starting with a pronoun should be considered as the same context as the previous sentence where the subject\nof the pronoun is given since their occurrence is not arbitrary and the fullest environmental unit for the distributional\ninvestigation is the connected discourse structures of such sentences.\n2.2\nDistributional Representations\nHinton et al. [52] utilize the idea of distributed representations for concepts. They propose patterns of hidden layer\nactivations (which are only allowed to be 0 or 1) as the representation of meanings. They argue that the most important\nevidence of distributed representations is their degree of similarity to the weaknesses and strengths of human mind.\nUnlike computer memory, human brain is able to retrieve memory from partial information. Distributed representations\nconform to this notion better than local distributions (i.e. bag of words model, where each meaning is associated with a\nsingle computational unit) since the meaning of a word is distributed across several units and a loss of an activation\nwill only slightly effect the memory retrieval process. Rest of the activations that are still there will be able to retrieve\nthe memory. Even if the occlusion of activations are strong enough to lead the system to an incorrect meaning, it will\nstill result in a meaning close to that of the target word, such as instead of apricot the word peach is recalled. Authors\nstate that, this phenomenon further reinforces the idea of being similar to human mind by showing the similarities\nwith deep dyslexia that occurs in adults with certain brain damage.\n2.3\nLanguage Modeling\nLanguage modeling is the task of predicting the next word given a sequence of words. Formally, it is the prediction of\nthe next word\u2019s probability distribution given a sequence of words (Equation 1).\n\ud835\udc43(\ud835\udc65\ud835\udc61+1|\ud835\udc65\ud835\udc61, ...,\ud835\udc651)\n(1)\nIn an alternative interpretation, a language model assigns a probability to a sequence of words. The probability\ncalculation can be formulated as the product of conditional probabilities in each subsequent step having the assumption\n A Survey On Neural Word Embeddings\n3\nthat they are independent (Equation 2).\n\ud835\udc43(\ud835\udc651, ...,\ud835\udc65\ud835\udc61) = \ud835\udc43(\ud835\udc651)\ud835\udc43(\ud835\udc652|\ud835\udc651)\ud835\udc43(\ud835\udc653|\ud835\udc652,\ud835\udc651)...\ud835\udc43(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61\u22121, ...,\ud835\udc651)\n=\n\ud835\udc61\ufffd\n\ud835\udc56=1\n\ud835\udc43(\ud835\udc65\ud835\udc56 |\ud835\udc65\ud835\udc56\u22121, ...,\ud835\udc651)\n(2)\nIn traditional language modeling, the next word\u2019s probability is calculated based on the statistics of n-gram occurrences.\nn-grams are \ud835\udc5b consecutive words. In n-gram language models [22, 62], an n-gram\u2019s probability is computed depending\non the preceding \ud835\udc5b \u2212 1 words instead of using the product of conditional probabilities of bi-grams, tri-grams, etc. to\nsimplify the computation.\nn-gram language models have some issues. When the length of n-grams increases, their occurrence becomes sparse.\nThis sparsity causes zero or division by zero probability values. The former one is resolved by smoothing and back-off is\nused to deal with the latter. Sparsity provides coarse-grained values in the resultant probability distribution. Moreover,\nstoring all n-gram statistics becomes a major problem when the size of \ud835\udc5b increases. This curse of dimensionality is a\nbottleneck for n-gram language models.\n2.4\nDistributional Representations through Language Modeling\nElman [39] was the first to implement the distributional model proposed by Hinton et al. [52], in a language model.\nHe proposes a specific recurrent neural network structure with memory, called the Elman network, to predict bits in\ntemporal sequences. Memory is provided to network through the use of context units that are fully-connected with\nhidden units. He makes a simulation to predict bits in XOR problem. The input sequence is in the form of an input pair\nfollowed by an output bit. In the solution scheme, two hidden units are expected to represent two main patterns in the\nXOR truth table. That is one hidden unit should have high activation for 01 or 10 pattern and the other should recognize\n11 or 00 pattern. As an alternative problem, letter sequences that are generated partially random and partially by a\nsimple rule are tried to be learned by a recurrent neural network where hidden unit activations are used to represent\nword meanings. The idea is that using such network structures, time can be modeled in an implicit way. In other words,\nthe use of a recurrent neural network helps in learning temporal structure in language.\nXu and Rudnicky [143] create the first language model based on neural networks. Their proposed model is based on\na single fully connected layer and uses one-hot vectors of words as inputs and outputs. They highlight computational\ncost as the major problem and in tackling the issue they mention the necessity of update mechanisms which only\nupdate those weights with non-zero input value due to one-hot encoding.\nBengio et al. [7] popularize the distributional representation idea by realizing it through a language model and lead\nto numerous other studies that are built on it. In their model architecture, they use a feed forward network with a single\nhidden layer and optional direct connections from input layer to softmax layer (Figure 1).\nIn addition to the advantages discussed by the aforementioned earlier works, they argue that distributional rep-\nresentations also break the curse of dimensionality in traditional n-gram models ([22], [62]) where the probability\nof each word depends on the discrete n-grams whose numbers can exceed millions. A considerably high number of\nsuch n-grams will highly unlikely to be observed in the training set which results in sparsity problems in conditional\nprobability calculations. A real valued feature vector representation of words will overcome this problem by working\nwith a smooth probability function. The conditional probability of seeing a word given a context is calculated by\nupdating the index of that word on the shared representation matrix of all the vocabulary. The probability function is\nsmooth in that the updates that are caused by similar contexts are alike.\n 4\nErhan Sezerer and Selma Tekir\nFig. 1. Neural network architecture in Bengio et al. [7]. Taken from the original article.\nA second advantage of the model is the ability to capture context-based similarities. In n-gram models, the sentences\n\"the cat is walking in the bedroom\" and \"a dog was running in a room\" will be considered as dissimilar since they are\nunable to consider contexts further than 1 \u2212 2 words and have no notion of similarity among word meanings. On the\nother hand, in the proposed model, increasing the probability of the sentence \"the cat is walking in the bedroom\" will\nincrease the probability of all the sentences below and help us generalize better:\n\"a dog was running in a room\"\n\"the cat is running in a room\"\n\"a dog is walking in a bedroom\"\n3\nWORD EMBEDDINGS WITH IMPROVED LANGUAGE MODELS\nOnce it is shown that neural language models are efficiently computable by Bengio et al. [7], newer language models\nalong with better word embeddings are developed successively. All of these models and their properties are summarized\nin Table 1.\nAlexandrescu and Kirchhoff [2] (FNLM) improve the model proposed by Bengio et al. [7] by including word-shape\nfeatures such as stems, affixes, capitalization, POS class, etc. at input.\nMorin and Bengio [97] focus on improving the performance of the earlier neural language models. Instead of\nusing softmax and predicting the output word over the entire dictionary, they propose a hierarchical organization for\nvocabulary terms. A binary tree of words is created based on the IS-A relation of Wordnet hierarchy. Instead of directly\npredicting each word\u2019s probability, prediction is performed as a binary decision over the constructed tree\u2019s branches\nand leaves. This technique is an alternative to importance sampling to increase efficiency. Although the authors report\n A Survey On Neural Word Embeddings\n5\nexponential speed-up, the accuracy of the resultant word embeddings is a bit worse than the original method and\nimportance sampling.\nMnih and Hinton [94] improve the hierarchical language model proposed by Morin and Bengio [97] by constructing\nand using a word hierarchy from distributional representations of words rather than a hierarchy built out of Wordnet.\nThus, their approach is entirely unsupervised. They calculate feature vectors for words by training a hierarchical\nlog-bilinear model (HLBL) and apply EM algorithm on mixture of two Gaussians to construct a data-driven binary tree\nfor words in the vocabulary. Authors also represent different senses of words as different leaves in the tree which is\nproposed in Morin and Bengio [97] but not implemented. Their model outperforms non-hierarchical neural models, the\nhierarchical neural language model that is based on Wordnet hierarchy, and the best n-gram models ([22], [62]).\nMnih and Hinton [93] propose three different language models that use distributed representation of words. In\nFactored Restricted Boltzmann Machine (RBM), they put an additional hidden layer over the distributed representation\nof the preceding words and exploit interactions between this hidden layer and the next word distributed representation.\nIn temporal RBM, they further put temporal connections among hidden layer units to capture longer dependencies in\nthe previous set of words, and finally in the log-bilinear model, called LBL, they use linear dependencies between the\nnext word and the preceding set of words. They report that the log-bilinear model outscores RBM models and also\nn-gram models ([22], [62]).\nCollobert and Weston [27] and Collobert et al. [28] (C&W) are among the precursors in using distributed represen-\ntations in various NLP problems such as part-of-speech tagging, named entity recognition, chunking, and semantic role\nlabeling. They propose a unified architecture for all of the problems where the words in the sentences are represented\nby word vectors trained from the Wikipedia Corpus in an unsupervised fashion. Although they use a feed forward\narchitecture with a sliding window approach in word-level tasks, they utilize a convolutional neural network (CNN)\narchitecture in semantic role labeling in order to incorporate the varying lengths of sentences, since in semantic role\nlabeling, sliding window-based approaches don\u2019t work because target words may depend on some other far away words\nin a sentence. By using trained word vectors and neural network architecture, their proposed method can capture the\nmeaning of words and succeed in various NLP tasks (almost) without using hand-crafted features. Their overall scheme\nis described as semi-supervised, being composed of unsupervised language modeling and other supervised tasks.\nMikolov et al. [89] propose a recurrent neural network-based language model (RNNLM), from where word repre-\nsentations can be taken. The model is able to consider contexts of arbitrary length, unlike the previous feed-forward\nmethods where a context size should be defined beforehand. The network can learn longer dependencies. It is proved\nuseful in tasks involving inflectional languages or languages with large vocabulary when compared to n-gram language\nmodels ([22], [62]).\n3.1\nEarly Word Embeddings\nWord2vec [88] is the first neural word embedding model that efficiently computes representations to leverage the\ncontext of target words. Thus, it can be considered as the initiator of early word embeddings.\nMikolov et al. [88] propose word2vec to learn high-quality word vectors. The authors removed the non-linearity\nin the hidden layer in the proposed model architecture of Bengio et al. [7] to gain an advantage in computational\ncomplexity. Due to this basic change, the system can be trained using billions of words efficiently. word2vec has two\nvariants: Continuous bag of words model (CBOW) and Skip-gram model.\nIn CBOW, a middle word is predicted given its context, the set of neighboring left and right words. When the input\nsentence \"nature is pleased with simplicity\" is processed, the system predicts the middle word \"pleased\" given the left\n 6\nErhan Sezerer and Selma Tekir\nTable 1. Properties of word embedding models.\nModel\nYear\nDimension\nTraining Corpus\nNN Model\nAim\nKnowledge-Base(s)\nFeature(s)\nBengio et al. [7]\n2003\n100\nBrown\nFFNN\nTraining\n-\n-\nMorin and Bengio [97]\n2005\n100\nBrown\nFFNN\nPerformance\nWordnet[91]\nHierarchical Binary Tree\nFNLM [2]\n2006\n45-64\nLDC ECA[42],\nTurkish News[49]\nFFNN\nTraining\nLDC ECA[42]\nTurkish News[49]\nWord Shape Features\nLBL [93]\n2007\n100\nAPNews\nRBM, FFNN\nTraining\n-\n-\nHLBL [94]\n2008\n100\nAPNews\nLBL\nPerformance\n-\nHierarchical Binary Tree\nC&W [27]\n2008\n15-100\nWiki\nFFNN, CNN\nTraining\n-\n-\nRNNLM [89]\n2010\n60-400\nGigaword\nRNN\nTraining\n-\n-\nCBOW [88]\n2013\n300-1000\nGoogle News\nFFNN\nTraining\n-\n-\nSkip-Gram [88]\n2013\n300-1000\nGoogle News\nFFNN\nTraining\n-\n-\nSGNS [90]\n2013\n300\nGoogle News\nFFNN\nPerformance\n-\nNegative Sampling\nivLBL/vLBL [95]\n2013\n100-600\nWiki\nLBL\nPerformance\n-\nNCE [47]\nGloVe [109]\n2014\n300\nWiki, Gigaword,\nCommoncrawl\nLBL+coocurence Matrix\nTraining\n-\n-\nDEPS [69]\n2014\n300\nWiki\nCBOW\nTraining\nStanford tagger[129]\nDependency parser[43]\nPOS,\nDependency relation\nLing et al. [75]\n2015\n50\nWiki\nCBOW+Attn.\nTraining\n-\n-\nSWE [78]\n2015\n300\nWiki\nSkip-Gram\nTraining\nWordnet[91]\nOrdinal Semantic Rules\nFaruqui et al. [40]\n2015\n-\n-\n-\nfine-tuning\nPPDB[107]\nFrameNet[6]\nWordNet[91]\nSemantic Relations\nYin and Sch\u00fctze [147]\n2016\n200\n-\n-\nEnsemble\n-\n-\nNgram2vec [149]\n2017\n300\nWiki\nSGNS+n-gram\nTraining\n-\n-\nDict2vec [128]\n2017\n300\nWiki\nSkip-Gram\nTraining\nOxford, Cambridge\nand Collins dict.\n-\nand right contexts. Every input word is in one-hot encoding where there is a vocabulary size (\ud835\udc49 ) vector of all zeros\nexcept a one in that word\u2019s index. In the single hidden layer, the average of the neighboring left and right vectors\n(\ud835\udc64\ud835\udc50) is computed to represent the context instead of applying a nonlinear transformation. As the order of words is\nnot considered by averaging, it is named a bag-of-words model. Then the middle word\u2019s (\ud835\udc64\ud835\udc61) probability given the\ncontext (\ud835\udc5d(\ud835\udc64\ud835\udc61 |\ud835\udc64\ud835\udc50)) is calculated through softmax on the context-middle word dot product vector (Equation 3). Finally,\nthe output loss is calculated based on the cross-entropy loss between the system predicted output and the ground-truth\nmiddle word.\n\ud835\udc5d(\ud835\udc64\ud835\udc61 |\ud835\udc64\ud835\udc50) =\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc64\ud835\udc50 \u00b7 \ud835\udc64\ud835\udc61)\n\ufffd\n\ud835\udc57 \u2208\ud835\udc49\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc64\ud835\udc57 \u00b7 \ud835\udc64\ud835\udc61)\n(3)\nIn Skip-gram, system predicts the most probable context words for a given input word. In terms of a language model,\nwhile CBOW predicts an individual word\u2019s probability, Skip-gram outputs the probabilities of a set of words, defined\nby a given context size. Due to high dimensionality in the output layer (all vocabulary words have to be considered),\nSkip-gram has higher computational complexity compared to CBOW. Rather than traversing all vocabulary in the\noutput layer, Skip-gram with Negative Sampling (SGNS) [90] formulates the problem as a binary classification where\none class represents the current context\u2019s probability. In contrast, the other class is connected to all other vocabulary\nterms\u2019 occurrence probability in the present context. In the latter probability calculation, a negative sampling method is\nincorporated [96], which is influenced by Noise Contrastive Estimation (NCE) [47], to speed up the training process.\nAs vocabulary terms are not distributed uniformly in contexts, sampling is performed from a distribution where the\norder of frequency of vocabulary words in corpora is taken into consideration. SGNS incorporates this sampling idea\nby replacing the Skip-gram\u2019s objective function. The new objective function (Equation 4) depends on maximizing\n\ud835\udc43(\ud835\udc37 = 1|\ud835\udc64,\ud835\udc50) where \ud835\udc64,\ud835\udc50 is the word-context pair. This probability denotes the probability of (\ud835\udc64,\ud835\udc50) coming from the\ncorpus data. Additionally, \ud835\udc43(\ud835\udc37 = 0|\ud835\udc62\ud835\udc56,\ud835\udc50) should be maximized if (\ud835\udc62\ud835\udc56,\ud835\udc50) pair is not included in the corpus data. In this\ncondition, (\ud835\udc62\ud835\udc56,\ud835\udc50) pair is sampled, as the name suggests negative sampled \ud835\udc58 times.\n A Survey On Neural Word Embeddings\n7\n\u2211\ufe01\n\ud835\udc64,\ud835\udc50\n\ufffd\nlog\ud835\udf0e\n\ufffd\u2212\u2192\n\ud835\udc64 \u00b7 \u2212\u2192\ud835\udc50\n\ufffd\ufffd\n+\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=1\n\ufffd\nlog\ud835\udf0e\n\ufffd\u2212\u2212\u2192\n\u2212\ud835\udc62\ud835\udc56 \u00b7 \u2212\u2192\ud835\udc50\n\ufffd\ufffd\n(4)\nBoth word2vec variants produced word embeddings that can capture multiple degrees of similarity including both\nsyntactic and semantic regularities.\nMnih and Kavukcuoglu [95] introduce speedups to the CBOW and Skip-gram models [88], called vLBL and ivLBL,\nby using noise-contrastive estimation (NCE) for the training of the unnormalized counterparts of these models. Training\nof the normalized model has a high cost due to the normalization over the whole vocabulary (the denominator term in\nEquation 3). NCE trains the unnormalized model by adapting a logistic regression classifier to discriminate between the\nsamples under the model and the samples from a noise distribution. Thus, the computational cost and accuracy become\ndependent on the number of noise samples. With the relatively small number of noise samples, the same accuracy level\nwith the normalized models is achieved in considerably shorter training times.\nPennington et al. [109] combine global matrix factorization and local context window-based prediction to form\na global log bilinear model called GloVe. GloVe uses ratios of co-occurrence probabilities of words as weights in its\nobjective function to cancel out the noise from non-discriminative words. As distinct from CBOW and Skip-gram [88],\ninstead of cross-entropy, GloVe uses the weighted least squares regression in its objective function. For the same corpus,\nvocabulary, window size, and training time, GloVe consistently outperforms word2vec.\nZhao et al. [149] (ngram2vec) improve word representations by adding n-gram co-occurrence statistics to the SGNS\n[90], GloVe [109], and PPMI models [70]. In order to incorporate these statistics into the SGNS model, instead of just\npredicting the context words, they also predict the context n-gram of words. In order to add it to the other systems,\nthey just add n-gram statistics to the co-occurrence matrix of words. They show improved scores over the models that\nthey are built upon.\nLevy and Goldberg [69] argue that although the word embeddings with Skip-gram are able to capture very useful\nrepresentations, they also learn from unwanted co-occurrences in the context, e.g. Australian and discovers in the\nsentence \"Australian scientist discovers stars with telescope\". In order to create a different context, they use dependency\ntrees to link each word in the sentence to the other according to the relations they have. Their experimental results\nshow that while their model (DEPS) is significantly better at representing syntactic relationships, it is worse at finding\nsemantic relationships. In this work, they also share a non-trivial interpretation of how word embeddings learn\nrepresentations, which is very rare in neural network solutions, by examining the activations of context for specific\nwords.\nLing et al. [75] augment CBOW [88] with an attention model in order to solve the shortcomings of it: Inability to\naccount for word order and lack of treating the importance of context words differently. They show that their method\ncan obtain better word representations than CBOW while still being faster than its complementary model Skip-gram\n[88].\nYin and Sch\u00fctze [147] put forward the idea of ensembling the existing embeddings in order to achieve performance\nenhancement and improved coverage on the vocabulary. They propose four different ensemble approaches on five\ndifferent word embeddings: Skip-Gram [90], Glove [109], Collobert&Weston [27], Huang [55], and Turian [130]. The first\nmethod CONC simply concatenates the word embeddings from five different models. SVD reduces the dimensionality\nof CONC. 1toN creates metaembeddings and 1to\ud835\udc41 + creates out of vocabulary (OOV) words for individual sets by\nrandomly initializing the embeddings for OOVs and the metaembeddings, then uses a setup similar to 1toN to update\nmetaembeddings as well as OOV embeddings. They also propose a MUTUALLEARNING method to solve OOV problem\n 8\nErhan Sezerer and Selma Tekir\nin CONC, SVD, and 1toN. They show that the ensemble approach outperforms individual embeddings on similarity,\nanalogy, and POS tagging tasks.\nThere have been some work to improve early word embeddings through knowledge bases.\nLiu et al. [78] (SWE) try to improve word embeddings by subjecting them with ordinal knowledge inequality\nconstraints. They form three different types of constraints:\n(1) Synonym-antonym rule: A synonym of a word should be more similar than an antonym. They find these pair of\nwords from the WordNet [91] synsets.\n(2) Semantic category rule: Similarity of words that belong to the same category should be larger than the similarity\nof words that are in different categories. i.e. (hacksaw, jigsaw) similarity should be greater than (hacksaw, mallet).\n(3) Semantic hierarchy rule: Shorter distances in hierarchy should infer larger similarities between words compared\nto longer distance cases. i.e (mallet, hammer) similarity should be larger than (mallet, tool).\nThe last two rules are constructed from the hypernymy-hyponymy information from Wordnet. They combine these\nconstraints with the Skip-gram algorithm [90] to train word embeddings and show that they can improve upon the\nbaseline algorithm.\nFaruqui et al. [40] aim to improve word embeddings with information from lexicons with a method called retrofitting.\nThey use a word graph where each word is a vertex, and each relation in the knowledge-base is an edge between words.\nTheir algorithm brings closer the words that are shown to be connected in the word graph and words that are found\nto be similar from the text. In other words, while they bring closer the words related in synsets, they also preserve\nthe similarity in the underlying pre-trained word embeddings (Skip-gram [88], GloVe [109], etc.). They use various\nknowledge-bases such as PPDB [107], WordNet [91], and FrameNet [6].\nTissier et al. [128] (dict2vec) improve word2vec [90] by incorporating dictionary information in the form of strong\nand weak pair of words into the training process. If a word \ud835\udc4e is in the definition of the word \ud835\udc4f in dictionary and \ud835\udc4f is\nin the definition of \ud835\udc4e too, then it is a strong pair. On the other hand, if \ud835\udc4e is in the definition of \ud835\udc4f but \ud835\udc4f is not in the\ndefinition of \ud835\udc4e, then they form a weak pair. The authors add this positive sampling information into the training process\nproportional to hyperparameters.\nDespite the success of these earlier word embeddings, there were still many limitations in terms of the accuracy\nof representations, each of which is targeted by many research works. In the succeeding subsections, we discuss\nthese limitations (such as morphology, senses, antonymy/synonymy, and so on) with the proposed solutions from the\nliterature.\n3.2\nEmbeddings Targeting Specific Semantic Relations\nAlthough the initial word embedding models successfully identified semantic and syntactic similarities of words,\nthey still need to be improved to address specific semantic relations among words such as synonymy-antonymy and\nhyponymy-hypernymy. To illustrate, consider the sentences \"She took a sip of hot coffee\" and \"He is taking a sip of cold\nwater.\" The antonyms \"cold\" and \"hot\" are deemed to be similar since their context is similar. Therefore, it becomes an\nissue to differentiate the synonyms \"warm\" and \"hot\" from the antonyms \"cold\" and \"hot\" considering they have similar\ncontexts in most occurrences.\nTable 2 presents the main approaches addressing synonym-antonym relations, hyponym-hypernym relations, and a\nstudy covering all types of relations.\n A Survey On Neural Word Embeddings\n9\nTable 2. Embeddings targeting specific semantic relations.\nWork\nBase Model\nYear\nKnowledge-Base\nMorphological\nFeatures\nSpecific Semantic Relations\ndLCE [104]\nSGNS [90])\n2016\nWordNet [91] and Wordnik\n\u2717\nSynonym-Antonym\nMrk\u0161i\u0107 et al. [98]\nGloVe [109] and\nparagram-SL999 [139]\n2016\nWordNet [91] and PPDB 2.0 [107]\n\u2717\nSynonym-Antonym\nVuli\u0107 et al. [134]\nSGNS [90])\n2017\n\u2717\n\u2713\nSynonym-Antonym\nYu et al. [148]\n\u2717\n2015\nProbase [142]\n\u2717\nHyponym-Hypernym\nLuu et al. [85]\n\u2717\n2016\nWordNet [91]\n\u2717\nHyponym-Hypernym\nNguyen et al. [103]\nSGNS [90])\n2017\nWordNet [91]\n\u2717\nHyponym-Hypernym\nWang et al. [136]\nSkip-gram [88]\n2019\n\u2717\n\u2717\nSynonym-Antonym,\nHyponym-Hypernym, Meronym\nNguyen et al. [104] propose a weight update for SGNS [90] to identify synonyms and antonyms from word embeddings.\nTheir system (dLCE) increases weights if there is a synonym in the context and makes a reduction in the case of an\nantonym. In order to come up with a list of antonyms and synonyms, they use WordNet [91] and Wordnik. They report\nstate-of-the-art results in similarity tasks and synonym-antonym distinguishing datasets.\nMrk\u0161i\u0107 et al. [98] propose a counter-fitting method to inject antonymy (REPEL) and synonymy (ATTRACT) constraints\ninto vector space representations to improve word vectors. The idea behind the ATTRACT rule is that synonymous\nwords should be closer to each other than any other word in the dictionary. Similarly, the REPEL constraint assumes that\nan antonym of a word should be farther away from the word than any other word in the dictionary. As knowledge-bases,\nthey use WordNet [91] and PPDB 2.0 [107], and as pre-trained word vectors they use GloVe [109] and paragram-SL999\n[139]. They report state-of-the-art results on various datasets.\nVuli\u0107 et al. [134] use ATTRACT and REPEL constraints on pretrained word embeddings. Their algorithm aims to\npull together ATTRACT pairs while pushing REPEL pairs apart. To form the ATTRACT and REPEL constraints, the\ninflectional and derivational morphological rules of four languages are used; English, Italian, Russian, and German.\nATTRACT constraints consist of suffixes such as (-s, -ed, -ing) to create ATTRACT word pairs such as (look, looking),\n(create, created). On the other hand, REPEL constraints consist of prefixes like (il, dis, anti, mis, ir,..) to create REPEL\nword pairs such as (literate, illiterate), (regular, irregular). In order to balance the changes they make to the original\nembeddings (they use SGNS [90]), there is a third constraint that tries to pull word embeddings to their original\npositions.\nIn their work, Yu et al. [148] train term embeddings for hypernymy identification. They use Probase [142] as their\ntraining data for hypernym/hyponym pairs and impose three constraints on the training process: 1) hypernyms and\nhyponyms should be similar to each other (dog and animal), 2) co-hyponyms should be similar (dog and cat), 3)\nco-hypernyms should be similar (car and auto). They create a neural network architecture to update word embeddings\nwithout optimizing parameters. They use 1-norm distance as a similarity measure. They use an SVM on the output\nterm embeddings to decide whether a word is a hypernym/hyponym to another word.\nLuu et al. [85] aim to identify is-a relationship through neural network architecture. First, they extract hypernyms\nand hyponyms using the relations in WordNet [91] to form a training set. Second, they create (hypernym, hyponym,\ncontext word) triples by finding all sentences in the dataset containing two hypernym/hyponyms found in the first step\nand using the words between the hypernym and hyponym as context words. Then, they give hyponym and context\nwords as input to the neural network and try to predict the hypernym by aggregating them with a feed-forward neural\nnetwork. The resultant hypernym, hyponym pairs along with an offset vector are given to SVM to predict whether\n 10\nErhan Sezerer and Selma Tekir\nthere is an is-a relationship or not. The authors state that since their method takes context words into account, their\nembeddings have good generalization capability and are able to identify unseen words.\nNguyen et al. [103] aim to learn hierarchical embeddings for hypernymy. They leverage hypernymy-hyponymy\ninformation from WordNet [91] and propose objective functions over/above SGNS embeddings [90] to move hypernymy-\nhyponymy pairs closer. The first objective function is based on the distributional inclusion hypothesis, while the second\nadopts distributional informativeness. They also propose an unsupervised hypernymy measure to be used by their\nhierarchical embeddings. In the proposed hypernymy measure, the cosine similarity between the hypernym and\nhyponym vectors (to detect the hypernymy) is multiplied by the hypernym to hyponym magnitude ratio (to account\nfor the directionality of the relation by the assumption that hypernyms are more general terms, being more frequent\nand thus having a large magnitude compared to hyponyms). Their evaluation also tests the generalization capability of\ntheir hypernymy solution, which proves that the model learns rather than memorizes prototypical hypernyms.\nWang et al. [136] propose a neural representation learning model for predicting different types of lexical relations,\ne.g., hypernymy, synonymy, meronymy, etc. Their solution avoids the \"lexical memorization problem\" because relation\ntriples\u2019 embeddings are learned rather than computing those relations through individual word embeddings. In order\nto learn a relation embedding for a pair of words, they use the Skip-gram model [88] over the neighborhood pairs\nwhere the similarity between pairs is defined on hyperspheres. Their lexical relation classification results verify the\neffectiveness of their approach.\n3.3\nSense Embeddings\nAnother drawback of early word embeddings is they unite all the senses of a word into one representation. In reality,\nhowever, a word gets meaning in its use and can mean different things in varying contexts. For example, even though\nthe words \"hot\" and \"warm\" are very similar when they are used to refer to temperature levels, they are not similar in\nthe sentences \"She took a sip of hot coffee\" and \"He received a warm welcome\". In the transition period to contextual\nembeddings, different supervised and unsupervised solutions are proposed for having sense embeddings.\nSch\u00fctze [121] was the first work aimed at identifying senses in texts. He defines the problem of word sense discrimina-\ntion as the decomposition of a word\u2019s occurrences into same sense groups. This definition is unsupervised in its nature.\nWhen the issue becomes labeling those sense groups, the task becomes a supervised one and is named as word sense\ndisambiguation. The reader can refer to Navigli [99] for a comprehensive survey on word sense disambiguation and\nCamacho-Collados and Pilehvar [19] for an in-depth examination of sense embedding methods and their development.\nTable 3 provides a classification of the studies that we analyze in this section. The classification dimensions include\nunsupervised/supervised, topical or not, knowledge base, probabilistic approach, exploiting syntactic information or\nnot, and neural network (NN) model.\nAt the outset, unsupervised learning is used to discriminate the different senses of a word.\nReisinger and Mooney [115] propose a multi-prototype based word sense discovery approach. In their approach\n(R&M), a word\u2019s all occurrences are collected as a set of feature vectors and are clustered by a centroid-based clustering\nalgorithm. The resultant clusters (fixed number) for each word are expected to capture meaningful variation in word\nusage rather than matching to traditional word senses. They define the similarity of words \ud835\udc34 and \ud835\udc35 as the \"maximum\ncosine similarity between one of A\u2019s vectors and one of B\u2019s vectors\" and provide experimental evidence on similarity\njudgments and near-synonym prediction. Moreover, variance in the prototype similarities is found to predict variation\nin human ratings.\n A Survey On Neural Word Embeddings\n11\nTable 3. Sense embeddings.\nUnsupervised\nR&M[115]\nSupervised\nWork\nTopical\nKnowledge Base\nProbabilistic\nSyntactic\nNN Model\nInformation\nHuang et al. [55]\n\u2717\n\u2717\nSpherical k-means\n\u2717\nCustom Language Model using\nboth local and global context\nPelevina et al. [108]\n\u2717\n\u2717\nGraph clustering on ego network\n\u2717\nCBOW [88]\nTWE [80]\n\u2713\n\u2717\nLDA [11]\n\u2717\nSkip-gram [88]\nSenseEmbed [57]\n\u2717\nBabelNet [100]\n\u2717\n\u2717\nCBOW [88]\nChen et al. [23]\n\u2717\nWordNet [91]\nContext clustering\n\u2717\nCNN\nJauhar et al. [58]\n\u2717\nWordNet [91]\nExpectation-Maximization (EM)\n\u2717\nSkip-gram [88]\nChen et al. [24]\n\u2717\nWordNet [91]\n\u2717\n\u2717\nSkip-gram [88]\nTian et al. [127]\n\u2717\n\u2717\nMixture of Gaussians (EM)\n\u2717\nSkip-gram [88]\nNieto Pi\u00f1a and Johansson [105]\n\u2717\nSALDO [14]\n\u2717\n\u2717\nSkip-gram [88]\nMSSG [101]\n\u2717\n\u2717\n\u2713\n\u2717\nSkip-gram [88]\nSAMS [26]\n\u2717\n\u2717\n\u2717\n\u2713\nRecursive Neural Network\nLi and Jurafsky [71]\n\u2717\n\u2717\nChinese Restaurant Process (CRP)\n\u2717\nCBOW-Skip-gram [88], SENNA [28]\nMSWE [102]\n\u2713\n\u2717\nLDA [11]\n\u2717\nSkip-gram [88]\nGuo et al. [46]\n\u2717\n\u2717\nAffinity Propagation Algorithm\n\u2717\nRNNLM model [89]\nLSTMEmbed [56]\n\u2717\nBabelNet [100]\n\u2717\n\u2717\nLSTM\nKumar et al. [63]\n\u2717\nKnowledge Graph\nEmbedding\n\u2717\n\u2717\nFramework consisting of\ndifferent types of Encoders\nFollowing Reisinger and Mooney [115], Huang et al. [55] also aim at creating multi-prototype word embeddings.\nThey compute vectors using a feed forward neural network architecture with one layer to produce single prototype\nword vectors and then perform spherical k-means to cluster them into multiple prototypes. They also introduce the idea\nof using global context where the vectors of words in a document are averaged to create a global semantic vector. The\nfinal score of embeddings is then calculated as the sum of scores of each word vector along with the global semantic\nvector.\nThe authors also argue that available test sets for similarity measurements are not sufficient for testing multi-\nprototype word embeddings because the scores of word pairs in those test sets are given in isolation, which lacks the\ncontextual information for senses. Therefore, they introduce a new test set in which the word pairs are scored within a\ncontext by mechanical turkers, where context is usually a paragraph from Wikipedia that contains the given word.\nFinally, they show that their model is capable of outperforming the former models when such a test set is used, although\nits performance is similar to others in previous test sets.\nPelevina et al. [108] aim at creating sense embeddings without using knowledge bases. Their model takes the existing\nsingle-prototype word embeddings and transforms them into multi-prototype sense embeddings by constructing an\nego network and performing graph clustering over it. In fact, the senses of a word they learn do not have to correspond\nto the senses of that word in the dictionary. They evaluate their method on their crowd-sourced dataset.\nLiu et al. [80] propose three different methods to create topical embeddings (TWE). They create their topical\nembeddings without the use of any knowledge base, but instead rely on LDA [11] to find the topics of each document\nthe word occurs in. Topical embeddings they create are similar to sense embeddings with the only difference being that\nthe number of topics may not correspond to the number of senses in the dictionary.\nIn their first model, named TWE-1, they learn word embeddings and topic embeddings separately and simultaneously\nwith the skip-gram method by treating topic embeddings as pseudo-words, which appear in all the positions of words\nunder this topic. The sense embeddings of a word \ud835\udc64 for topic \ud835\udc61 are then constructed by concatenating the word\n 12\nErhan Sezerer and Selma Tekir\nembedding \ud835\udc64 with the corresponding topic embedding \ud835\udc61. Their second model TWE-2 treats word embeddings and topic\nembeddings as tuples and train them together. This method may lead to sparsity issues since some words on a specific\ntopic may not be frequent. The last method they propose, TWE-3, also train word and topic embeddings together, but\nthis time the weights of embeddings are shared over all word-topic pairs. They show that the TWE-1 method gives\nthe best results overall, and the independence assumption between words and topics in the first model is given as the\nreason behind its performance.\nExploiting vast information in knowledge bases to learn sense representations has proved useful. The approaches\nthat rely mainly on knowledge bases to compute sense embeddings include Iacobacci et al. [57], Chen et al. [23], Jauhar\net al. [58], and Chen et al. [24].\nIacobacci et al. [57] (SenseEmbed) use BabelNet [100] as a knowledge-base to retrieve word senses and to tag words\nwith the correct sense. They train the sense-tagged corpora on the CBOW architecture and achieve state-of-the art\nresults in various word similarity and relatedness datasets.\nChen et al. [23] also use a knowledge-base (WordNet) to solve the sense-embedding problem. They use CNN to\ninitialize sense-embeddings from the example sentences of synsets in WordNet. Then, they apply context clustering to\ncreate distributed representations of senses. The representations they obtain achieve promising results.\nJauhar et al. [58] propose two models for learning sense-embeddings using ontological resources like WordNet\n[91]. In their first model, they retrofit pretrained embeddings by imposing two conditions on them: pulling together\nthe words that are ontologically-related (by using the graphs constructed from the relationships in WordNet) and\nleveraging the tension between sense-agnostic neighbors from the same graph. They implement the first method over\nSkip-gram [90] and Huang et al. [55] and show that their method can improve the success of the previous methods.\nTheir second method constructs embeddings from scratch by training them with an Expectation-Maximization (EM)\nobjective function that pulls together ontologically-related words similar to the first model and finds the correct sense\nof the word from WordNet and creates a vector for each sense.\nChen et al. [24] propose a unified model for word sense representation (WSR) and word sense disambiguation (WSD).\nThe main idea behind this is that both models may benefit from each other. Their solution is composed of three steps:\nFirst, they initialize single-prototype word vectors using Skip-gram [90] and initialize the sense embeddings using the\nglosses in WordNet [91]. They take the average of words in WordNet synset glosses to initialize the sense embeddings.\nSecond, they perform word sense disambiguation using some rules on the given word vectors and sense vectors. Finally,\nusing the disambiguated senses, they learn sense vectors by modifying the Skip-gram objective such that both context\nwords and context words\u2019 senses must be optimized given the middle word in context.\nTian et al. [127] propose a probabilistic approach to provide a solution to sense embeddings. They improve the\nSkip-gram algorithm by introducing the mixture of Gaussians idea to represent the given middle word in context in the\nobjective function. Every Gaussian represents a specific sense, and the mixture is their multi-prototype vector. The\nnumber of Gaussians, in other words, the number of senses, is a hyperparameter of the model. They use Expectation-\nMaximization (EM) algorithm to solve the probabilistic model.\nNieto Pi\u00f1a and Johansson [105] extend the Skip-gram [88] method to find sense representations of words. They get\nthe number of senses from a knowledge-base and for each word in the training corpus, they find the most probable\nsense by using the likelihoods of context words. They only train the sense with the highest probability. They train their\nsystem on Swedish text and measure their success by comparing the senses to the ground-truth in the knowledge-base\n(SALDO [14]).\n A Survey On Neural Word Embeddings\n13\nNeelakantan et al. [101] (MSSG) also aim at creating word vectors for each sense of a word. Unlike most other\nmodels, they do it by introducing the sense prediction into the neural network and jointly performing sense vector\ncalculation and word sense discrimination. Their first model relies on Skip-gram and induces senses by clustering the\ncontext word representations around each word. Then, the word is assigned to the closest sense by calculating the\ndistance to the sense clusters\u2019 centers. Here the count of clusters is the same for all words and is a hyperparameter.\nTheir second model is a non-parametric variant of the first one where a varying number of senses is learned for each\nword. A new cluster (sense) for a word type is created with probability proportional to the distance of its context to the\nnearest cluster (sense). They show that their second method can outperform the first since it can better learn the senses\u2019\nnature.\nCheng and Kartsaklis [26] consider capturing syntactical information to better address senses. They use recursive\nneural networks on parsed sentences to learn sense embeddings. Each input is disambiguated to its sense by calculating\nthe average distance of the words\u2019 embeddings in the sentence to sense cluster means. They define two negative\nsampling methods to train the network. One negative example is created to swap the target word with a random word\n(as in [90] and [47]), another negative sampling changes the order of words in a sentence, which further enforces the\nmodel (SAMS) to learn syntactic dependencies.\nLi and Jurafsky [71] decide the number of senses in an unsupervised fashion by using the Chinese Restaurant Process\n(CRP). They combine CRP with neural network training methods by determining the sense of a word by looking at its\ncontext. They also compare sense-embedding methods with single-prototype models across various NLP tasks to see if\nthey are beneficial. They state that in some tasks (POS tagging, semantic relatedness, semantic relation identification),\nsense-embeddings outperform single-prototype methods. Still, they fail to improve their scores on some other tasks\n(NER, sentiment analysis).\nInstead of getting the number of senses from a knowledge-base, Nguyen et al. [102] (MSWE) use LDA [11] to find\nword to topic and topic to document probability distributions. Here the number of topics is a parameter to the model.\nThey train different weights for each sense of a word using two different optimization methods. The first model learns\nword vectors based on the most suitable topic. On the other hand, their second model considers all topics to learn them.\nThey conclude that this second method can be considered as a generalization of the Skip-gram model [88] given the\nfact that it behaves as Skip-gram if the mixture weights are set to zero.\nGuo et al. [46] exploit bilingual resources to find sense embeddings, motivated by the idea that if a word in a\nsource language translates into multiple words in a target language, that means different words in the target language\ncorresponds to a sense in the source language. For this purpose, they use Chinese to English translation data to induce\nsenses in an unsupervised fashion. They represent the initial words with the word embeddings from C&W [27] and\nuse the affinity propagation algorithm to cluster the translated words into dynamic clusters, which means that their\nmethod can learn a different number of senses for each word. Then, they use the RNNLM model [89] to train the sense\nembeddings.\nIacobacci and Navigli [56] propose an LSTM-based architecture (LSTMEmbed) to jointly learn word and sense\nembeddings. Input contexts are provided from semantically annotated data, and one bidirectional LSTM processes\nthe left context while another one handles the right one. As an extra layer, the concatenation of both outputs is\nlinearly projected into a dense representation. Then, the optimization objective tries to maximize the similarity between\nthe produced dense output and pretrained word embeddings from SGNS. Consideration of these pretrained word\nembeddings in the final phase increases the vocabulary use of the proposed system. Their experiments on the word to\nsense similarity and word-based semantic evaluations prove the usefulness of their approach.\n 14\nErhan Sezerer and Selma Tekir\nTable 4. Morpheme embedding models.\nModel\nYear\nTraining Corpus\nKnowledge-Base\nNN Model\nDimension\nLuong et al. [84]\n2013\nWiki\nMorfessor[31]\nrecNN\n50\nCLBL [15]\n2014\nACL MT\nMorfessor[31]\nLBL\n-\nQiu et al. [111]\n2014\nWiki\nMorfessor[31],Root,Syllable[72]\nCBOW\n200\nBian et al. [9]\n2014\nWiki\nMorfessor[31], WordNet[91],\nFreebase[13], Longman Dict.\nCBOW\n600\nCharWNN [38]\n2014\nWiki\n-\nCNN\n100\nKNET [32]\n2015\nWiki\nMorfessor[31], Syllable[72]\nSkip-Gram\n100\nAutoExtend [117]\n2015\nGoogle News\nWordNet [91]\nAutoencoder\n300\nMorph-LBL [29]\n2015\nTIGER [16]\nTIGER [16]\nLBL\n200\nSoricut and Och [124]\n2015\nWiki\n-\nSkip-Gram\n500\nC2W [74]\n2015\nWiki\n-\nbiLSTM[44]\n50\nCotterell et al. [30]\n2016\nWiki\nCELEX [48]\nGGM\n100\nFasttext [12]\n2016\nWiki\n-\nSkip-Gram\n300\nchar2vec [20]\n2016\ntext8 (wiki)\n-\nLSTM[53]+Attn\n256\nKim et al. [61]\n2016\nACL MT\n-\nCNN+LSTM\n300-650\nLMM [144]\n2018\nGigaword\nMorfessor[31]\nCBOW\n200\nKumar et al. [63] propose a framework that combines a context encoder with a definition encoder to provide sense\npredictions for out of vocabulary words. In the case of rare and unseen words, most word sense disambiguation (WSD)\nsystems rely on the most frequent sense (MFS) on the training set. In the part of the definition encoder, sentence\nencoders along with knowledge graph embeddings are utilized. Here instead of using discrete labels for senses, the score\nfor each sense in the inventory is calculated by the dot product of the sense embedding with the projected context-aware\nembedding.\n3.4\nMorpheme Embeddings\nThe quest for morphological representations is a result of two important limitations of earlier word embedding models.\nThe first point is, words are not the smallest units of meaning in languages, morphemes are. Even if a model does not see\nthe word unpleasant in the training it should be able to deduce that it is the negative form of pleasant. Word embedding\nmethods that don\u2019t take morphological information into account can not produce any results in such a situation. The\nsecond limitation is the data scarcity problem of morphologically rich languages and agglutinative languages. Unlike\nEnglish, morphologically rich languages have many more noun and/or verb forms inflected by gender, case, or number,\nwhich may not exist in the training corpora. The same thing is also valid for agglutinative languages in which words can\nhave many forms according to the suffix(es) they take. Therefore, models that take morphemes/lexemes into account is\nneeded.\nResearchers propose several ways to target morphological information in order to obtain sub-word information for\nsolving the rare/unknown word problem of earlier word embedding methods and also to have better representations of\nwords for morphologically rich languages. While some of the works are proposed to train embeddings directly from\nmorphemes/lexemes, others adjust the representations of other word embedding models. Summary of these models and\ntheir properties can be seen in Table 4.\n A Survey On Neural Word Embeddings\n15\n3.4.1\nTraining Morphological Embeddings from Scratch. There are two main ways for training morpheme embeddings\nfrom scratch: While some methods ([84], [15], [111], [9], [32], [29], [144], [124]) propose to use tools or special rules for\ndissecting a text to its morphemes, others ([12], [20], [74], [38]) prefer using characters or character n-grams as input\nto learn morphemes along with their representations.\nLuong et al. [84]\u2019s work is the first work that attempts to incorporate morphological information in word embeddings.\nThey train morphological embeddings with recursive neural networks. They divide words into (prefix, stem, affix) tuples\nby using morfessor [31] and feed them to a recursive neural network. Word embeddings are then constructed by a\nword-based neural language model (NLM). Instead of initializing the vectors with random numbers, they initialize them\nwith the pre-trained word embeddings from Collobert et al. [28] and Huang et al. [55] in order to focus on learning the\nmorphemic semantics.\nSimilar to Luong et al. [84], Botha and Blunsom [15] (CLBL) also use morfessor [31] to find the morphemes of words\nin text and train both the target word and context words by first factoring them into their morphemes. They learn\nthe morphology-based word representations with an additive-LBL of their factor embeddings, e.g., surface form, stem,\naffixes, etc.\nQiu et al. [111] incorporate morphemes into the CBOW [88] architecture: Instead of predicting a word from the\ncontext words, they propose to use both morphemes and words as input and for prediction. They control the relative\ncontributions of words and morphemes with two parameters that weigh the information to be extracted from each\ninput. They use three different tools for extracting morphemes from corpus: Morfessor [31], root, and syllable [72].\nBian et al. [9] investigate three different methods for finding better representations for words and morphemes: First,\nthey transform CBOW [88] into a new basis by using morphemes (segmented by using morfessor [31]) instead of\nwords. They later represent words as the aggregate of the morphemes they are composed of. Second, they provide\nadditional information to their first model by feeding semantic and syntactic information vectors as inputs along with\nthe morpheme vectors. As semantic and syntactic information, they use synsets, syllables, syntactical transformation,\nand antonym and synonyms from Freebase [13], WordNet [91], and Longman dictionaries1. Finally, they use syntactic\nknowledge (POS tagging vector) and semantic knowledge (entity vector and relation matrix) as auxiliary tasks, where\nthey use syntactic/semantic information as outputs around the center word to be predicted. Their relation matrix\nconsists of relations such as belong-to and is-a relation. They examine the effects of both semantic and syntactic\ninformation compared to the baseline model (CBOW) and report the relative effects in various tasks.\nSoricut and Och [124] aim at improving word vectors and solving the rare word problem by using morphology\ninduction. In their method, they first extract candidate morphological rules. In this step, they find the word pairs\n(\ud835\udc641,\ud835\udc642) such that \ud835\udc642 is formed by substituting prefixes and suffixes up to 6 characters from \ud835\udc641 (i.e., (\ud835\udc4f\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc51,\ud835\udc4f\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc54) is\nproduced from the rule (\ud835\udc60\ud835\udc62\ud835\udc53 \ud835\udc53 \ud835\udc56\ud835\udc65 : \ud835\udc52\ud835\udc51 : \ud835\udc56\ud835\udc5b\ud835\udc54)). Later they form their rules from word pairs. After training their embeddings\nwith the Skip-gram method [88], they keep the rule if the word pair (\ud835\udc641,\ud835\udc642) is similar in embedding space; otherwise,\nthe rule is removed from the candidate rule list. Thus, they use their morphological rules to obtain representations for\nrare words that may or may not be in the training set.\nCui et al. [32] (KNET) use co-occurrence statistics to construct word embeddings with sub-word information. They\nleverage four different morphological information inspired by the advances in cognitive psychology: i) edit distance\nsimilarity ii) longest common sub-string similarity, iii) morpheme similarity (share roots, affixes, etc. by using morfessor\n[31]), and iv) syllable similarity (by using hyphenation tool [73]). They combine the aforementioned morphological\n1www.longmandictionariesonline.com\n 16\nErhan Sezerer and Selma Tekir\ninformation into a relation matrix and construct morphological embeddings from it. On the other hand, they also\ncreate word embeddings by using the Skip-gram method [90]. A combination of these two embeddings with weighted\naveraging is used in order to obtain the final word embeddings. Unlike most other word embedding methods, authors\ndo not change the digits in the text with zeros; instead, they change the digits with their text counterparts to reflect the\ninformation better.\nDifferent from other morphology-based models, Cotterell and Sch\u00fctze [29] implement a semi-supervised approach\n(MorphLBL) where a partially morphologically tagged dataset (TIGER dataset of German newspaper [16]) is used.\nThey augment the LBL model [93] to both predict word and morpheme together. They also introduce a new metric for\nmeasuring the success of morphological models called MorphDist.\nDos Santos and Zadrozny [38], Ling et al. [74], Bojanowski et al. [12], and Cao and Rei [20] come up with character-\nbased solutions instead of using a tool/knowledge-base to find morphemes in sentences.\nIn their work (CharWNN), Dos Santos and Zadrozny [38] use word embeddings together with character embeddings\nto compensate for the need for hand-crafted features in part-of-speech (POS) tagging, where the morphological structure\nof words plays a significant role. In their architecture, they use Skip-gram [88] for word embeddings and train their\ncharacter embeddings from scratch.\nThe compositional model of Ling et al. [74], called C2W, takes the characters of a word as input and uses bidirectional-\nLSTM to construct word vectors by concatenating the last state of LSTM in each direction.\nBojanowski et al. [12] propose a model, called Fasttext, that takes character 3- to 6-grams of words and represents\nthe words with a bag of n-grams. i.e., for the word \u201cwhere\u201d the 3-grams are: (<wh, whe, her, ere, re>), where < and\n> are special characters for denoting the beginning and end of the word, respectively. N-grams are then summed to\nproduce word embeddings. Thus, as the model shares representations across words, it can have better representations\nfor rare words. They perform extensive tests on morphologically rich languages to see how their model works and\nlearns the subword information.\nCao and Rei [20] aim at solving unsupervised morphology induction and learning word embeddings jointly by using\nbidirectional LSTMs with the Bahdanau attention [4] on characters. The output of the attention layer is fed to Skip-gram\n[88] algorithm to compute word representations. They prove that the attention layer learns to split the words into\nmultiple morphemes by showing that their algorithm outperforms other morpheme induction methods. However, it is\nnot only designed for solving that problem. They also show that since their method (char2vec) focused on finding\nmorpheme representations through characters, it is better at tasks that measure syntactic similarity. On the other hand,\nthey argue that their method is worse at tasks that measure semantic similarity since characters do not convey any\nsemantic information of words alone.\nTo address both syntactic and semantic features, Kim et al. [61] use a mixture of character and word-level features.\nIn their model, at the lowest level of the hierarchy, character-level features are processed by a CNN; after transferring\nthese features over a highway network, high-level features are learned using an LSTM. Thus, the resulting embeddings\nshow good syntactic and semantic patterns. For instance, the closest words to the word richard are returned as eduard,\ngerard, edward, and carl, where all of them are person names and have a high syntactic similarity to the query word.\nDue to character-aware processing, their models are able to produce good representations for out-of-vocabulary words.\nXu et al. [144] (LMM) also aim at enhancing word representations with morphological information. In incorporating\nmorphological information, the authors suggest using the latent meaning of morphemes instead of morphemes\nthemselves. They state that although the words \ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc4f\ud835\udc59\ud835\udc52 and \ud835\udc62\ud835\udc5b\ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc4f\ud835\udc59\ud835\udc52 have similar semantics, the methods based\n A Survey On Neural Word Embeddings\n17\non morphemes cannot catch it. Instead, they use the latent meaning of morphemes that they extract from knowledge-\nbases (i.e. in=not, un=not, ible=able, able=able, cred=believe, believ=believe). They use CBOW [88] as pretrained word\nembeddings and show improvements using their method on them.\n3.4.2\nAdjusting the Existing Embeddings. Among the models that adjust the pre-trained word embeddings, Rothe and\nSch\u00fctze [117] take any word embeddings and transform them into embeddings for lexemes and synsets. To do that,\nthey use WordNet [91] synsets and lexemes although they note that their model (AutoExtend) can get the information\nfrom the other knowledge bases such as Freebase [13]. They consider words and synsets as the sum of their respective\nlexemes and enforce three constraints on the system i) synset constraint, ii) lexeme constraint, and iii) WordNet\nconstraint (because some synsets contain only a single word). They use an autoencoder where the result of the encoding\ncorresponds to synset vectors, and the hidden layer in encoding and its counterpart in decoding correspond to lexeme\nvectors. Two lexeme vectors are then averaged to produce the final lexeme embeddings.\nOn the other hand, Cotterell et al. [30] use a Gaussian graphical model where word embeddings are represented\nas the sum of their morphemes. Their system takes the output of the other word embedding methods as input and\nconverts them by learning their morpheme embeddings and calculating the word embeddings by summing them. They\nalso note that it is also possible to extrapolate the embeddings of OOV words with their method since one can compute\ntheir morpheme embeddings from the same morpheme in other words.\n4\nCONTEXTUAL REPRESENTATIONS\nAs it is shown in the last section, many methods have been proposed for solving the deficiencies of embedding\nmethods. Each of them is specialized on a single problem such as sense representation, morpheme representation,\netc., while none of them was able to combine different aspects together into a single model, a single solution. It is\nthe idea of contextual representations to provide a solution that covers each aspect successfully. The main idea behind\ncontextual representations is that words should not have a single representation to be used in every context. Instead,\na representation should be calculated separately for different contexts. Contextual representation methods calculate\nthe embedding of a word from the surrounding words each time the word is seen, contrary to the earlier methods\nwhere each word is represented with a fixed vector of weights. This leads to an implicit solution to many problems\nsuch as sense representations, antonymy/synonymy, and hypernymy/hyponymy, since now multi-sense words can\nhave different representations according to their context. Furthermore, it has also been proposed to use characters as\ninput which also incorporates the sub-word information into embeddings. Therefore, contextual representation models,\ndescribed below, are able to incorporate different aspects together into a single model. Liu et al. [79] examine contextual\nembeddings in detail by comparing their pre-training methods, objectives, and downstream learning approaches.\nIn such a first attempt to create contextual representations, Melamud et al. [87] developed a neural network architec-\nture based on bidirectional-LSTMs to learn context embeddings with target word embeddings jointly. They feed words\nto a 2-layer bidirectional LSTM network in order to predict a target word in a sentence. They use sentences as context\nand feed the left side of the target word to left to right (forward) biLSTM and feed the right side of the target word to\nright to left (backward) biLSTM. To jointly learn context and target word embeddings, they use the Skip-gram objective\nfunction sampled on context-word occurrences. Furthermore, they show that this is equivalent to the factorization of a\ncontext-target word co-occurrence matrix. Although the previous word embedding models create both context and\ntarget word embeddings, they only use target-target similarity as representations and ignore the context embeddings. In\nthis work, the authors also use context-context and context-target to show that contextual embeddings can significantly\n 18\nErhan Sezerer and Selma Tekir\nimprove NLP systems\u2019 performance. They also show that since bidirectional LSTM structures can learn long-term\ncontextual dependencies, their model, context2vec, is able to differentiate polysemous words with a high success rate.\nCoVe [86] uses Glove [109] as the initial word embeddings and feeds them to a machine translation architecture\nto learn contextual representations. The authors argue that pre-training the contextual representations on machine\nlearning tasks, where there are vast amounts of data, can lead to better contextual representations to transfer learning\nto other downstream tasks. They concatenate the output of the encoder of a machine translation model (as contextual\nembeddings) with the GloVe embeddings to construct their final word representations.\nUsing language modeling and learning word representations as to the pre-training objective, then fine-tuning the\narchitecture to downstream tasks is first proposed by Dai and Le [33] and Howard and Ruder [54]. While Dai and Le\n[33] propose to use RNNs and autoencoders to tackle the issue, ULMFiT [54] introduces novel fine-tuning ideas such as\ndiscriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing to their LSTM model, inspired\nfrom the advances in transfer learning in computer vision. After the success shown by these models, the aim is shifted\nfrom creating word representations to using their system as pre-trained models and then fine-tuning a classifier on top\nto perform downstream tasks.\nELMO [110] improves on the character-aware neural language model by Kim et al. [61]. The architecture takes\ncharacters as input to a CNN network from where it is fed to a 2-layer bidirectional-LSTM network to predict a target\nword. They show that this architecture can learn various aspects of words such as semantic, syntactic, and sub-word\ninformation. First, they show that, since the model takes characters as inputs, it is able to learn sub-word information\neven for the unseen words. Second, they show that while the first layer of biLSTM better captures the syntactic similarity\nof words, the second layer better captures the semantics. Therefore, they propose to use the different layers of the model\nto create word representations. They also propose to use a weighted averaging method for combining the different\nlayers. They show that including ELMO representations can improve many state-of-the-art models in various NLP\ntasks.\nInstead of using words as input, Flair [1] uses a character-level language model to learn contextual word representa-\ntions. Different from ELMO [110] where character level inputs are later converted into word features, in this work, the\nauthors propose to use characters only. They feed the characters of an input string to a single layer LSTM network and\npredict the next character. They later form the word representation by concatenating the backward LSTM output from\nthe beginning of the word with the forward LSTM output from the end of the word. They also try concatenating other\npre-trained word vectors with their contextual representations in downstream tasks and show that this can improve\nthe results.\nBERT [36] uses bidirectional transformer [132] architecture to learn contextual word representations. Different from\nthe earlier approaches (ELMO [110], Melamud et al. [87]) BERT is bidirectional. Although ELMO also considers both\nsides of a target word, it considers them separately as the left and right sides. Instead, BERT spans the entire sentence\nwith both right to left and left to right transformers. To do so, without also spanning the target word, they mask the\ntarget word. Therefore, they call this model a masked language model (MLM).\nIn addition to the token (word) embeddings, they also use segment (sentence) embeddings and position embeddings\n(words\u2019 position in segments) as input which enables BERT to consider multiple sentences as context and to represent\ninter-sentence relations. Giving multiple sentences as input helps BERT be integrated into most downstream tasks\nthat require inter-sentence connections such as question answering (QA) and natural language inference (NLI) easily\nwithout requiring any other architecture. For further details, the reader can refer to the work of Rogers et al. [116],\n A Survey On Neural Word Embeddings\n19\nwhich provides an in-depth survey on how exactly BERT works and what kind of information it captures during training\nand fine-tuning.\nXLNet [145] is an autoregressive method that combines the advantages of two language modeling methods: Au-\ntoregressive models (i.e. transformer-XL [34]) and autoencoder models (i.e. BERT [36]). Specifically, it considers both\nsides of the target word by employing a permutation language modeling object without masking any words like BERT,\nwhich allows their model to capture also the relation between the masked word and the context words, unlike BERT.\nALBERT [66] aims at lowering the memory consumption and training times of BERT [36]. To accomplish this, they\nperform two changes on the original BERT model: They factorize the embeddings into two matrices to use smaller\ndimensions, and they apply weight sharing to decrease the number of parameters. They state that weight sharing also\nallows the model to generalize better. They show that although they can obtain state-of-the-art results over BERT with\nfewer parameters, ALBERT requires a longer time to train than BERT.\nRoBERTa [81] revises the pre-training design choices of BERT [36] by trying alternatives in a controlled way.\nSpecifically, dynamic masking for the Masked Language Model (MLM), input format of entire sentences from a single\ndocument with the Next Sentence Prediction (NSP) loss removed, and byte-level Byte Pair Encoding (BPE) vocabulary\ngive better performance. Moreover, they extend the training set size and the size of mini-batches in training. As a result,\nRoBERTa [81] achieves state-of-the-art results in GLUE, RACE, and SQuAD benchmarks.\nIn their work, called ERNIE, Sun et al. [125] improve on BERT by introducing two knowledge masking strategies\ninto their masked language modeling. In addition to masking out random words in the sentence, they mask phrases\nand named entities to incorporate real-world knowledge into language modeling/representation. In their successive\nwork, ERNIE 2.0 [126], they implement continual multi-task learning. Including the one in ERNIE, they define seven\npre-training tasks categorized into word-aware, structure-aware, and semantic-aware pre-training tasks, aiming to\ncapture lexical, syntactic, and semantic relations, respectively.\nGPT and its variants rely on a meta-learner idea using a conditional language model in diverse NLP tasks. This\nconditional language model predicts the next word conditioned both on an unsupervised pre-trained language model\nand the previous set of words in context. In GPT-3, Brown et al. [17] pre-train a 175 billion parameter transformer-based\nlanguage model on a sufficiently large and diverse corpus and tests its performance in zero-shot, one-shot, and few-shot\nsettings. Their learning curves for these three settings show that a larger model better learns a task from contextual\ninformation. Authors apply task-specific input transformations, e.g., delimiting context and question from the answer\nin reading comprehension, to test the model\u2019s performance in different NLP tasks. Their few-shot results prove the\neffectiveness of their approach by outperforming state-of-the-art on LAMBADA language modeling dataset [106],\nTriviaQA closed book open domain question answering dataset [59], and PhysicalQA (PIQA) common sense reasoning\ndataset [10].\n5\nPERFORMANCE OF WORD REPRESENTATIONS\nDue to the popularity of the field, many datasets have been proposed and tested upon. In this section, we report the\nstructure of the datasets and the performance of the aforementioned word embedding models on them.\n5.1\nDatasets\nDepending on their aim, the datasets produced to measure the success of embedding models can be divided into four\ncategories: Similarity tasks, analogy task, synonym selection tasks, and downstream tasks.\n 20\nErhan Sezerer and Selma Tekir\n5.1.1\nSimilarity Tasks. These datasets provide pairs of words whose similarity is rated by human judgments. They\nall use Spearman\u2019s rank correlation (\ud835\udf0c) with average human judgment to measure the performance and quality of\nembeddings.\n\u2022 WordSim-353 (WS-353): Finkelstein et al. [41] produced a corpus that contains human judgements, rated from 1\nto 10, on 353 pairs of words.\n\u2022 SCWS: Huang et al. [55] introduced this dataset in which the word pairs are scored by mechanical turkers\nwithin a context, which is usually a paragraph from Wikipedia that contains the given word. The reason for\nintroducing such a dataset is that the available test sets for similarity measurements are not sufficient for testing\nmulti-prototype word embeddings because the scores of word pairs in those test sets are given in isolation, which\nlacks the contextual information for senses.\n\u2022 RG-65: This dataset, developed by Rubenstein and Goodenough [118], is composed of 65 noun pairs whose\nsimilarity is rated by humans.\n\u2022 MC-30: The dataset [92] contains 30 pairs of words.\n\u2022 MEN: It [18] contains 3000 pairs of words together with human assigned similarity scores obtained from Amazon\nMechanical Turk.\n\u2022 YP-130: Similar to the previous test sets, YP-130 citeYP130 also contains human assigned similarity scores to\n130 word pairs.\n\u2022 RW: Unlike the previous word similarity datasets, RW [84] consists of 2034 pairs of rare words which are not\nfrequently seen in texts. The motivation behind this dataset is to provide a sufficient number of complex and rare\nwords to test the expressiveness of morphological models since the previous datasets mostly contain frequent\nwords that are insufficient for such tests.\n\u2022 Simlex-999: Simlex-999 dataset [51] contains 999 pairs of words whose similarity is annotated by mechanical\nturkers.\n5.1.2\nAnalogy Task. Semantic-syntactic word relationship test set (Google Analogy Task) introduced by Mikolov\net al. [88] consists of pairs of words in the form of \ud835\udc4e is to \ud835\udc4e\u2217 as \ud835\udc4f is to \ud835\udc4f\u2217 (such as Paris is to France as London is\nto England). The aim is to find \ud835\udc4f\u2217, given \ud835\udc4e, \ud835\udc4e\u2217, and \ud835\udc4f (cosine distance is used as a distance metric to find the miss-\ning word). There are 8869 semantic and 10675 syntactic questions in the dataset, and the success is measured by accuracy.\n5.1.3\nSynonym Selection Tasks. Given a word as input, this task aims to select the most synonym-like word among the\nlist of candidates. Accuracy (%) is used to measure the performance.\n A Survey On Neural Word Embeddings\n21\n\u2022 ESL-50: Contains 50 synonym selection questions from ESL (English as a second language) tests.\n\u2022 TOEFL-80: Contains 80 synonym selection questions from TOEFL (Test of English as Foreign Language) tests.\n\u2022 RD-300: Contains 300 synonym selection problems from Reader\u2019s Digest Power Game.\n5.1.4\nDownstream Tasks. As representations and models get better and the difference between word embedding\nmethods and language models gets closer, experiments are shifted from similarity tasks to downstream tasks.\nGLUE benchmark dataset [135] is introduced to provide a stable testing environment for researchers. It consists of\nseveral downstream tasks:\n\u2022 CoLA: The Corpus of Linguistic Acceptability [138] is a sentence classification task where the aim is to determine\nwhether a sentence is linguistically acceptable or not. It contains 9594 sentences from linguistic publications and\nthe success is measured by Matthew\u2019 Correlation Coefficient (MCC).\n\u2022 SST-2: The Stanford Sentiment Treebank [123] consists of 68.8\ud835\udc58 sentences from movie reviews. The aim is to\nclassify the sentiment of sentences. Accuracy is used to measure the performance.\n\u2022 MRPC: Microsoft Research Paraphrase Corpus [37] contains 5800 pairs of sentences from news sources on the\nweb. Each pair is annotated by humans indicating whether they are semantically equivalent or not. Performance\nis measured by accuracy.\n\u2022 STS-B: Semantic Textual Similarity Benchmark [21] is composed of 8628 pairs of sentences from various sources,\nannotated between 1 and 5, determining how similar they are. Success is measured by Spearman\u2019s rank correlation\n(\ud835\udf0c).\n\u2022 QQP: Quora Question Pairs [25] dataset contains over 400\ud835\udc58 question pairs where the aim is to determine whether\nthe questions are semantically similar or not. Success is measured by accuracy.\n\u2022 MNLI: Multi-Genre Natural Language Inference [140] dataset is composed of 430\ud835\udc58 crowd-sourced sentence pairs\nannotated with entailment information. The aim is to predict whether a second sentence is a contradiction,\nentailment, or neutral to the first one. Accuracy is used to measure the performance.\n\u2022 QNLI: Questions Natural Language Inference [112] dataset is a modified version of the SQuAD dataset [114]. It\ncontains over 100\ud835\udc58 sentence/context pairs where the aim is to determine if the context contains an answer to the\nquestion.\n\u2022 RTE: Recognizing Textual Entailment [8] is similar to MNLI, where the aim is to predict the type of entailment\nbetween a paragraph and a sentence, entailment, contradiction, and unknown being the choices.\n\u2022 WNLI: Winograd Natural Language Inference [68] dataset also concerns natural language inference similar to\nthe MNLI and the RTE datasets.\nStanford Question Answering Dataset (SQuAD 1.1 [114] and SQuAD 2.0 [113]) is a reading comprehension dataset\nthat is composed of Wikipedia articles and questions related to them. The aim is to find the text segment that answers\nthe related question. There are 150\ud835\udc58 questions 50\ud835\udc58 of which is unanswerable from the given context article. Any\nmodel built for this task should also determine whether the question is answerable or not in addition to answering the\nquestions.\n 22\nErhan Sezerer and Selma Tekir\nTable 5. Word embedding models\u2019 performances in similarity tasks (in chronological order).\nModel\nDim.\nWS-353\nSCWS (\ud835\udf0c \u00d7 100)\nRG-65\nMEN\nYP-130\nRW\nMC-30\nSimlex-999\n(\ud835\udf0c \u00d7 100)\navgSim\navgSimC\nglobalSim\nlocalSim\nMaxSimC\n(\ud835\udf0c \u00d7 100)\n(\ud835\udf0c \u00d7 100)\n(\ud835\udf0c \u00d7 100)\n(\ud835\udf0c \u00d7 100)\n(\ud835\udf0c \u00d7 100)\n(\ud835\udf0c \u00d7 100)\nHLBL [94]\n100\n33.23\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nC&W [27]\n50\n29.53\n-\n-\n57.08\n-\n-\n48.07\n57.07\n-\n-\n-\n-\nC&W [27]\n50\n49.83\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR&M [115]\n-\n73.43\n60.43\n60.53\n62.58\n-\n60.48\n-\n-\n-\n-\n-\n-\nRNNLM [89]\n640\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nHuang et al. [55]\n50\n71.3\n62.8\n65.7\n58.62\n26.12\n-\n-\n-\n-\n-\n-\n-\nCBOW [88]\n400\n69.47\n64.27\n-\n-\n-\n-\n73.27\n66.57\n34.37\n-\n-\n-\nSkip-Gram [88]\n100\n58.95\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nSkip-Gram [90]\n300\n70.49\n66.66\n66.66\n65.29\n-\n-\n-\n-\n-\n-\n-\n-\nSkip-Gram [90]\n256\n66.71\n-\n-\n-\n-\n-\n-\n55.71\n-\n38.81\n-\n-\nLuong et al.[84]\n50\n64.6\n-\n-\n48.5\n-\n-\n65.4\n-\n-\n34.4\n71.7\n-\nCLBL [15]\n-\n39.0\n-\n-\n-\n-\n-\n41.0\n-\n-\n30.0\n-\n-\nTian et al. [127]\n50\n-\n-\n65.4\n-\n-\n63.6\n-\n-\n-\n-\n-\n-\nQiu et al. [111]\n200\n65.2\n-\n-\n53.4\n-\n-\n67.4\n-\n-\n32.9\n81.6\n-\nMSSG [101]\n300\n70.9\n67.3\n69.1\n65.5\n59.8\n-\n-\n-\n-\n-\n-\n-\nChen et al. [24]\n200\n-\n66.2\n68.9\n64.2\n-\n-\n-\n-\n-\n-\n-\n-\nGloVe [109]\n300\n75.9\n-\n-\n59.6\n-\n-\n82.9\n-\n-\n47.8\n83.6\n41.015\nGuo et al. [46]\n50\n-\n49.3\n-\n-\n-\n55.4\n-\n-\n-\n-\n-\n-\nKNET [32]\n100\n66.1\n-\n-\n-\n-\n-\n-\n-\n-\n39.3\n-\n-\nCNN-VMSSG [23]\n300\n-\n65.7\n66.4\n66.3\n61.1\n-\n-\n-\n-\n-\n-\n-\nAutoExtend [117]\n300\n-\n68.9\n69.8\n-\n-\n-\n-\n-\n-\n-\n-\n-\nSenseEmbed [57]\n400\n77.9\n62.4\n-\n-\n-\n-\n89.4\n80.5\n73.4\n-\n-\n-\nTWE-1 [80]\n400\n-\n-\n68.1\n-\n-\n67.3\n-\n-\n-\n-\n-\n-\nJauhar et al. [58]\n80\n63.9\n-\n-\n65.7\n-\n-\n73.4\n64.6\n-\n-\n75.8\n-\nSAMS [26]\n300\n-\n62.5\n-\n59.9\n58.5\n-\n-\n-\n-\n-\n-\n-\nSWE [78]\n300\n72.8\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nSoricut and Och [124]\n500\n71.2\n-\n-\n-\n-\n-\n75.1\n-\n-\n41.8\n-\n-\nCotterell et al. [30]\n100\n58.9\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nchar2vec [20]\n256\n34.5\n-\n-\n-\n-\n-\n-\n32.2\n-\n28.2\n-\n-\nBojanowski et al. [12]\n300\n71.0\n-\n-\n-\n-\n-\n-\n-\n-\n47.0\n-\n-\nYin and Sch\u00fctze [147]\n200\n76.0\n-\n-\n-\n-\n-\n-\n82.5\n-\n61.6\n85.7\n48.5\ndLCE [104]\n500\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n59.0\nNgram2vec [149]\n300\n-\n-\n-\n-\n-\n-\n-\n76.0\n-\n44.6\n-\n42.1\nMSWE [102]\n300\n72.4\n66.7\n66.7\n66.8\n-\n-\n-\n76.4\n-\n35.6\n-\n39.2\nDict2vec [128]\n300\n75.6\n-\n-\n-\n-\n-\n87.5\n75.6\n64.6\n48.2\n86.0\n-\nLMM [144]\n200\n61.5\n-\n-\n63.0\n-\n-\n63.1\n-\n-\n43.1\n-\n-\nLSTMEmbed [56]\n400\n61.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nRACE dataset [65] is also a dataset for reading comprehension taken from the English exams for middle and high\nschool Chinese students. The aim is to find the correct answer to the question about a specific text passage among the\nchoices. There are approximately 28\ud835\udc58 passages and 100\ud835\udc58 questions.\nOne can find the links of the datasets in Appendix B. The leaderboards of current state-ot-the-art can be tracked\neither from the respective websites or from the ACL Wiki website (https://aclweb.org/aclwiki/State_of_the_art). The\nreader can refer to Bakarov [5] for comparisons, advantages, and disadvantages of the evaluation methods of word\nembedding models.\n5.2\nResults\nIn this section, we report the results obtained by the models examined in this survey on aforementioned datasets. In\nTables 5,6,7, and 8, the results in similarity, analogy, synonym selection, and downstream tasks are given respectively.\nWhile reporting the results, we follow a few criteria to make it as fair and simple as possible:\n\u2022 Unless noted otherwise, all of the results are taken from the original papers. (The results taken from other sources\nare marked with numbered superscripts. See Appendix B for details.)\n\u2022 If more than one paper report results on the same model, we take the one in the original paper.\n\u2022 If the author(s) provide several variations of a model, we report only the one with the best score.\n A Survey On Neural Word Embeddings\n23\nTable 6. Word embedding models\u2019 performances in analogy task (in chronological order).\nModel\nDimension\nGoogle Analogy Task (acc. %)\nSyntactic\nSemantic\nTotal\nC&W [27]\n50\n9.34\n12.34\n11.04\nRNNLM [89]\n640\n8.64\n36.54\n24.64\nCBOW [88]\n1000\n57.3\n68.9\n63.7\nSkip-Gram [88]\n1000\n66.1\n65.1\n65.6\nSkip-Gram [90]\n100\n36.413\n28.013\n32.613\nSkip-Gram [90]\n300\n61.0\n61.0\n61.0\nSkip-Gram [90]\n256\n51.31\n33.91\n43.61\nivLBL [95]\n100\n46.1\n40.0\n43.3\nivLBL [95]\n300\n63.0\n65.2\n64.0\nvLBL [95]\n300\n64.8\n54.0\n60.0\nvLBL [95]\n600\n67.1\n60.5\n64.1\nQiu et al. [111]\n200\n58.4\n25.0\n43.3\nMSSG [101]\n300\n-\n-\n64.010\nGloVe [109]\n300\n69.3\n81.9\n75.0\nKNET [32]\n100\n46.9\n24.9\n36.3\nchar2vec [20]\n256\n52.5\n2.5\n35.5\nBojanowski et al. [12]\n300\n74.9\n77.8\n-\nYin and Sch\u00fctze [147]\n200\n76.3\n92.5\n77.0\nNgram2vec [149]\n300\n71.0\n74.2\n72.5\nMSWE [102]\n50\n-\n-\n69.9\nLMM [144]\n200\n20.4\n-\n-\nAlthough some of the differences in performances of word representations are due to the models themselves, it\nshould be noted that the size of the datasets that the models are trained on can be different, therefore, can affect the\nfairness of comparison.\nTable 5 shows word embedding models\u2019 performances in similarity tasks. SenseEmbed [57] is the best performing\nmodel in WS-353, RG-65, and YP-130 datasets according to the reported results. Yin and Sch\u00fctze [147] has superior\nperformance in the datasets of MEN and RW, while Dict2vec [128] outperforms others on MC-30. In SCWS, AutoExtend\n[117] gives the highest correlation coefficient scores. In general, GloVe [109], SenseEmbed [57], Yin and Sch\u00fctze [147],\nand Dict2vec [128] perform well on similarity datasets.\nSenseEmbed\u2019s [57] success can be attributed to its capability to disambiguate senses by being trained on sense-tagged\ncorpora. Glove [109] is generally robust as it\u2019s a mixture of global co-occurrence and local context-based methods.\nWhen it comes to Yin and Sch\u00fctze [147], it is an ensemble of existing embeddings, including Glove, which produces\nbetter representations for OOV words due to its ensemble nature. Thus, it has good coverage of words in similarity\ndatasets. Dict2vec\u2019s [128] performance proves the effectiveness of positive sampling over word2vec [90].\nWord embedding models\u2019 performances are tested on Google Analogy Task that includes both syntactic and semantic\nanalogies (Table 6). The best accuracy scores are obtained by Yin and Sch\u00fctze [147] in this category. Glove[109] follows it\nas the second-best performing model. Results in the Google Analogy task can be interpreted much as those in similarity\ntasks.\nIn synonym-selection tasks, three models\u2019 (Skip-Gram [90], Jauhar et al. [58], SWE [78]) results are reported (Table\n7). In ESL-50 and RD-300 datasets, the only model with the reported performance is Jauhar et al. [58]. In TOEFL-80,\nSWE[78] outperforms the others. Here, SWE\u2019s success can be explained by its synonym-antonym rule in learning word\nembeddings.\nIn Table 8, word embedding models\u2019 performances on downstream tasks are provided. In GLUE benchmark, CBOW\n[88], BiLSTM+Cove+Attn [86], and BiLSTM+Elmo+Attn [110] are behind human baselines except for the task of QQP.\n 24\nErhan Sezerer and Selma Tekir\nTable 7. Word embedding models\u2019 performances in synonym selection tasks (in chronological order).\nModel\nDimension\nESL-50 (%)\nTOEFL-80 (%)\nRD-300 (%)\nSkip-Gram [90]\n300\n-\n83.711\n-\nSkip-Gram [90]\n400\n62.014\n87.014\n-\nGloVe [109]\n300\n60.014\n88.714\n-\nMSSG [101]\n300\n57.114\n78.314\n-\nJauhar et al. [58]\n80\n63.6\n73.3\n66.7\nJauhar et al. [58]\n80\n73.314\n80.014\n-\nLi and Jurafsky [71]\n300\n50.014\n82.614\n-\nSWE [78]\n300\n-\n88.7\n-\nLSTMEmbed [56]\n400\n72.0\n92.5\n-\nTable 8. Word embedding models\u2019 performances in downstream tasks.\nModel\nCoLA\nSST-2\nMRPC\nSTS-B\nQQP\nMNLI\nQNLI\nRTE\nWNLI\nSQuAD 2.0\nRACE\n(mcc)\n(%)\n(F1)\n(\ud835\udf0c \u00d7 100)\n(F1)\nm/mm (%/%)\n(%)\n(%)\n(%)\n(F1)\n(%)\nCBOW [88]\n0.0\n80.0\n81.5\n58.7\n51.4\n56.0/56.4\n72.1\n54.1\n62.3\nBiLSTM+Cove+Attn [86]\n8.3\n80.7\n80.0\n68.4\n60.5\n68.1/68.6\n72.9\n56.0\n18.3\n-\n-\nBiLSTM+Elmo+Attn [110]\n33.6\n90.4\n84.4\n72.3\n63.1\n74.1/74.5\n79.8\n58.9\n65.1\n-\n-\nGLUE Human Baselines\n66.4\n97.8\n86.3\n92.6\n59.5\n92.0/92.8\n91.2\n93.6\n95.9\n-\n-\nSQuAD Human Baselines [113]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n89.4\n-\nTurkers [65]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n73.3\nBERT [36]\n60.5\n94.9\n89.3\n86.5\n72.1\n86.7/85.9\n91.1\n70.1\n65.1\n89.112\n72.012\nERNIE 2.0 [126]\n63.5\n95.6\n90.2\n90.6\n73.8\n88.7/88.8\n94.6\n80.2\n67.8\n-\n-\nXLNet [145] (ensemble)\n67.8\n96.8\n92.9\n91.6\n74.7\n90.2/89.7\n98.6\n86.3\n90.4\n89.112\n81.812\nRoBERTa [81] (ensemble)\n67.8\n96.7\n92.3\n91.9\n74.3\n90.8/90.2\n98.9\n88.2\n89.0\n89.812\n83.212\nALBERT [66]\n71.4\n96.9\n90.9\n93.0\n-\n90.8\n95.3\n89.2\n-\n90.9\n86.5\nALBERT [66] (ensemble)\n69.1\n97.1\n93.4\n92.5\n74.2\n91.3/91.0\n99.2\n89.2\n91.8\n92.2\n89.4\nGPT-3 Few-Shot [17] -\n-\n-\n-\n-\n-\n-\n-\n69.0\n-\n69.8\n45\nIn QQP, CBOW is still underperforming but BiLSTM+Cove+Attn [86] and BiLSTM+Elmo+Attn[110] are superior to\nhuman performance.\nAs for the original BERT[36] and its variants, in the tasks of MRPC, QQP, QNLI, they consistently outperform\nhuman baselines. In SST-2, MNLI, RTE, and WNLI, human performance is better. In STS-B, the only model with\nsuperior performance to humans is ALBERT[66], In CoLA, and the tasks of question answering (SQuAD 2.0), and\nreading comprehension (RACE), starting from XLNET[145] better performances over human are observed. GPT-3\n[17] is promising with its language model meta-learner idea and gives its best performance in the Few-Shot setting.\nAlthough it is behind the state-of-the-art by a large margin in GLUE benchmark, in RTE its score is beyond CBOW [88],\nBiLSTM+Cove+Attn [86], and BiLSTM+Elmo+Attn [110].\nTable 8 proves the success of contextual representations, especially the transformer-based models (BERT [36] and its\nsuccessors), by going beyond human performance in most of the downstream tasks. However, it can be said that in\nnatural language inference tasks such as MNLI, WNLI, and RTE, these probabilistic language representations still have\nsome limitations in meeting causal inference requirements.\n6\nCONCLUSION\nHuman-level language understanding is one of the oldest challenges in computer science. Many scientific works have\nbeen dedicated to finding good representations for semantic units (words, morphemes, characters) in languages since\nit is preliminary for all downstream tasks in NLP. Most of these studies use the distributional hypothesis, where the\nmeaning of a word is measured from its neighboring words.\n A Survey On Neural Word Embeddings\n25\nDistributed representation through a neural network is intuitive in that it resembles human mind\u2019s representation\nof concepts. Beyond that, pre-trained language models\u2019 knowledge has been transferred to fine-tuned task-specific\nmodels, which introduced a boost in performance. To summarize, neural language models with their updated weights\nas well as learned representations in their layers have become a source of knowledge.\nFrom the release of early word embeddings to current contextual representations, the area of semantics has experi-\nenced a transformation, which becomes evident by substantial performance improvements in all NLP tasks. The idea of\npre-training a language model then fine-tuning it on a downstream task has become a de facto standard in almost all\nsubfields of NLP.\nRecently, contextual models, such as BERT and its variants, showed great success in downstream NLP tasks using\nmasked language modeling and transformer structures. They have become state-of-the-art word embeddings and\nobtained human-level results on some of the downstream tasks.\nOver the last few years, there has been an increase in the studies that consider experiential (visual) information by\nbuilding multi-modal language models and representations [82, 122, 146]. The idea of multi-modal language modeling\nis based on human language acquisition, where learning starts with concrete concepts through images early on (As\npointed out by the \"pointing phase\" in children [76, 77]) and then continues with learning abstract ideas through the text\n[3, 45, 133]. Fueled by the success of text-based language models and advancements in cognitive psychology, perhaps\nthis type of multi-modal language modeling can be the next goal to tackle in the future.\nACKNOWLEDGMENTS\nWe want to thank Tu\u011fkan Tu\u011flular for his valuable comments and feedback in the development of this survey.\nREFERENCES\n[1] Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual String Embeddings for Sequence Labeling. In COLING 2018, 27th International\nConference on Computational Linguistics. 1638\u20131649.\n[2] Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored Neural Language Models. In Proceedings of the Human Language Technology Conference\nof the NAACL, Companion Volume: Short Papers (New York, New York) (NAACL-Short \u201906). Association for Computational Linguistics, Stroudsburg,\nPA, USA, 1\u20134.\n[3] Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations.\nPsychological Review 116, 3 (2009), 463\u2013498.\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. CoRR\nabs/1409.0473 (2014).\n[5] Amir Bakarov. 2018. A Survey of Word Embeddings Evaluation Methods. CoRR abs/1801.09536 (2018). arXiv:1801.09536 http://arxiv.org/abs/1801.\n09536\n[6] Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on\nComputational Linguistics - Volume 1 (Montreal, Quebec, Canada) (COLING \u201998). Association for Computational Linguistics, Stroudsburg, PA, USA,\n86\u201390. https://doi.org/10.3115/980451.980860\n[7] Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language Model. J. Mach. Learn. Res. 3\n(March 2003), 1137\u20131155.\n[8] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The Fifth PASCAL Recognizing Textual Entailment\nChallenge. In In Proc Text Analysis Conference (TAC\u201909.\n[9] Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-powered Deep Learning for Word Embedding. In Proceedings of the 2014th European\nConference on Machine Learning and Knowledge Discovery in Databases - Volume Part I (Nancy, France) (ECMLPKDD\u201914). Springer-Verlag, Berlin,\nHeidelberg, 132\u2013148. https://doi.org/10.1007/978-3-662-44848-9_9\n[10] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about Physical Commonsense in Natural\nLanguage. CoRR abs/1911.11641 (2019). arXiv:1911.11641 http://arxiv.org/abs/1911.11641\n[11] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. J. Mach. Learn. Res. 3 (March 2003), 993\u20131022.\n[12] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching Word Vectors with Subword Information. Transactions of\nthe Association for Computational Linguistics 5 (2016), 135\u2013146.\n 26\nErhan Sezerer and Selma Tekir\n[13] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A Collaboratively Created Graph Database for\nStructuring Human Knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data (Vancouver, Canada)\n(SIGMOD \u201908). ACM, New York, NY, USA, 1247\u20131250. https://doi.org/10.1145/1376616.1376746\n[14] Lars Borin, Markus Forsberg, and Lennart L\u00f6nngren. 2013. SALDO: a touch of yin to WordNet\u2019s yang. Language Resources and Evaluation 47, 4\n(2013), 1191\u20131211. https://doi.org/10.1007/s10579-013-9233-4\n[15] Jan A. Botha and Phil Blunsom. 2014. Compositional Morphology for Word Representations and Language Modelling. In Proceedings of the 31st\nInternational Conference on International Conference on Machine Learning - Volume 32 (Beijing, China) (ICML\u201914). JMLR.org, II\u20131899\u2013II\u20131907.\n[16] Sabine Brants, Stefanie Dipper, Peter Eisenberg, Silvia Hansen-Schirra, Esther K\u00f6nig, Wolfgang Lezius, Christian Rohrer, George Smith, and\nHans Uszkoreit. 2004. TIGER: Linguistic Interpretation of a German Corpus. Research on Language and Computation 2, 4 (01 Dec 2004), 597\u2013620.\nhttps://doi.org/10.1007/s11168-004-7431-3\n[17] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. CoRR abs/2005.14165 (2020).\narXiv:2005.14165 https://arxiv.org/abs/2005.14165\n[18] Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal Distributional Semantics. J. Artif. Int. Res. 49, 1 (Jan. 2014), 1\u201347.\n[19] Jose Camacho-Collados and Mohammad Taher Pilehvar. 2018. From Word to Sense Embeddings: A Survey on Vector Representations of Meaning.\nJ. Artif. Int. Res. 63, 1 (Sept. 2018), 743\u2013788. https://doi.org/10.1613/jair.1.11259\n[20] Kris Cao and Marek Rei. 2016. A Joint Model for Word Embedding and Word Morphology. In Proceedings of the 1st Workshop on Representation\nLearning for NLP. Association for Computational Linguistics, Berlin, Germany, 18\u201326. https://doi.org/10.18653/v1/W16-1603\n[21] Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual\nand Crosslingual Focused Evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Association for\nComputational Linguistics, Vancouver, Canada, 1\u201314. https://doi.org/10.18653/v1/S17-2001\n[22] Stanley F. Chen and Joshua Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proceedings of the 34th\nAnnual Meeting on Association for Computational Linguistics (ACL \u201996). Association for Computational Linguistics, 310\u2013318.\n[23] Tao Chen, Ruifeng Xu, Yulan He, and Xuan Wang. 2015. Improving Distributed Representation of Word Sense via WordNet Gloss Composition and\nContext Clustering. In ACL.\n[24] Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A Unified Model for Word Sense Representation and Disambiguation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, 1025\u20131035.\nhttps://doi.org/10.3115/v1/D14-1110\n[25] Zihan chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. 2018. Quora Question Pairs. https://www.quora.com/q/quoradata/First-Quora-Dataset-\nRelease-Question-Pairs\n[26] Jianpeng Cheng and Dimitri Kartsaklis. 2015. Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning. In\nProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal,\n1531\u20131542. https://doi.org/10.18653/v1/D15-1177\n[27] Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask\nLearning. In Proceedings of the 25th International Conference on Machine Learning (Helsinki, Finland) (ICML \u201908). New York, NY, USA, 160\u2013167.\nhttps://doi.org/10.1145/1390156.1390177\n[28] Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost)\nfrom Scratch. J. Mach. Learn. Res. 12 (Nov. 2011), 2493\u20132537.\n[29] Ryan Cotterell and Hinrich Sch\u00fctze. 2015. Morphological Word-Embeddings. In Proceedings of the 2015 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Denver, Colorado,\n1287\u20131292. https://doi.org/10.3115/v1/N15-1140\n[30] Ryan Cotterell, Hinrich Sch\u00fctze, and Jason Eisner. 2016. Morphological Smoothing and Extrapolation of Word Embeddings. In Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Berlin, Germany). Association for Computational\nLinguistics, 1651\u20131660. https://doi.org/10.18653/v1/P16-1156\n[31] Mathias Creutz and Krista Lagus. 2007. Unsupervised Models for Morpheme Segmentation and Morphology Learning. ACM Trans. Speech Lang.\nProcess. 4, 1, Article 3 (Feb. 2007), 34 pages. https://doi.org/10.1145/1187415.1187418\n[32] Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, Hanjun Dai, and Tie-Yan Liu. 2015. KNET: A General Framework for Learning Word Embedding Using\nMorphological Knowledge. ACM Trans. Inf. Syst. 34, 1, Article 4 (Aug. 2015), 25 pages. https://doi.org/10.1145/2797137\n[33] Andrew M. Dai and Quoc V. Le. 2015. Semi-Supervised Sequence Learning. In Proceedings of the 28th International Conference on Neural Information\nProcessing Systems - Volume 2 (Montreal, Canada) (NIPS\u201915). MIT Press, Cambridge, MA, USA, 3079\u20133087.\n[34] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models\nbeyond a Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for\nComputational Linguistics, Florence, Italy, 2978\u20132988. https://doi.org/10.18653/v1/P19-1285\n A Survey On Neural Word Embeddings\n27\n[35] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis.\nJOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE 41, 6 (1990), 391\u2013407.\n[36] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171\u20134186.\n[37] Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News\nSources. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics. COLING, Geneva, Switzerland, 350\u2013356.\n[38] C\u00edcero Nogueira Dos Santos and Bianca Zadrozny. 2014. Learning Character-level Representations for Part-of-speech Tagging. In Proceedings of the\n31st International Conference on International Conference on Machine Learning - Volume 32 (Beijing, China) (ICML\u201914). JMLR.org, II\u20131818\u2013II\u20131826.\n[39] Jeffrey L. Elman. 1990. Finding structure in time. Cognitive Science 14, 2 (1990), 179\u2013211.\n[40] Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015. Retrofitting Word Vectors to Semantic\nLexicons. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. Association for Computational Linguistics, Denver, Colorado, 1606\u20131615. https://doi.org/10.3115/v1/N15-1184\n[41] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing Search in Context:\nThe Concept Revisited. In Proceedings of the 10th International Conference on World Wide Web (Hong Kong, Hong Kong) (WWW \u201901). ACM, New\nYork, NY, USA, 406\u2013414.\n[42] H Gadalla, H Kilany, H Arram, A Yacoub, A El-Habashi, A Shalaby, K Karins, E Rowson, R MacIntyre, P Kingsbury, et al. 1997. CALLHOME\nEgyptian Arabic Transcripts. Linguistic Data Consortium, Philadelphia (1997).\n[43] Yoav Goldberg and Joakim Nivre. 2012. A Dynamic Oracle for Arc-Eager Dependency Parsing. In COLING.\n[44] Alex Graves and J\u00fcrgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures.\nNeural networks : the official journal of the International Neural Network Society 18 5-6 (2005), 602\u201310.\n[45] Thomas L. Griffiths, Joshua B. Tenenbaum, and Mark Steyvers. 2007. Topics in semantic representation. Psychological Review 114 (2007), 2007.\n[46] Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning Sense-specific Word Embeddings By Exploiting Bilingual Resources. In\nProceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association\nfor Computational Linguistics, Dublin, Ireland, 497\u2013507.\n[47] Michael U. Gutmann and Aapo Hyv\u00e4rinen. 2012. Noise-contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural\nImage Statistics. J. Mach. Learn. Res. 13, 1 (Feb. 2012), 307\u2013361.\n[48] R H Baayen, R Piepenbrock, and Hedderik Rijn. 1993. The CELEX lexical data base on CD-ROM. (01 1993).\n[49] Dilek Z Hakkani-T\u00fcr, Kemal Oflazer, and G\u00f6khan T\u00fcr. 2002. Statistical morphological disambiguation for agglutinative languages. Computers and\nthe Humanities 36, 4 (2002), 381\u2013410.\n[50] Zellig S. Harris. 1954. Distributional Structure. Word 10, 2-3 (1954), 146\u2013162. https://doi.org/10.1080/00437956.1954.11659520\n[51] Felix Hill, Roi Reichart, and Anna Korhonen. 2015. SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation. Computational\nLinguistics 41, 4 (Dec. 2015), 665\u2013695. https://doi.org/10.1162/COLI_a_00237\n[52] G. E. Hinton, J. L. McClelland, and D. E. Rumelhart. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1.\nMIT Press, Cambridge, MA, USA, Chapter Distributed Representations, 77\u2013109.\n[53] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (Nov. 1997), 1735\u20131780. https://doi.org/10.1162/\nneco.1997.9.8.1735\n[54] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 328\u2013339.\nhttps://doi.org/10.18653/v1/P18-1031\n[55] Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and\nMultiple Word Prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1 (Jeju\nIsland, Korea) (ACL \u201912). Association for Computational Linguistics, Stroudsburg, PA, USA, 873\u2013882.\n[56] Ignacio Iacobacci and Roberto Navigli. 2019. LSTMEmbed: Learning Word and Sense Representations from a Large Semantically Annotated Corpus\nwith Long Short-Term Memories. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy,\nJuly 28- August 2, 2019, Volume 1: Long Papers. 1685\u20131695.\n[57] Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2015. SensEmbed: Learning Sense Embeddings for Word and Relational Similarity.\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers) (Beijing, China). Association for Computational Linguistics, 95\u2013105. https://doi.org/10.3115/v1/P15-1010\n[58] Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy. 2015. Ontologically Grounded Multi-sense Representation Learning for Semantic Vector Space\nModels. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. Association for Computational Linguistics, Denver, Colorado, 683\u2013693. https://doi.org/10.3115/v1/N15-1070\n[59] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset\nfor Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,\nCanada, July 30 - August 4, Volume 1: Long Papers, Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, 1601\u20131611.\nhttps://doi.org/10.18653/v1/P17-1147\n 28\nErhan Sezerer and Selma Tekir\n[60] Faiza Khan Khattak, Serena Jeblee, Chlo\u00e9 Pou-Prom, Mohamed Abdalla, Christopher Meaney, and Frank Rudzicz. 2019. A survey of word\nembeddings for clinical text. Journal of Biomedical Informatics: X 4 (2019), 100057. https://doi.org/10.1016/j.yjbinx.2019.100057\n[61] Yoon Kim, Yacine Jernite, David A. Sontag, and Alexander M. Rush. 2016. Character-Aware Neural Language Models. In Proceedings of the Thirtieth\nAAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, Dale Schuurmans and Michael P. Wellman (Eds.). AAAI\nPress, 2741\u20132749. http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12489\n[62] R. Kneser and H. Ney. 1995. Improved backing-off for M-gram language modeling. In 1995 International Conference on Acoustics, Speech, and Signal\nProcessing, Vol. 1. 181\u2013184 vol.1.\n[63] Sawan Kumar, Sharmistha Jat, Karan Saxena, and Partha Talukdar. 2019. Zero-shot Word Sense Disambiguation using Sense Definition Embeddings.\nIn Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers. 5670\u20135681.\n[64] Andrey Kutuzov, Lilja \u00d8vrelid, T. Szymanski, and Erik Velldal. 2018. Diachronic word embeddings and semantic shifts: a survey. ArXiv abs/1806.03537\n(2018).\n[65] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding Comprehension Dataset From Examinations.\nIn Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Copenhagen,\nDenmark, 785\u2013794. https://doi.org/10.18653/v1/D17-1082\n[66] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In International Conference on Learning Representations.\n[67] R\u00e9mi Lebret and Ronan Collobert. 2014. Word Embeddings through Hellinger PCA. In Proceedings of the 14th Conference of the European Chapter of the\nAssociation for Computational Linguistics. Association for Computational Linguistics, Gothenburg, Sweden, 482\u2013490. https://doi.org/10.3115/v1/E14-\n1051\n[68] Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth International Conference on the\nPrinciples of Knowledge Representation and Reasoning.\n[69] Omer Levy and Yoav Goldberg. 2014. Dependency-Based Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers) (Baltimore, Maryland). Association for Computational Linguistics, 302\u2013308.\nhttps:\n//doi.org/10.3115/v1/P14-2050\n[70] Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions\nof the Association for Computational Linguistics 3 (2015), 211\u2013225. https://doi.org/10.1162/tacl_a_00134\n[71] Jiwei Li and Dan Jurafsky. 2015. Do Multi-Sense Embeddings Improve Natural Language Understanding?. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, 1722\u20131732. https://doi.org/10.\n18653/v1/D15-1200\n[72] Franklin Mark Liang. 1983. Word Hy-phen-a-tion by Com-put-er. Technical Report.\n[73] Franklin Mark Liang. 1983. Word Hy-phen-a-tion by Com-put-er (Hyphenation, Computer). Ph.D. Dissertation. Stanford University, Stanford, CA,\nUSA. AAI8329742.\n[74] Wang Ling, Chris Dyer, Alan W Black, Isabel Trancoso, Ram\u00f3n Fermandez, Silvio Amir, Lu\u00eds Marujo, and Tiago Lu\u00eds. 2015. Finding Function in\nForm: Compositional Character Models for Open Vocabulary Word Representation. In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, 1520\u20131530. https://doi.org/10.18653/v1/D15-1176\n[75] Wang Ling, Yulia Tsvetkov, Silvio Amir, Ram\u00f3n Fermandez, Chris Dyer, Alan W Black, Isabel Trancoso, and Chu-Cheng Lin. 2015. Not All Contexts\nAre Created Equal: Better Word Representations with Variable Attention. In Proceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, Lisbon, Portugal, 1367\u20131372. https://doi.org/10.18653/v1/D15-1161\n[76] Ulf Liszkowski, Malinda Carpenter, Tricia Striano, and Michael Tomasello. 2006. 12- and 18-Month-Olds Point to Provide Information for Others.\nJournal of Cognition and Development 7, 2 (2006), 173\u2013187. https://doi.org/10.1207/s15327647jcd0702_2\n[77] Ulf Liszkowski, Malinda Carpenter, and Michael Tomasello. 2008. Twelve-month-olds communicate helpfully and appropriately for knowledgeable\nand ignorant partners. Cognition 108, 3 (2008), 732\u2013739. https://doi.org/10.1016/j.cognition.2008.06.013\n[78] Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and Yu Hu. 2015. Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints. In\nProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers). Association for Computational Linguistics, Beijing, China, 1501\u20131511. https://doi.org/10.3115/v1/P15-1145\n[79] Qi Liu, Matt J. Kusner, and P. Blunsom. 2020. A Survey on Contextual Embeddings. ArXiv abs/2003.07278 (2020).\n[80] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical Word Embeddings. In Proceedings of the Twenty-Ninth AAAI Conference on\nArtificial Intelligence (Austin, Texas) (AAAI\u201915). AAAI Press, 2418\u20132424.\n[81] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).\n[82] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-\nLanguage Tasks. In Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and\nR. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf\n[83] Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior research methods, instruments,\n& computers 28, 2 (1996), 203\u2013208.\n A Survey On Neural Word Embeddings\n29\n[84] Thang Luong, Richard Socher, and Christopher Manning. 2013. Better Word Representations with Recursive Neural Networks for Morphology. In\nProceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, Sofia, Bulgaria,\n104\u2013113.\n[85] Anh Tuan Luu, Yi Tay, Siu Cheung Hui, and See Kiong Ng. 2016. Learning Term Embeddings for Taxonomic Relation Identification Using\nDynamic Weighting Neural Network. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for\nComputational Linguistics, Austin, Texas, 403\u2013413. https://doi.org/10.18653/v1/D16-1039\n[86] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in Translation: Contextualized Word Vectors.. In NIPS, Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 6297\u20136308.\n[87] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning Generic Context Embedding with Bidirectional LSTM. In Proceedings\nof the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016. 51\u201361.\n[88] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. CoRR abs/1301.3781\n(2013). arXiv:1301.3781\n[89] Tomas Mikolov, Martin Karafi\u00e1t, Luk\u00e1s Burget, Jan Cernock\u00fd, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In\nINTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010.\n1045\u20131048.\n[90] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and Their\nCompositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (Lake Tahoe, Nevada)\n(NIPS\u201913). Curran Associates Inc., USA, 3111\u20133119.\n[91] George A. Miller. 1995. WordNet: A Lexical Database for English. Commun. ACM 38, 11 (Nov. 1995), 39\u201341. https://doi.org/10.1145/219717.219748\n[92] George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes 6, 1 (1991), 1\u201328.\nhttps://doi.org/10.1080/01690969108406936\n[93] Andriy Mnih and Geoffrey Hinton. 2007. Three New Graphical Models for Statistical Language Modelling. In Proceedings of the 24th International\nConference on Machine Learning (Corvalis, Oregon, USA) (ICML \u201907). 641\u2013648.\n[94] Andriy Mnih and Geoffrey Hinton. 2008. A Scalable Hierarchical Distributed Language Model. In Proceedings of the 21st International Conference on\nNeural Information Processing Systems (Vancouver, British Columbia, Canada) (NIPS\u201908). Curran Associates Inc., USA, 1081\u20131088.\n[95] Andriy Mnih and Koray Kavukcuoglu. 2013. Learning Word Embeddings Efficiently with Noise-contrastive Estimation. In Proceedings of the\n26th International Conference on Neural Information Processing Systems - Volume 2 (Lake Tahoe, Nevada) (NIPS\u201913). Curran Associates Inc., USA,\n2265\u20132273.\n[96] Andriy Mnih and Yee Whye Teh. 2012. A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. In Proceedings of the 29th\nInternational Coference on International Conference on Machine Learning (Edinburgh, Scotland) (ICML\u201912). Omnipress, USA, 419\u2013426.\n[97] Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural Network Language Model. In Proceedings of the Tenth International\nWorkshop on Artificial Intelligence and Statistics, Robert G. Cowell and Zoubin Ghahramani (Eds.). Society for Artificial Intelligence and Statistics,\n246\u2013252.\n[98] Nikola Mrk\u0161i\u0107, Diarmuid \u00d3 S\u00e9aghdha, Blaise Thomson, Milica Ga\u0161i\u0107, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and\nSteve Young. 2016. Counter-fitting Word Vectors to Linguistic Constraints. In Proceedings of the 2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, California,\n142\u2013148. https://doi.org/10.18653/v1/N16-1018\n[99] Roberto Navigli. 2009. Word Sense Disambiguation: A Survey. ACM Comput. Surv. 41, 2, Article 10 (Feb. 2009), 69 pages. https://doi.org/10.1145/\n1459352.1459355\n[100] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The Automatic Construction, Evaluation and Application of a Wide-coverage\nMultilingual Semantic Network. Artif. Intell. 193 (Dec. 2012), 217\u2013250. https://doi.org/10.1016/j.artint.2012.07.001\n[101] Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient Non-parametric Estimation of Multiple Embeddings\nper Word in Vector Space. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Doha, Qatar, 1059\u20131069. https://doi.org/10.3115/v1/D14-1113\n[102] Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. 2017. A Mixture Model for Learning Multi-Sense\nWord Embeddings. In Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017). Association for Computational\nLinguistics, Vancouver, Canada, 121\u2013127. https://doi.org/10.18653/v1/S17-1015\n[103] Kim Anh Nguyen, Maximilian K\u00f6per, Sabine Schulte im Walde, and Ngoc Thang Vu. 2017. Hierarchical Embeddings for Hypernymy Detection\nand Directionality. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational\nLinguistics, Copenhagen, Denmark, 233\u2013243. https://doi.org/10.18653/v1/D17-1022\n[104] Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Integrating Distributional Lexical Contrast into Word Embeddings for\nAntonym-Synonym Distinction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers).\nAssociation for Computational Linguistics, Berlin, Germany, 454\u2013459. https://doi.org/10.18653/v1/P16-2074\n[105] Luis Nieto Pi\u00f1a and Richard Johansson. 2015. A Simple and Efficient Method to Generate Word Sense Representations. In Proceedings of the\nInternational Conference Recent Advances in Natural Language Processing. INCOMA Ltd. Shoumen, BULGARIA, Hissar, Bulgaria, 465\u2013472.\n 30\nErhan Sezerer and Selma Tekir\n[106] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fern\u00e1ndez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer\nLinguistics. https://doi.org/10.18653/v1/p16-1144\n[107] Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2015. PPDB 2.0: Better paraphrase ranking,\nfine-grained entailment relations, word embeddings, and style classification. In Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for\nComputational Linguistics, Beijing, China, 425\u2013430. https://doi.org/10.3115/v1/P15-2070\n[108] Maria Pelevina, Nikolay Arefiev, Chris Biemann, and Alexander Panchenko. 2016. Making Sense of Word Embeddings. In Proceedings of the 1st\nWorkshop on Representation Learning for NLP (Berlin, Germany). Association for Computational Linguistics, 174\u2013183. https://doi.org/10.18653/v1/\nW16-1620\n[109] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Representation. In EMNLP. ACL, 1532\u20131543.\n[110] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized\nWord Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers). 2227\u20132237.\n[111] Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Co-learning of Word Representations and Morpheme Representations. In Proceedings\nof COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for\nComputational Linguistics, Dublin, Ireland, 141\u2013150.\n[112] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don\u2019t Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Melbourne,\nAustralia, 784\u2013789. https://doi.org/10.18653/v1/P18-2124\n[113] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don\u2019t Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Melbourne,\nAustralia, 784\u2013789. https://doi.org/10.18653/v1/P18-2124\n[114] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas,\n2383\u20132392. https://doi.org/10.18653/v1/D16-1264\n[115] Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype Vector-space Models of Word Meaning. In Human Language Technologies: The 2010\nAnnual Conference of the North American Chapter of the Association for Computational Linguistics (Los Angeles, California) (HLT \u201910). Association\nfor Computational Linguistics, Stroudsburg, PA, USA, 109\u2013117.\n[116] Anna Rogers, O. Kovaleva, and A. Rumshisky. 2020. A Primer in BERTology: What we know about how BERT works. ArXiv abs/2002.12327 (2020).\n[117] Sascha Rothe and Hinrich Sch\u00fctze. 2015. AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes. In Proceedings of the\n53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers) (Beijing, China). Association for Computational Linguistics, 1793\u20131803. https://doi.org/10.3115/v1/P15-1173\n[118] Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Commun. ACM 8, 10 (Oct. 1965), 627\u2013633.\nhttps:\n//doi.org/10.1145/365628.365657\n[119] Sebastian Ruder, Ivan Vuli\u0107, and Anders S\u00f8gaard. 2019. A Survey of Cross-lingual Word Embedding Models. J. Artif. Int. Res. 65, 1 (May 2019),\n569\u2013630. https://doi.org/10.1613/jair.1.11640\n[120] G. Salton, A. Wong, and C. S. Yang. 1975. A Vector Space Model for Automatic Indexing. Commun. ACM 18, 11 (Nov. 1975), 613\u2013620.\nhttps:\n//doi.org/10.1145/361219.361220\n[121] Hinrich Sch\u00fctze. 1998. Automatic Word Sense Discrimination. Computational Linguistics 24, 1 (1998), 97\u2013123.\n[122] Erhan Sezerer and Selma Tekir. 2021. Incorporating Concreteness in Multi-Modal Language Models with Curriculum Learning. Applied Sciences 11,\n17 (2021). https://doi.org/10.3390/app11178241\n[123] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep\nModels for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, Seattle, Washington, USA, 1631\u20131642.\n[124] Radu Soricut and Franz Josef Och. 2015. Unsupervised Morphology Induction Using Word Embeddings. In HLT-NAACL.\n[125] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: Enhanced\nRepresentation through Knowledge Integration. ArXiv abs/1904.09223 (2019).\n[126] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. ERNIE 2.0: A Continual Pre-training Framework\nfor Language Understanding. ArXiv abs/1907.12412 (2020).\n[127] Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. 2014. A Probabilistic Model for Learning Multi-Prototype\nWord Embeddings. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City\nUniversity and Association for Computational Linguistics, Dublin, Ireland, 151\u2013160.\n[128] Julien Tissier, Christophe Gravier, and Amaury Habrard. 2017. Dict2vec : Learning Word Embeddings using Lexical Dictionaries. In Proceedings\nof the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Copenhagen, Denmark,\n A Survey On Neural Word Embeddings\n31\n254\u2013263. https://doi.org/10.18653/v1/D17-1024\n[129] Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency\nNetwork. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language\nTechnology - Volume 1 (Edmonton, Canada) (NAACL \u201903). Association for Computational Linguistics, USA, 173\u2013180. https://doi.org/10.3115/\n1073445.1073478\n[130] Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-supervised Learning. In\nProceedings of the 48th Annual Meeting of the Association for Computational Linguistics (Uppsala, Sweden) (ACL \u201910). Association for Computational\nLinguistics, Stroudsburg, PA, USA, 384\u2013394.\n[131] Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. J. Artif. Int. Res. 37, 1 (Jan. 2010),\n141\u2013188.\n[132] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nAll you Need. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett (Eds.). Curran Associates, Inc., 5998\u20136008.\n[133] Gabriella Vigliocco, Lotte Meteyard, Mark Andrews, and Stavroula Kousta. 2009. Toward a theory of semantic representation. Language and\nCognition 1, 2 (2009), 219\u2013247.\n[134] Ivan Vuli\u0107, Nikola Mrk\u0161i\u0107, Roi Reichart, Diarmuid \u00d3 S\u00e9aghdha, Steve Young, and Anna Korhonen. 2017. Morph-fitting: Fine-Tuning Word Vector\nSpaces with Simple Language-Specific Rules. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Linguistics, Vancouver, Canada, 56\u201368. https://doi.org/10.18653/v1/P17-1006\n[135] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis\nPlatform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP. Association for Computational Linguistics, Brussels, Belgium, 353\u2013355. https://doi.org/10.18653/v1/W18-5446\n[136] Chengyu Wang, Xiaofeng He, and Aoying Zhou. 2019. SphereRE: Distinguishing Lexical Relations with Hyperspherical Relation Embeddings. In\nProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers. 1727\u20131737.\n[137] Yanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul Kingsbury, and Hongfang Liu. 2018. A\ncomparison of word embeddings for the biomedical natural language processing. Journal of Biomedical Informatics 87 (2018), 12 \u2013 20.\nhttps:\n//doi.org/10.1016/j.jbi.2018.09.008\n[138] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural Network Acceptability Judgments. Transactions of the Association for\nComputational Linguistics 7 (2019), 625\u2013641.\n[139] John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. From Paraphrase Database to Compositional Paraphrase Model and Back.\nTransactions of the Association for Computational Linguistics 3 (2015), 345\u2013358. https://doi.org/10.1162/tacl_a_00143\n[140] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) (New Orleans, Louisiana). Association for Computational Linguistics, 1112\u20131122.\n[141] Ludwig Wittgenstein. 1953. Philosophical Investigations. Basil Blackwell.\n[142] Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q. Zhu. 2012. Probase: A Probabilistic Taxonomy for Text Understanding. In Proceedings\nof the 2012 ACM SIGMOD International Conference on Management of Data (Scottsdale, Arizona, USA) (SIGMOD \u201912). ACM, New York, NY, USA,\n481\u2013492. https://doi.org/10.1145/2213836.2213891\n[143] Wei Xu and Alex Rudnicky. 2000. Can artificial neural networks learn language models?. In Sixth International Conference on Spoken Language\nProcessing.\n[144] Yang Xu, Jiawei Liu, Wei Yang, and Liusheng Huang. 2018. Incorporating Latent Meanings of Morphological Compositions to Enhance Word\nEmbeddings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for\nComputational Linguistics, Melbourne, Australia, 1232\u20131242. https://doi.org/10.18653/v1/P18-1114\n[145] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining\nfor Language Understanding. In Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc,\nE. Fox, and R. Garnett (Eds.). Curran Associates, Inc., 5753\u20135763.\n[146] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016. Stacked Attention Networks for Image Question Answering. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR). 21\u201329. https://doi.org/10.1109/CVPR.2016.10\n[147] Wenpeng Yin and Hinrich Sch\u00fctze. 2016. Learning Word Meta-Embeddings. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, 1351\u20131360. https://doi.org/10.\n18653/v1/P16-1128\n[148] Zheng Yu, Haixun Wang, Xuemin Lin, and Min Wang. 2015. Learning Term Embeddings for Hypernymy Identification. In Proceedings of the 24th\nInternational Conference on Artificial Intelligence (Buenos Aires, Argentina) (IJCAI\u201915). AAAI Press, 1390\u20131397.\n[149] Zhe Zhao, Tao Liu, Shen Li, Bofang Li, and Xiaoyong Du. 2017. Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence\nStatistics. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,\nCopenhagen, Denmark, 244\u2013253. https://doi.org/10.18653/v1/D17-1023\n 32\nErhan Sezerer and Selma Tekir\nA\nREADING GUIDE FOR BEGINNERS\nIn Figure 2, the milestone papers of each subtopic are listed. It is not an exhaustive list created to serve as a starting\npoint for researchers who are not familiar with the subject. For the details of each subfield, readers should refer to\nthe corresponding chapters in the survey. Dashed sets represent non-neural models, which are not the subject of this\nsurvey.\nWittgenstein, 1953\nHarris, 1954\nDistributional Hypothesis\nHinton et al., 1986\nElman, 1990\nDistributional Representations\nBengio et al. 2003\nCount-Based Models\nChen and Goodman, 1996\nKneser and Ney, 1995\nProbabilistic Models\nGrif\ufb01ths et al., 2007\nAndrews et al., 2007\nVigliocco et al., 2009\nLuong et al., 2013\nSch\u00fctze, 1998\nCollobert & Weston, 2008 (C&W)\nMikolov et al., 2013a, 2013b (word2vec)\nPennignton et al., 2014 (GloVe)\nNguyen et al., 2016\nYu et al., 2015\nReisinger & Mooney, 2010\nHuang et al., 2012\nEarly word\nEmbeddings\nSense\nEmbeddings\nMorpheme\nEmbeddings\nSemantic Relation\nEmbeddings\nMelamud et al., 2016 (context2vec)\nHoward and Ruder, 2018 (Ulm\ufb01t)\nPeters et al., 2018 (Elmo)\nDevlin et al., 2019 (BERT)\nContextual Representations\nFig. 2. Evolution of neural word embeddings.\n A Survey On Neural Word Embeddings\n33\nB\nDETAILS ON DATASETS AND RESULTS\nB.1\nDatasets\n\u2022 WS-353: http://gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n\u2022 SCWS: http://www-nlp.stanford.edu/~ehhuang/SCWS.zip\n\u2022 RG-65: There are no formal links to this dataset\n\u2022 MC-30: There are no formal links to this dataset\n\u2022 MEN: https://staff.fnwi.uva.nl/e.bruni/MEN\n\u2022 YP-130: https://www.researchgate.net/publication/257946337_Verb_similarity_on_the_taxonomy_of_WordNet_-\n_dataset/link/02e7e5266fe99269cc000000/download\n\u2022 RW: http://www-nlp.stanford.edu/~lmthang/morphoNLM/rw.zip\n\u2022 Simlex-999: https://fh295.github.io/simlex.html\n\u2022 Google Analogy Task: http://download.tensorflow.org/data/questions-words.txt\n\u2022 ESL-50: https://www.apperceptual.com/home (personal communication)\n\u2022 TOEFL-80: http://lsa.colorado.edu/mail_sub.html (personal communication)\n\u2022 RD-300: https://arxiv.org/ftp/arxiv/papers/1204/1204.0140.pdf (Appendix K; also contains TOEFL-80 and ESL-50)\n\u2022 Glue Benchmark (CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI):\nhttps://gluebenchmark.com/tasks\n\u2022 Stanford Question Answering Dataset (SQuAD 1.1 [114] and SQuAD 2.0 [113]):\nhttps://rajpurkar.github.io/SQuAD-explorer/\n\u2022 RACE dataset: http://www.cs.cmu.edu/~glai1/data/race/\nB.2\nResults\nYou can find the sources of experimental results here. Each number corresponds to the numbered superscripts used in\nthe tables:\n1: reported in [20]\n2: reported in [23]\n3: reported in [55]\n4: reported in [88]\n5: reported in [30]\n6: reported in [117]\n7: reported in [57]\n8: reported in [80]\n9: reported in [101]\n10: reported in [102]\n11: reported in [78]\n12: reported in [66]\n13: reported in [95]\n14: reported in [56]\n15: reported in [98]\n"}, "Generate & Rank: A Multi-task Framework for Math Word Problems": {"authors": ["Jianhao Shen", "Yichun Yin", "Lin Li", "Lifeng Shang", "Xin Jiang", "Ming Zhang", "Qun Liu"], "title": "Generate & Rank: A Multi-task Framework for Math Word Problems", "url": "https://arxiv.org/pdf/2109.03034.pdf", "abstract": "Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% $\\rightarrow$ 85.4%) higher than the state-of-the-art.", "arxiv_id": "2109.03034", "published_date": "2021-09-07", "year": 2021, "introduction": "Introduction Solving math word problems (MWP) (Bobrow, 1964) is an important and fundamental task in natural language processing (NLP), which requires to provide a solution expression given a mathematical problem description, as illustrated in Table 1. Many recent studies formalize MWP as a generation task and commonly adopt LSTM-based sequence-tosequence (Seq2Seq) models (Wang et al., 2017, 2018b; Xie and Sun, 2019), where problem texts are source sequences, mathematical expressions are target sequences and the model learns the mapping \u2020 This work is done when Jianhao Shen is an intern at Huawei Noah\u2019s Ark Lab *Corresponding author 1Code will be available soon. Original MWP Problem A project is completed in 25 days by 12 workers. If it takes 20 days to complete, how many workers will it take? Solution 25 * 12 / 20 Number-mapped MWP Problem A project is completed in NUM0 days by NUM1 workers. If it takes NUM2 days to complete, how many workers will it take? Solution NUM0 * NUM1 / NUM2 Table 1: An example of MWP, where numbers are usually mapped to special tokens, such as Num0/1/2. from source texts to target expressions. These studies have proposed numerous advanced techniques to improve the MWP solver, but their performance is still unsatisfactory yet. We argue that it is not suf\ufb01cient to model MWP as only a generation task, because there is a signi\ufb01cant difference between mathematical expressions and natural language sequences: one minor mistake in a mathematical expression will change the whole semantic thus lead to a wrong answer, whereas natural language is more robust to such minor mistakes. The objective function of the generation task is to maximize generation likelihood on ground-truth expressions, which does not have an explicit strategy to make the model learn to distinguish between ground-truth and expressions that have minor mistakes. In addition, previous works (Liu et al., 2019a; Xie and Sun, 2019; Zhang et al., 2020) \ufb01nd that the performance of generation models degrades fast as the expression gets longer. To handle the above problems, we propose Generate & Rank, a multi-task framework for MWP, which introduces a new ranker to explicitly distinguish between correct and incorrect expressions. Speci\ufb01cally, our framework includes two modules: a generator and a ranker. The former is designed to generate candidate expressions given a problem text and the latter aims to rank the candidate arXiv:2109.03034v1  [cs.CL]  7 Sep 2021 ", "conclusion": "Conclusion and Future Work We propose Generate & Rank, a new multi-task framework for math word problems. Speci\ufb01cally, our model has a generator and a ranker which enhance each other with joint training. We also use tree-based disturbance and online update to further improve the performance. The experimental results on the benchmark show that our work consistently outperforms baselines in all datasets. In future work, we will explore the generation and ranking framework to other tasks like summarization and translation. Acknowledgements This paper is partially supported by National Key Research and Development Program of China with Grant No. 2018AAA0101900/2018AAA0101902 as well as the National Natural Science Foundation of China (NSFC Grant No. 62106008 and No. 61772039). ", "full_text": "Generate & Rank: A Multi-task Framework\nfor Math Word Problems\nJianhao Shen1\u2020, Yichun Yin2, Lin Li3, Lifeng Shang2,\nXin Jiang2, Ming Zhang1*, Qun Liu2\n1Department of Computer Science, School of EECS, Peking University\n2Huawei Noah\u2019s Ark Lab\n3Huawei HiSilicon\n{jhshen, mzhang_cs}@pku.edu.cn\n{yinyichun, lilin29, shang.lifeng, jiang.xin, qun.liu}@huawei.com\nAbstract\nMath word problem (MWP) is a challenging\nand critical task in natural language process-\ning. Many recent studies formalize MWP as\na generation task and have adopted sequence-\nto-sequence models to transform problem de-\nscriptions to mathematical expressions. How-\never, mathematical expressions are prone to\nminor mistakes while the generation objec-\ntive does not explicitly handle such mistakes.\nTo address this limitation, we devise a new\nranking task for MWP and propose Gener-\nate & Rank, a multi-task framework based\non a generative pre-trained language model.\nBy joint training with generation and ranking,\nthe model learns from its own mistakes and\nis able to distinguish between correct and in-\ncorrect expressions. Meanwhile, we perform\ntree-based disturbance specially designed for\nMWP and an online update to boost the ranker.\nWe demonstrate the effectiveness of our pro-\nposed method on the benchmark and the re-\nsults show that our method consistently out-\nperforms baselines in all datasets. Particularly,\nin the classical Math23k, our method is 7%\n(78.4% \u2192 85.4%) higher than the state-of-the-\nart1.\n1\nIntroduction\nSolving math word problems (MWP) (Bobrow,\n1964) is an important and fundamental task in nat-\nural language processing (NLP), which requires to\nprovide a solution expression given a mathematical\nproblem description, as illustrated in Table 1. Many\nrecent studies formalize MWP as a generation task\nand commonly adopt LSTM-based sequence-to-\nsequence (Seq2Seq) models (Wang et al., 2017,\n2018b; Xie and Sun, 2019), where problem texts\nare source sequences, mathematical expressions are\ntarget sequences and the model learns the mapping\n\u2020 This work is done when Jianhao Shen is an intern at\nHuawei Noah\u2019s Ark Lab\n*Corresponding author\n1Code will be available soon.\nOriginal MWP\nProblem\nA project is completed in 25 days by 12\nworkers. If it takes 20 days to complete,\nhow many workers will it take?\nSolution\n25 * 12 / 20\nNumber-mapped MWP\nProblem\nA project is completed in NUM0 days by\nNUM1 workers. If it takes NUM2 days to\ncomplete, how many workers will it take?\nSolution\nNUM0 * NUM1 / NUM2\nTable 1: An example of MWP, where numbers are usu-\nally mapped to special tokens, such as Num0/1/2.\nfrom source texts to target expressions. These stud-\nies have proposed numerous advanced techniques\nto improve the MWP solver, but their performance\nis still unsatisfactory yet.\nWe argue that it is not suf\ufb01cient to model MWP\nas only a generation task, because there is a sig-\nni\ufb01cant difference between mathematical expres-\nsions and natural language sequences: one minor\nmistake in a mathematical expression will change\nthe whole semantic thus lead to a wrong answer,\nwhereas natural language is more robust to such\nminor mistakes. The objective function of the gen-\neration task is to maximize generation likelihood\non ground-truth expressions, which does not have\nan explicit strategy to make the model learn to\ndistinguish between ground-truth and expressions\nthat have minor mistakes. In addition, previous\nworks (Liu et al., 2019a; Xie and Sun, 2019; Zhang\net al., 2020) \ufb01nd that the performance of generation\nmodels degrades fast as the expression gets longer.\nTo handle the above problems, we propose Gen-\nerate & Rank, a multi-task framework for MWP,\nwhich introduces a new ranker to explicitly distin-\nguish between correct and incorrect expressions.\nSpeci\ufb01cally, our framework includes two modules:\na generator and a ranker. The former is designed\nto generate candidate expressions given a prob-\nlem text and the latter aims to rank the candidate\narXiv:2109.03034v1  [cs.CL]  7 Sep 2021\n expressions. They are built based on an encoder-\ndecoder model and are jointly trained with genera-\ntion loss and ranking loss. In this work, we build\nour model based on BART (Lewis et al., 2020),\na widely used pre-trained language model that\nachieves SOTA performance on various sequence-\nto-sequence tasks (Ahmad et al., 2021; Liu et al.,\n2020). During multi-task training, expressions pro-\nduced by the generator are used to construct an\nexpression bank and train the ranker, in which\nway the model can learn from its own mistakes.\nTo construct more informative candidates for the\nranker, we specially design tree-based disturbance\nfor MWP. We also introduce an online update mech-\nanism to generate a new set of candidate expres-\nsions at each training epoch. The overall train-\ning procedure is in an iterative manner, in which\nthe ranker and generator continue to enhance each\nother.\nTo evaluate the effectiveness of the proposed\nmodel, we conduct extensive experiments on the\ndatasets of Math23K (Wang et al., 2017) and\nMAWPS (Koncel-Kedziorski et al., 2016). The\nresults show that our model outperforms typical\nbaselines. Particularly, we obtain an improvement\nof 7% in the Math23K dataset that is extensively\nstudied. Moreover, we do ablation study and model\nanalysis, which shows that (1) joint training im-\nproves the performance of the generator and ranker\nover separate training; (2) both strategies of con-\nstructing candidate expressions and online updating\nare important to the success of the ranker. We also\n\ufb01nd that with the ranker, our model achieves a large\nimprovement in generation of long expressions.\nThe contributions of our work are two-fold: (1)\nWe propose Generate & Rank, a new multi-task\nframework to train a pre-trained language model\nfor math word problem solving. To construct infor-\nmative candidate expressions for the ranker, we pro-\npose two effective generation methods and also in-\ntroduce an online update strategy. (2) Experiments\nshow that our proposed model consistently outper-\nforms the state-of-the-art models and achieves a\nsigni\ufb01cant improvement on the Math23K dataset.\n2\nPreliminaries\n2.1\nMath Word Problem\nA math word problem P is a sequence of word to-\nkens and numeric values, which typically describes\na partial quantitative state of a world and some up-\ndates or relationships among quantities, then asks a\nquestion about an unknown quantity. The solution\nS to the question is a mathematical expression that\nconsists of math operators and numbers. In solving\na math word problem, we usually do not care about\nthe speci\ufb01c number of a quantity, so the numbers\nin problems and solution expressions are mapped\nto special tokens NUM#i according to their orders\nin the problem text. Table 1 gives an example of an\noriginal math word problem and the corresponding\nnumber-mapped problem.\n2.2\nBART\nBART is a widely-used pre-trained language model.\nIt follows a standard encoder-decoder structure us-\ning Transformer layers (Vaswani et al., 2017) and\nis pre-trained with text denoising tasks. The pre-\ntrained BART can be \ufb01ne-tuned for tasks of se-\nquence classi\ufb01cation and generation.\nTransformer-based Encoder-Decoder.\nBART\nuses an encoder-decoder structure that is the\nmainstream architecture for sequence-to-sequence\ntasks. The encoder adopts the bidirectional self-\nattention to map an input sequence of tokens P =\n(x1, x2, . . . , xn) to a sequence of continuous rep-\nresentations R = (r1, r2, . . . , rn). The BART en-\ncoder is composed of multiple Transformer layers,\neach consists of a multi-head self-attention (MHA)\nmodule and a fully connected feed-forward (FFN)\nmodule. We denote the mapping function of the\nBART encoder as follows:\n(r1, r2, . . . , rn) = BARTEnc(x1, x2, . . . , xn)\n(1)\nThe BART decoder also consists of multiple\nTransformer layers. Besides MHA and FFN mod-\nules, the decoder layer adds another multi-head\nattention over the output of the encoder. The de-\ncoder takes in one token si at a time, and gives an\noutput state based on the output of the encoder and\nprevious tokens in the decoder input. This output\nstate is then fed into a linear transformation fol-\nlowed by a softmax function to get the predicted\nnext-token probabilities. This one-step decoding\nprocess is denoted as follows:\nP(\u2217) = softmax(diW + b)\n(2)\ndi = BARTDec(R; s0, s1, . . . , si\u22121),\n(3)\nwhere s0 is a special [bos] token indicating the\nstart of decoding, and R is the output of encoder.\nBART Pre-training. BART is pre-trained by the\ntasks of recovering a corrupted document to orig-\n Task #1: Generating\nExpression\nGround-truth\nEn/Decoder\nShared BART\nRanker\nTask #2: Ranking\nGenerating Loss\nRanking Loss\nExpression\nExpression\nExpression\nGenerate\nCandidates\nExpression \nBank \nEncoder\nProblem\nProblem\nDisturb\nDecoder\nScore\nDecoder\nEncoder\n\ufffd Multi-task Training\n\ufffd Expression Online Updating\nExpression\n+\nFigure 1: Our proposed Generate & Rank framework for BART-based MWP solver. The model consists of a\ngenerator and a ranker. They share BART encoder and decoder, and are jointly trained with generating loss and\nranking loss. We construct an expression bank for training the ranker with expressions produced by the generator\nand ones obtained by tree-based disturbance. The expression bank is updated every epoch so that the model can\nconstantly learn from new informative examples.\ninal one. The input to BART is corrupted in two\nways: (1) a number of text spans are replaced with a\nsingle [MASK] token; (2) sentences in a document\nare shuf\ufb02ed in a random order. The objective of\nBART pre-training is to minimize the cross-entropy\nloss between the decoder\u2019s generation probabilities\nand the ground-truth of original document.\n3\nMethodology\nWe propose Generate & Rank, a BART-based multi-\ntask framework for math word problems.\nOur\nmodel consists of a generator and a ranker, which\nshare a BART model and are jointly trained with a\ngenerating task and ranking task. The objective of\ngenerating is to generate expressions given a math\nword problem. We also add a ranking task so that\nthe model can select a correct expression from a\nset of candidates. We construct an expression bank\nto provide training examples for the ranker. Figure\n1 shows our proposed framework and we introduce\ndetails for each task and the whole framework in\nthe following sections.\n3.1\nMulti-task Training\nTask #1: Generating. We \ufb01rst formulate the math\nword problem as a sequence-to-sequence task, in\nwhich BART is trained to generate solution ex-\npressions given a math word problem. Follow-\ning the \ufb01ne-tuning strategy of BART (Lewis et al.,\n2020), we take problem text, a sequence of tokens\nP = (x1, x2, . . . , xn), as input to BART encoder,\nand minimize negative log-likelihood of the solu-\ntion expression S = (s1, s2, . . . , sm),\nJGEN =\n1\n|D|\n\ufffd\n(P,S)\u2208D\n\u2212 log Pr(S|P),\n(4)\nwhere the conditional probability is decomposed in\nan auto-regressive way as:\nPr(S|P) =\nm\n\ufffd\ni=1\nPr(si|P, Sj<i)\n(5)\nPr(\u2217|P, Sj<i) = softmax(diW + b)\n(6)\ndi = BARTDec(R; Sj<i)\n(7)\nR = BARTEnc(P).\n(8)\nAdditionally, we add two special tokens s1 =[bos]\nand sm =[eos] to indicate the start and end sym-\nbols of decoding sequences.\nTask #2: Ranking. Through generating, we obtain\nmany candidate solution expressions. To decide\nwhich expression is a correct solution to the prob-\nlem, we propose a ranking task which is essentially\na task of sequence pair classi\ufb01cation. Given pairs\nof problems and candidate expressions, the ranker\nchooses the expression with highest ranking score\nas the \ufb01nal solution to the problem. Speci\ufb01cally,\nwe add an MLP classi\ufb01er on top of the \ufb01nal layer\nhidden state of the last decoder token. The last\n decoder token is always a special [eos] token and\nits corresponding hidden state can attend to all to-\nken representations of problem text and expression.\nSame as the generation task, we feed the problem\ntext into the encoder and expression into the de-\ncoder, obtaining sequence representations. The last\ndecoder representation is then taken as input to the\nclassi\ufb01er for ranking score prediction:\nPr(\u00b7|P, S) = softmax(d\u2032\nm+1)\n(9)\nd\u2032\nm+1 = tanh(dm+1W1 + b1)W2 + b2\n(10)\ndm+1 = BARTDec(R; S),\n(11)\nwhere R is the output of the encoder, S is the\nexpression token sequence, dm+1 is the decoder\nrepresentation of the last token, and W1|2 and b1|2\nare trainable parameters. The training objective\nof the ranker is cross-entropy between classi\ufb01er\noutput and correct labels,\nJRANK = \u2212\n1\n|D+ \u222a D\u2212|\n\ufffd\n\ufffd\n(P,S)\u2208D+\nlog Pr(1|P, S)\n+\n\ufffd\n(P,S)\u2208D\u2212\nlog Pr(0|P, S)\n\ufffd\n(12)\nwhere D+ and D\u2212 are sets of positive and nega-\ntive examples, respectively. We introduce how to\ngenerate negative examples in the next section.\nOptimization Objective. We train the model on\nthe joint loss of two tasks together:\nJ = JGEN + JRANK.\n(13)\nand the two modules share BART parameters.\n3.2\nExpression Bank\nBy de\ufb01nition, any expression that does not equal\nthe ground-truth can serve as a negative example,\nbut we cannot use all of them due to limited com-\nputational resources. To train the ranker ef\ufb01ciently,\nwe use two different strategies, namely model-\nbased generation and tree-based disturbance, to\nconstruct an expression bank for ranker training.\nModel-based Generation. The \ufb01rst strategy is\nto produce new expressions with the generator.\nSpeci\ufb01cally, given a problem, we use beam search\nwith the generator to produce top-K expressions.\nEach expression is labeled as positive or negative\ndepending on whether its calculation result equals\nthe result of ground-truth.\nTree-based Disturbance. Our second way to con-\nstruct new expressions is adding disturbance to\nground-truth expressions. We design four kinds of\ndisturbances which are illustrated in Figure 2. The\nground-truth expression is \ufb01rst transformed to an\nabstract syntax tree (AST) (Liu et al., 2019a). Then\nwe disturb tree nodes or sub-structures to produce\nnew expressions in four ways: a) Expand. A leaf\nnode is expanded into a sub-tree with a new oper-\nation and a number. b) Edit. A node is randomly\nchanged to another while keeping the expression\nvalid (i.e., a number node will be changed to an-\nother number, and an operator node to another op-\nerator). c) Delete. Delete a leaf node and replace\nits father with its sibling node. d) Swap. Swap the\nleft and right children of an operation node.\nWe use the above methods to construct the ex-\npression bank. Since new expressions may also\nbe correct (for example, swapping two operands\nof addition or multiplication), we compare the nu-\nmerical results of newly obtained expressions with\nthat of the ground-truth, and add them to positive\nor negative samples depending on the comparison.\nThen both positive and negative pairs are sampled\nfrom this expression bank for the multi-task train-\ning. In order to make the model learn with more\ninformative examples, we do an online update for\nexpression bank, which means that we use new ex-\npressions obtained by model-based generation and\ntree-based disturbance at each training epoch.\n/\n+\n+\nNUM1 / (NUM2 + NUM3)\n( NUM1 + NUM3 ) / (NUM2 + NUM3)\n(a) Expand\nNUM1 / (NUM2 - NUM3)\n(b) Edit\nNUM1 / NUM3\n(c) Delete\n(NUM2 + NUM3) / NUM1\n(d) Swap\nGround-truth\nNUM1\nNUM2\nNUM3\n/\n+\nNUM2\nNUM3\nNUM3\nNUM1\n/\n-\nNUM1\nNUM2\nNUM3\n/\nNUM1\nNUM3\n/\n+\nNUM1\nNUM2\nNUM3\nFigure 2: Overview of tree-based disturbance.\n Algorithm 1 Training Algorithm\nInput: MWP Dataset D = {(P, S)}\nParameter: Pre-trained BART encoder and de-\ncoder parameters \u03b8e and \u03b8d, random initialized\nranker \u03b8v, beam size K, epoch number M\n1: // Fine-tune the generator\n2: for epoch = 1 to M do\n3:\nFine-tuning BART encoder \u03b8e and decoder\n\u03b8d on D with generation loss Eq. (4).\n4: end for\n5: // Construct expression bank\n6: D+ \u2190 D, D\u2212 \u2190 {}\n7: for (P, S) \u2208 D do\n8:\nGenerate top-K expressions { \u00afSi} for prob-\nlem P with beam search\n9:\nGet new expressions { \u00afS\u2032\ni} by adding tree-\nbased disturbance to S\n10:\n{ \u00afSi} \u2190 { \u00afSi} \u222a { \u00afS\u2032\ni}\n11:\nfor \u00afS \u2208 { \u00afSi} do\n12:\nif result of \u00afS equals result of S then\n13:\nD+ \u2190 D+ \u222a {(P, \u00afS)}\n14:\nelse\n15:\nD\u2212 \u2190 D\u2212 \u222a {(P, \u00afS)}\n16:\nend if\n17:\nend for\n18: end for\n19: // Joint training\n20: for epoch = 1 to M do\n21:\nTrain \u03b8e, \u03b8d, \u03b8v w.r.t. the joint loss Eq.(13)\non D+ and D\u2212\n22:\nRepeat lines 6-18 to reconstruct expression\nbank\n23: end for\n3.3\nTraining Procedure\nThe training procedure includes multi-task train-\ning and expression online updating. We \ufb01rst \ufb01ne-\ntune the pre-trained BART for the generation task\n(JGEN in Eq. 4). After that, we use the \ufb01ne-tuned\nBART and tree-based disturbance to generate ex-\npressions as the training samples for the ranker.\nThen we do the joint training of generation and\nranking. This process is performed in an itera-\ntive manner and the two modules (i.e., generator\nand ranker) continue to enhance each other. Mean-\nwhile, training examples for ranking are updated\nafter each epoch. We summarize the overall train-\ning procedure in Algorithm 1.\n3.4\nModel Inference\nWe perform a two-stage model inference, namely\ngeneration and ranking. Speci\ufb01cally, given a new\nproblem text sequence P, we \ufb01rst pass it to the\nencoder to get the problem representation R. Then\nwe perform the beam search to generate top-K ex-\npressions. These generated expressions are used as\ncandidate solutions for the ranker. All expressions\nare passed to the ranker and that with the highest\nscore is selected as the \ufb01nal result.\n4\nExperiment\n4.1\nExperimental Setup\nDatasets. We conduct the experiments on two\ncommonly-used datasets: Math23K (Wang et al.,\n2017) and MAWPS (Koncel-Kedziorski et al.,\n2016). Math23K is a large-scale Chinese dataset\nthat contains 23,162 math word problems and their\ncorresponding expression solutions. MAWPS is a\nEnglish dataset containing 2,373 problems. All the\nproblems are one-unknown-variable linear prob-\nlems and can be solved with a single expression.\nBaselines. We compare our model with the follow-\ning baselines including the state-of-the-art models:\nDNS (Wang et al., 2017) uses a vanilla Seq2Seq\nmodel to generate expressions. Math-EN (Wang\net al., 2018b) uses the equation normalization to\navoid equation duplication problem. T-RNN (Wang\net al., 2019b) applies recursive neural networks\nto model the tree structures of expressions.\nS-\nAligned (Chiang and Chen, 2019) tracks the se-\nmantic meanings of operands with a stack during\ndecoding. Group-ATT (Li et al., 2019) leverages\nthe attention mechanism to enrich problem repre-\nsentation. Both AST-Dec (Liu et al., 2019a) and\nGTS (Xie and Sun, 2019) develop a tree-based de-\ncoder to generate expressions. Graph2Tree (Zhang\net al., 2020) proposes to build a quantity cell graph\nand a comparison graph to better capture the quan-\ntity relationships of the problem. Multi-E/D (Shen\nand Jin, 2020) is an ensemble model which com-\nbines multiple encoders and decoders.\nImplementation Details. We use the PyTorch2\nimplementations and pre-trained language models\nprovided by the Transformers library3. Since the\nMath23K dataset is a Chinese dataset and of\ufb01cially\nreleased BART is only for English, we switch to\n2https://pytorch.org/\n3https://github.com/huggingface/\ntransformers\n mBART25 (Liu et al., 2020), which is a multilin-\ngual BART for 25 languages including Chinese.\nFor the MAWPS dataset, we also use mBART25.\nWe optimize our model with AdamW (Loshchilov\nand Hutter, 2019). The training hyperparameters\nare set as follows. We set the batch size to 128, the\nlearning rate to 5e-5 and the warm-up ratio to 0.1.\nThe weight decay is set to 0.01. The number of\nepochs M for \ufb01ne-tuning and multi-task training\nare set to 50. We set beam size K to 10 in beam\nsearch and expression bank size to 20 unless oth-\nerwise stated. All experiments are carried out on\nNVIDIA Tesla V100. We use 8 GPUs for training\nand 1 for testing. For our proposed framework, the\ntraining time is 1.5 hours for one epoch and testing\ntime is 15 minutes for the whole test set.\nEvaluation Metric. Both MAWPS and Math23K\nare evaluated with a metric of \u201csolution accuracy\u201d,\nthat is, the expression is considered as correct if it\ninduces the same number as the ground-truth. For\nthe Math23K dataset, some baselines are evaluated\nusing the public available test set while others use\nthe results of 5-fold cross-validation. We report our\nresults on both settings. For the MAWPS dataset,\nmodels are evaluated with 5-fold cross-validation.\n4.2\nResults and Analysis\nEvaluation results of our model and baselines are\nsummarized in Table 2. We observe that: (1) di-\nrect \ufb01ne-tuning of mBART already outperforms the\nstate-of-the-art models on Math23K, which shows\nthe powerful generation ability of mBART. (2)\non MAWPS, mBART outperforms most Seq2Seq\nbaselines but is worse than GTS and Graph2Tree.\nThese two models leverage tree structure of expres-\nsions during decoding which is critical for math\nword problem solving. We believe that pre-trained\nlanguage models would achieve a better perfor-\nmance if combined with structure information, and\nwe leave it as a future work4. (3) Generate &\nRank framework further improves mBART and\nachieves new state-of-the-art results. In particu-\nlar, Generate & Rank outperforms mBART base-\nlines by more than 4% in all the evaluation set-\ntings and also outperforms the previous best mod-\nels by 7% on Math23K\u2020, 7.4% on 5-fold cross-\nvalidation Math23K\u2021. The improvement over pre-\ntrained mBART demonstrates the effectiveness of\n4One may think that the sequence decoder might not al-\nways generate valid expressions. However, we check all ex-\npressions generated by mBART and \ufb01nd that 99.9% are valid.\nour multi-task training framework.\nModel\nMath23K\u2020\nMath23K\u2021\nMAWPS\u2021\nDNS\n-\n58.1\n59.5\nMath-EN\n66.7\n-\n69.2\nT-RNN\n66.9\n-\n66.8\nS-Aligned\n-\n65.8\n-\nGroup-ATT\n69.5\n66.9\n76.1\nAST-Dec\n69.0\n-\n-\nGTS\n75.6\n74.3\n82.6\nGraph2Tree\n77.4\n75.5\n83.7\nMulti-E/D\n78.4\n76.9\n-\nmBART\n80.8\n80.0\n80.1\nGenerate & Rank\n85.4\n84.3\n84.0\nTable 2: Solution accuracy on MAWPS and Math23K.\n\u2020 refers to the result of test set and \u2021 denotes the result\nof 5-fold cross-validation. \u201c-\u201d means that the results\nare not reported in the original papers.\n4.3\nAblation Study and Model Analysis\nTo better understand our model, we further con-\nduct ablation study on Math23K to show how the\nproposed components affect performance.\n4.3.1\nEffect of Joint Training\nTo investigate the effect of joint training, we intro-\nduce the baseline of two-stage training (i.e., w/o\nJoint), which means we \ufb01rst train the generator,\nthen train the ranker, and the modules are trained\nindependently. We also study the effect of joint\ntraining on generation and perform comparison be-\ntween mBART and our generator (i.e., w/o Ranker).\nThe results are listed in Table 3. We can see that the\njoint training brings 2.2% improvement compared\nwith the two-stage training and 2.6% for the gen-\nerator compared with the mBART trained alone,\nsuggesting that the joint training of generator and\nranker bene\ufb01ts each other. Besides, the joint train-\ning is more space ef\ufb01cient since we only need to\nsave one uni\ufb01ed model rather than two.\nModel\nAcc\nGenerate & Rank\n85.4\nw/o Joint\n83.2\nw/o Ranker\n83.4\nw/o both (mBART)\n80.8\nTable 3: Effect of joint training.\n4.3.2\nEffect of Expression Bank Strategy\nWe investigate the effect of different strategies to\nconstruct the expression bank. Here we choose a\nrandom sampling strategy as our baseline, where\n the set of expressions that appeared in the training\ndata is sampled as the expression bank. We eval-\nuate different strategies with and without online\nupdating and summarize the results in Table 4.\nStrategy\nOnline\nw/o Online\nRandom Sample\n75.2\n69.7\nModel\n84.2\n83.2\nModel+Tree\n85.4\n83.1\nTable 4: Accuracy for different expression bank strate-\ngies. The expression bank size is 20 for all settings.\nWe can see that our strategies outperform the\nrandom sampling strategy. Since the ground-truth\ncan not be accessed during model inference, we\ncannot use the tree-based disturbance to generate\ncandidate expressions as in the training phase. This\ndiscrepancy between training and inference leads to\npoor performance if we only use tree-based distur-\nbance to construct the expression bank. However,\ncombining the tree-based disturbance and model-\nbased generation strategies, we can obtain better re-\nsults than the only model-based generation, which\ngives evidence that the tree-based disturbance con-\ntains some informative examples that the generator\ndoes not cover and it is possible to improve the per-\nformance based on the human knowledge of math\nexpression.\nWe can also see that strategies have a perfor-\nmance drop without online updating. We conjec-\nture that without online updating the ranker may\ntend to memorize existing negative expressions\nthus generalize poorly on new problems. As for\nstrategies with model-based generation, there is an-\nother possible reason: the generator keeps updating\nduring multi-task training, so the previously gener-\nated expressions are no longer good samples of the\ncurrent model, and newly generated expressions are\nmore informative. To summarize, both strategies\nof constructing the expressions bank and online\nupdating play an important role in the success of\nthe ranker.\n4.3.3\nImpact of Expression Bank Size\nWe further analyze the impact of expression bank\nsize on the ranker and results are shown in Figure 3.\nIf the model-based generation is used, performance\nreaches the best at expression bank size 20. This\nsuggests that the expression bank size should not\nbe too small nor too large. One possible reason\nmay be that the generated expressions cannot cover\n#Op\nPro\nAST-Dec\nG2T\nmBART\nGenerate & Rank\n1\n17.3\n82.7\n85.5\n90.2\n90.8 (+0.6)\n2\n52.2\n74.5\n83.7\n88.1\n90.2 (+2.1)\n3\n19.1\n59.9\n71.7\n71.2\n79.1 (+7.9)\n4\n6.6\n42.4\n51.5\n53.0\n63.6 (+10.6)\n5\n3.4\n44.1\n38.2\n41.2\n58.8 (+17.6)\n6\n0.9\n55.6\n55.6\n55.6\n88.8 (+33.2)\nTable 5: Accuracy for increasing length of expressions.\n#Op is the number of operations in expressions. Pro de-\nnotes proportion of expressions with different lengths.\npossible mistakes when the expression bank is too\nsmall, and when the expression bank is too large,\nlow-quality expressions may be generated and hin-\nder ranker training. Tree-based disturbance has a\nsimilar trend and the best bank size is 10.\nFigure 3:\nAccuracy with different expression bank\nsizes from 5 to 30.\n4.3.4\nModel Analysis\nIn Table 5, we list how the model accuracy changes\nwith respect to the number of operations in expres-\nsions. We do not discuss the case of 6 operators\nsince it has too few examples and high variance.\nFor expressions less than 6 operators, all models\nperform worse when the expression gets longer.\nThis is as expected since longer expressions re-\nquire more steps of reasoning and have less data to\ntrain. In addition, we also observe that Generate\n& Rank training has larger improvement over \ufb01ne-\ntuned mBART on longer expressions. This implies\nthat our model is more suitable to handle complex\nproblems and expressions.\nFollowing Liu et al. (2019a), we also examine\nthe performance of our model in different domains.\nThe domain of each problem is de\ufb01ned by whether\nit contains any keywords of this domain and we\n use the same keyword list as Liu et al. (2019a).\nTable 6 shows the results. We observe the similar\npattern that the \ufb01ne-tuned mBART has limitations\nin geometry which requires external knowledge\nsuch as formulas for the circumference and area of\na circle. Interestingly, our proposed model mainly\nimproves on these domains. This suggests that the\nranking task may be a better choice to learn and\nuse mathematical knowledge than generating.\nDomain\nPro\nmBART\nGenerate & Rank\nDistance & Speed\n11.8\n83.9\n83.9\nTracing\n2.7\n85.2\n85.2\nEngineering\n5.8\n86.2\n87.9\nInterval\n0.6\n66.7\n66.7\nCircle Geometry\n1.9\n73.7\n78.9\nPlane Geometry\n1.2\n75.0\n83.3\nPro\ufb01t\n1.1\n72.7\n72.7\nSolid Geometry\n1.6\n81.3\n87.5\nInterest Rate\n0.9\n100.0\n100.0\nProduction\n0.4\n100.0\n100.0\nTable 6: Accuracy for different problem domains. Pro\ndenotes the proportion of each domain in the test data.\nNote that the sum of proportion is not 100% since there\nare problems not belonging to any speci\ufb01ed domain.\n5\nRelated Work\n5.1\nMath Word Problem\nRule-based methods. Early approaches on math\nword problems mainly craft rules and templates\nfor pattern matching (Bobrow, 1964; Slagle, 1965;\nFletcher, 1985; Bakman, 2007). These methods\nrely heavily on manual design and can only solve a\nlimited scope of problems.\nParsing-based methods. Later on, researchers use\nstatistical methods to solve MWP and achieve a\ngreat performance improvement. One line of re-\nsearch focuses on semantic parsing, which lever-\nages traditional machine learning techniques to\nidentify entities, quantities, and operators from the\nproblem text. Roy et al. (2015) proposes three\ntypes of classi\ufb01ers to identify different elements of\nproblems. ARIS (Hosseini et al., 2014) splits the\nproblem into fragments and updates a logic tem-\nplate named state by verb categorization. Other\nworks (Sundaram and Khemani, 2015; Mitra and\nBaral, 2016; Liang et al., 2016) follow a similar\nprocess with different templates and annotations.\nTwo-stage methods. Another research line \ufb01rst\nobtains an expression template then maps numbers\nto the template slots. Kushman et al. (2014) train\na classi\ufb01er to select from a set of pre-de\ufb01ned tem-\nplates. Roy and Roth (2015) propose to construct\ncandidate expressions in a bottom-up manner and\ntrain a global scoring function to guide the beam\nsearch process. ALGES (Koncel-Kedziorski et al.,\n2015) converts the process of searching valid ex-\npressions to an integer linear programming prob-\nlem and adopts a different scoring function. Unit-\nDep (Roy and Roth, 2017) proposes Unit Depen-\ndency Graph to enhance the scoring function.\nDeep learning methods. Recently, deep learning\nmodels have become prevailing methods for math\nword problems. DNS (Wang et al., 2017) is the\n\ufb01rst to apply vanilla RNN-based models to MWP.\nMath-EN (Wang et al., 2018b) introduces equation\nnormalization and compares three Seq2Seq mod-\nels on MWP solving. Group-ATT (Li et al., 2019)\nuses multi-head attention to capture different as-\npects of features. Some works also leverage tree\nstructures and graph information to improve per-\nformance (Wang et al., 2019b; Chiang and Chen,\n2019; Liu et al., 2019a; Xie and Sun, 2019; Zhang\net al., 2020). Shen and Jin (2020) propose a model\nof multi-encoders and multi-decoders.\n5.2\nPre-trained Language Model\nPre-trained language models have obtained state-\nof-the-art results in many NLP benchmarks (Wang\net al., 2018a, 2019a). These models are usually\nbased on Transformer layers (Vaswani et al., 2017)\nand trained on large corpus with self-supervised\ntasks. According to their architectures, pre-trained\nlanguage models can be categorized into three\ntypes: encoder-only, decoder-only and encoder-\ndecoder models. BERT (Devlin et al., 2019) is an\nencoder-only model which \ufb01rstly proposes masked\ntoken prediction and next sentence prediction to\ntrain a language representation model. Follow-\ning this, many other models are proposed like\nRoBERTa (Liu et al., 2019b) and SpanBERT (Joshi\net al., 2020). Decoder-only models are typically\nauto-regressive models trained to estimate the prob-\nability distribution of a text corpus, including\nGPT2 (Radford et al., 2019), GPT3 (Brown et al.,\n2020) and XLNet (Yang et al., 2019). Encoder-\ndecoder models like BART (Lewis et al., 2020) and\nT5 (Raffel et al., 2020) use the encoder-decoder ar-\nchitecture and are trained on sequence-to-sequence\ntasks such as text denoising and translation.\n 6\nConclusion and Future Work\nWe propose Generate & Rank, a new multi-task\nframework for math word problems. Speci\ufb01cally,\nour model has a generator and a ranker which en-\nhance each other with joint training. We also use\ntree-based disturbance and online update to further\nimprove the performance. The experimental results\non the benchmark show that our work consistently\noutperforms baselines in all datasets. In future\nwork, we will explore the generation and ranking\nframework to other tasks like summarization and\ntranslation.\nAcknowledgements\nThis paper is partially supported by National Key\nResearch and Development Program of China with\nGrant No. 2018AAA0101900/2018AAA0101902\nas well as the National Natural Science Foundation\nof China (NSFC Grant No. 62106008 and No.\n61772039).\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Uni\ufb01ed pre-training\nfor program understanding and generation. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics.\nYe\ufb01m\nBakman.\n2007.\nRobust\nUnderstanding\nof Word Problems with Extraneous Information.\narXiv:math/0701393.\nDaniel G. Bobrow. 1964. Natural Language Input for a\nComputer Problem Solving System.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,\nSandhini Agarwal,\nAriel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020.\nLanguage Models are Few-Shot\nLearners. arXiv:2005.14165 [cs].\nTing-Rui\nChiang\nand\nYun-Nung\nChen.\n2019.\nSemantically-Aligned\nEquation\nGeneration\nfor\nSolving and Reasoning Math Word Problems.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational\nLinguistics:\nHuman\nLanguage\nTechnologies, Volume 1 (Long and Short Papers),\npages 2656\u20132668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171\u20134186.\nCharles R. Fletcher. 1985. Understanding and solving\narithmetic word problems: A computer simulation.\nBehavior Research Methods, Instruments, & Com-\nputers, 17(5):565\u2013571.\nMohammad Javad Hosseini,\nHannaneh Hajishirzi,\nOren Etzioni, and Nate Kushman. 2014. Learning to\nSolve Arithmetic Word Problems with Verb Catego-\nrization. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 523\u2013533.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving Pre-training by Representing\nand Predicting Spans. Transactions of the Associa-\ntion for Computational Linguistics, 8(0):64\u201377.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing Algebraic Word Problems into Equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585\u2013597.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A Math Word Problem Repository.\nIn\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1152\u20131157.\nNate Kushman, Yoav Artzi, Luke Zettlemoyer, and\nRegina Barzilay. 2014. Learning to Automatically\nSolve Algebra Word Problems. In Proceedings of\nthe 52nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 271\u2013281.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension.\nIn Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7871\u20137880.\nJierui Li,\nLei Wang,\nJipeng Zhang,\nYan Wang,\nBing Tian Dai, and Dongxiang Zhang. 2019. Mod-\neling Intra-Relation in Math Word Problems with\nDifferent Functional Multi-Head Attentions. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6162\u2013\n6167.\n Chao-Chun\nLiang,\nKuang-Yi\nHsu,\nChien-Tsung\nHuang, Chung-Min Li, Shen-Yu Miao, and Keh-\nYih Su. 2016. A tag-based statistical English math\nword problem solver with understanding, reasoning\nand explanation. In Proceedings of the Twenty-Fifth\nInternational Joint Conference on Arti\ufb01cial Intelli-\ngence, IJCAI\u201916, pages 4254\u20134255.\nQianying Liu, Wenyv Guan, Sujian Li, and Daisuke\nKawahara. 2019a.\nTree-structured Decoding for\nSolving Math Word Problems.\nIn Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2370\u20132379.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020.\nMultilingual Denoising\nPre-training for Neural Machine Translation. Trans-\nactions of the Association for Computational Lin-\nguistics, 8(0):726\u2013742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nIlya Loshchilov and Frank Hutter. 2019.\nDecou-\npled weight decay regularization.\nIn 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nArindam Mitra and Chitta Baral. 2016. Learning To\nUse Formulas To Solve Simple Arithmetic Problems.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2144\u20132153.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020.\nExploring\nthe Limits of Transfer Learning with a Uni\ufb01ed Text-\nto-Text Transformer. Journal of Machine Learning\nResearch, 21(140):1\u201367.\nSubhro Roy and Dan Roth. 2015.\nSolving General\nArithmetic Word Problems. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1743\u20131752.\nSubhro Roy and Dan Roth. 2017. Unit Dependency\nGraph and Its Application to Arithmetic Word Prob-\nlem Solving. Proceedings of the AAAI Conference\non Arti\ufb01cial Intelligence, 31(1).\nSubhro Roy, Tim Vieira, and Dan Roth. 2015. Reason-\ning about Quantities in Natural Language. Transac-\ntions of the Association for Computational Linguis-\ntics, 3:1\u201313.\nYibin Shen and Cheqing Jin. 2020. Solving Math Word\nProblems with Multi-Encoders and Multi-Decoders.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 2924\u20132934.\nJames R. Slagle. 1965. Experiments with a deductive\nquestion-answering program.\nCommunications of\nthe ACM, 8(12):792\u2013798.\nSowmya S Sundaram and Deepak Khemani. 2015. Nat-\nural Language Processing for Solving Simple Word\nProblems. In Proceedings of the 12th International\nConference on Natural Language Processing, pages\n394\u2013402.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, unde\ufb01ne-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS\u201917, page 6000\u20136010.\nAlex Wang,\nYada Pruksachatkun,\nNikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. Superglue:\nA stickier benchmark for general-purpose language\nunderstanding systems. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\npages 3261\u20133275.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding.\nIn\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353\u2013355.\nLei Wang, Yan Wang, Deng Cai, Dongxiang Zhang,\nand Xiaojiang Liu. 2018b. Translating a Math Word\nProblem to a Expression Tree. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1064\u20131069.\nLei Wang, Dongxiang Zhang, Jipeng Zhang, Xing\nXu, Lianli Gao, Bing Tian Dai, and Heng Tao\nShen. 2019b.\nTemplate-Based Math Word Prob-\nlem Solvers with Recursive Neural Networks. Pro-\nceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence, 33(01):7144\u20137151.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 845\u2013\n854.\nZhipeng Xie and Shichao Sun. 2019. A Goal-Driven\nTree-Structured Neural Model for Math Word Prob-\nlems.\nIn Proceedings of the Twenty-Eighth Inter-\nnational Joint Conference on Arti\ufb01cial Intelligence,\npages 5299\u20135305.\n Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding.\nIn Advances in Neural\nInformation Processing Systems, volume 32, pages\n5754\u20135764.\nJipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan\nWang, Jie Shao, and Ee-Peng Lim. 2020. Graph-to-\nTree Learning for Solving Math Word Problems. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3928\u2013\n3937.\n"}, "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations": {"authors": ["Betty van Aken", "Benjamin Winter", "Alexander L\u00f6ser", "Felix A. Gers"], "title": "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations", "url": "https://arxiv.org/pdf/1909.04925.pdf", "abstract": "Bidirectional Encoder Representations from Transformers (BERT) reach state-of-the-art results in a variety of Natural Language Processing tasks. However, understanding of their internal functioning is still insufficient and unsatisfactory. In order to better understand BERT and other Transformer-based models, we present a layer-wise analysis of BERT's hidden states. Unlike previous research, which mainly focuses on explaining Transformer models by their attention weights, we argue that hidden states contain equally valuable information. Specifically, our analysis focuses on models fine-tuned on the task of Question Answering (QA) as an example of a complex downstream task. We inspect how QA models transform token vectors in order to find the correct answer. To this end, we apply a set of general and QA-specific probing tasks that reveal the information stored in each representation layer. Our qualitative analysis of hidden state visualizations provides additional insights into BERT's reasoning process. Our results show that the transformations within BERT go through phases that are related to traditional pipeline tasks. The system can therefore implicitly incorporate task-specific information into its token representations. Furthermore, our analysis reveals that fine-tuning has little impact on the models' semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.", "arxiv_id": "1909.04925", "published_date": "2019-09-11", "year": 2019, "introduction": "INTRODUCTION In recent months, Transformer models have become more and more prevalent in the field of Natural Language Processing. Originally they became popular for their improvements over RNNs in Machine Translation [36]. Now however, with the advent of large models and an equally large amount of pre-training being done, they have proven adept at solving many of the standard Natural Language Processing tasks. Main subject of this paper is BERT [8], arguably the most popular of the recent Transformer models and the first to display significant improvements over previous state-of-the-art models in a number of different benchmarks and tasks. Problem of black box models. Deep Learning models achieve increasingly impressive results across a number of different domains, whereas their application to real-world tasks has been moving somewhat more slowly. One major impediment lies in the lack of transparency, reliability and prediction guarantees in these largely black box models. While Transformers are commonly believed to be moderately interpretable through the inspection of their attention values, current research suggests that this may not always be the case [16]. This paper takes a different approach to the interpretation of said Transformer Networks. Instead of evaluating attention values, our approach examines the hidden states between encoder layers directly. There are multiple questions this paper will address: (1) Do Transformers answer questions decompositionally, in a similar manner to humans? (2) Do specific layers in a multi-layer Transformer network solve different tasks? (3) How does fine-tuning influence the network\u2019s inner state? (4) Can an evaluation of network layers help determine why and how a network failed to predict a correct answer? We discuss these questions on the basis of fine-tuned models on standard QA datasets. We choose the task of Question Answering as an example of a complex downstream task that, as this paper will show, requires solving a multitude of other Natural Language Processing tasks. Additionally, it has been shown that other NLP tasks can be successfully framed as QA tasks [23], therefore our analysis should translate to these tasks as well. While this work focuses on the BERT architecture, we perform preliminary tests on the small GPT-2 model [29] as well, which yield similar results. arXiv:1909.04925v1  [cs.CL]  11 Sep 2019 ", "conclusion": "CONCLUSION AND FUTURE WORK Our work reveals important findings about the inner functioning of Transformer networks. The impact of these findings and how future work can build upon them is described in the following: ", "full_text": "How Does BERT Answer Questions?\nA Layer-Wise Analysis of Transformer Representations\nBetty van Aken\u2217\nbvanaken@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nBenjamin Winter\u2217\nBenjamin.Winter@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nAlexander L\u00f6ser\naloeser@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nFelix A. Gers\ngers@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nABSTRACT\nBidirectional Encoder Representations from Transformers (BERT)\nreach state-of-the-art results in a variety of Natural Language Pro-\ncessing tasks. However, understanding of their internal functioning\nis still insufficient and unsatisfactory. In order to better under-\nstand BERT and other Transformer-based models, we present a\nlayer-wise analysis of BERT\u2019s hidden states. Unlike previous re-\nsearch, which mainly focuses on explaining Transformer models\nby their attention weights, we argue that hidden states contain\nequally valuable information. Specifically, our analysis focuses on\nmodels fine-tuned on the task of Question Answering (QA) as an\nexample of a complex downstream task. We inspect how QA models\ntransform token vectors in order to find the correct answer. To this\nend, we apply a set of general and QA-specific probing tasks that\nreveal the information stored in each representation layer. Our qual-\nitative analysis of hidden state visualizations provides additional\ninsights into BERT\u2019s reasoning process. Our results show that the\ntransformations within BERT go through phases that are related\nto traditional pipeline tasks. The system can therefore implicitly\nincorporate task-specific information into its token representations.\nFurthermore, our analysis reveals that fine-tuning has little impact\non the models\u2019 semantic abilities and that prediction errors can be\nrecognized in the vector representations of even early layers.\nKEYWORDS\nneural networks, transformers, explainability, word representation,\nnatural language processing, question answering\nACM Reference Format:\nBetty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019.\nHow Does BERT Answer Questions? A Layer-Wise Analysis of Transformer\nRepresentations. In The 28th ACM International Conference on Information\nand Knowledge Management (CIKM \u201919), November 3\u20137, 2019, Beijing, China.\nACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3357384.3358028\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nCIKM \u201919, November 3\u20137, 2019, Beijing, China\n\u00a9 2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-6976-3/19/11...$15.00\nhttps://doi.org/10.1145/3357384.3358028\n1\nINTRODUCTION\nIn recent months, Transformer models have become more and more\nprevalent in the field of Natural Language Processing. Originally\nthey became popular for their improvements over RNNs in Machine\nTranslation [36]. Now however, with the advent of large models\nand an equally large amount of pre-training being done, they have\nproven adept at solving many of the standard Natural Language\nProcessing tasks. Main subject of this paper is BERT [8], arguably\nthe most popular of the recent Transformer models and the first\nto display significant improvements over previous state-of-the-art\nmodels in a number of different benchmarks and tasks.\nProblem of black box models. Deep Learning models achieve in-\ncreasingly impressive results across a number of different domains,\nwhereas their application to real-world tasks has been moving\nsomewhat more slowly. One major impediment lies in the lack of\ntransparency, reliability and prediction guarantees in these largely\nblack box models.\nWhile Transformers are commonly believed to be moderately\ninterpretable through the inspection of their attention values, cur-\nrent research suggests that this may not always be the case [16].\nThis paper takes a different approach to the interpretation of said\nTransformer Networks. Instead of evaluating attention values, our\napproach examines the hidden states between encoder layers di-\nrectly. There are multiple questions this paper will address:\n(1) Do Transformers answer questions decompositionally, in a\nsimilar manner to humans?\n(2) Do specific layers in a multi-layer Transformer network\nsolve different tasks?\n(3) How does fine-tuning influence the network\u2019s inner state?\n(4) Can an evaluation of network layers help determine why\nand how a network failed to predict a correct answer?\nWe discuss these questions on the basis of fine-tuned models on\nstandard QA datasets. We choose the task of Question Answering\nas an example of a complex downstream task that, as this paper\nwill show, requires solving a multitude of other Natural Language\nProcessing tasks. Additionally, it has been shown that other NLP\ntasks can be successfully framed as QA tasks [23], therefore our\nanalysis should translate to these tasks as well. While this work\nfocuses on the BERT architecture, we perform preliminary tests on\nthe small GPT-2 model [29] as well, which yield similar results.\narXiv:1909.04925v1  [cs.CL]  11 Sep 2019\n Contributions. With the goal to improve understanding of internal\nworkings of Transformers we present the following contributions:\nFirst, we propose a layer-wise visualisation of token represen-\ntations that reveals information about the internal state of Trans-\nformer networks. This visualisation can be used to expose wrong\npredictions even in earlier layers or to show which parts of the\ncontext the model considered as Supporting Facts.\nSecond, we apply a set of general NLP Probing Tasks and extend\nthem by the QA-specific tasks of Question Type Classification and\nSupporting Fact Extraction. This way we can analyse the abilities\nwithin BERT\u2019s layers and how they are impacted by fine-tuning.\nThird, we show that BERT\u2019s transformations go through similar\nphases, even if fine-tuned on different tasks. Information about gen-\neral language properties is encoded in earlier layers and implicitly\nused to solve the downstream task at hand in later layers.\n2\nRELATED WORK\nTransformer Models. Our analyses focus on BERT, which belongs\nto the group of Transformer networks, named after how represen-\ntations are transformed throughout the network layers. We also\npartly include the more recent Transformer model GPT-2 [29]. This\nmodel represents OpenAI\u2019s improved version of GPT [28] and while\nGPT-2 has not yet climbed leaderboards like BERT has, its larger\nversions have proven adept enough at the language modeling task,\nthat Open-AI has decided not to release their pre-trained models.\nThere are also other Transformer models of note, where a similar\nanalysis might prove interesting in future work. Chief among them\nare the Universal Transformer [7] and TransformerXL [6], both\nof which aim to improve some of the flaws of the Transformer\narchitecture by adding a recurrent inductive bias.\nInterpretability and Probing. Explainability and Interpretabil-\nity of neural models have become an increasingly large field of\nresearch. While there are a multitude of ways to approach these\ntopics [9, 12, 20], we especially highlight relevant work in the area\nof research that builds and applies probing tasks and methodologies,\npost-hoc, to trained models. There have been a number of recent\nadvances on this topic. While the majority of the current works aim\nto create or apply more general purpose probing tasks [2, 4, 33],\nBERT specifically has also been probed in previous papers. Tenney\net al. [34] proposes a novel \"edge-probing\" framework consisting\nof nine different probing tasks and applies it to the contextualized\nword embeddings of ELMo, BERT and GPT-1. Both semantic and\nsyntactic information is probed, but only pre-trained models are\nstudied, and not specifically fine-tuned ones. A similar analysis [11]\nadds more probing tasks and addresses only the BERT architecture.\nQiao et al. [27] focus specifically on analysing BERT as a Ranking\nmodel. The authors probe attention values in different layers and\nmeasure performance for representations build from different BERT\nlayers. Like [34], they only discuss pre-trained models.\nThere has also been work which studies models not through\nprobing tasks but through qualitative visual analysis. Zhang and\nZhu [41] offer a survey of different approaches, though limited\nto CNNs. Nagamine et al. [25] explore phoneme recognition in\nDNNs by studying single node activations in the task of speech\nrecognition. Hupkes et al. [15] go one step further, by not only\ndoing a qualitative analysis, but also training diagnostic classifiers\nto support their hypotheses. Finally, Li et al. [18] take a look at word\nvectors and the importance of some of their specific dimensions on\nboth sequence tagging and classification tasks.\nThe most closely related previous work is proposed by Liu et al.\n[21]. Here, the authors also perform a layer-wise analysis of BERT\u2019s\ntoken representations. However, their work solely focuses on prob-\ning pre-trained models and disregards models fine-tuned on down-\nstream tasks. Furthermore, it limits the analysis to the general\ntransferability of the network and does not analyze the specific\nphases that BERT goes through.\nAdditionally, our work is motivated by Jain and Wallace [16]. In\ntheir paper, the authors argue that attention, at least in some cases,\nis not well suited to solve the issues of explainability and inter-\npretability. They do so both by constructing adversarial examples\nand by a comparison with more traditional explainability methods.\nIn supporting this claim, we propose revisiting evaluating hidden\nstates and token representations instead.\n3\nBERT UNDER THE MICROSCOPE\nWe focus our analysis on fine-tuned BERT models. In order to\nunderstand which transformations the models apply to input tokens,\nwe take two approaches: First, we analyse the transforming token\nvectors qualitatively by examining their positions in vector space.\nSecond, we probe their language abilities on QA-related tasks to\nexamine our results quantitatively.\n3.1\nAnalysis of Transformed Tokens\nThe architecture of BERT and Transformer networks in general al-\nlows us to follow the transformations of each token throughout the\nnetwork. We use this characteristic for an analysis of the changes\nthat are being made to the tokens\u2019 representations in every layer.\nWe use the following approach for a qualitative analysis of these\ntransformations: We randomly select both correctly and falsely\npredicted samples from the test set of the respective dataset. For\nthese samples we collect the hidden states from each layer while\nremoving any padding. This results in the representation of each\ntoken throughout the model\u2019s layers.\nThe model can transform the vector space freely throughout its\nlayers and we do not have references for semantic meanings of po-\nsitions within these vector spaces. Therefore we consider distances\nbetween token vectors as indication for semantic relations.\nDimensionality Reduction. BERT\u2019s pre-trained models use vec-\ntor dimensions of 1024 (large model) and 512 (base model). In order\nto visualize relations between tokens, we apply dimensionality re-\nduction and fit the vectors into two-dimensional space. To that\nend we apply T-distributed Stochastic Neighbor Embedding (t-SNE)\n[35], Principal Component Analysis (PCA) [10] and Independent\nComponent Analysis (ICA) [3] to vectors in each layer. As the re-\nsults of PCA reveal the most distinct clusters for our data, we use\nit to present our findings.\nK-means Clustering. In order to verify that clusters in 2D space\nrepresent the actual distribution in high-dimensional vector space,\nwe additionally apply a k-means clustering [22]. We choose the\n Figure 1: Schematic overview of the BERT architecture and\nour probing setup. Question and context tokens are pro-\ncessed by N encoder blocks with a Positional Embedding\nadded beforehand. The output of the last layer is fed into\na span prediction head consisting of a Linear Layer and a\nSoftmax. We use the hidden states of each layer as input to\na set of probing tasks to examine the encoded information.\nnumber of clusters k in regard to the number of observed clusters\nin PCA, which vary over layers. The resulting clusters correspond\nwith our observations in 2D space.\n3.2\nProbing BERT\u2019s Layers\nOur goal is to further understand the abilities of the model after\neach transformation. We therefore apply a set of semantic probing\ntasks to analyze which information is stored within the transformed\ntokens after each layer. We want to know whether specific layers\nare reserved for specific tasks and how language information is\nmaintained or forgotten by the model.\nWe use the principle of Edge Probing introduced by Tenney et al.\n[34]. Edge Probing translates core NLP tasks into classification tasks\nby focusing solely on their labeling part. This enables a standard-\nized probing mechanism over a wide range of tasks. We adopt the\ntasks Named Entity Labeling, Coreference Resolution and Relation\nClassification from the original paper as they are prerequisites for\nlanguage understanding and reasoning [39]. We add tasks of Ques-\ntion Type Classification and Supporting Fact Identification due to\ntheir importance for Question Answering in particular.1\nNamed Entity Labeling. Given a span of tokens the model has to\npredict the correct entity category. This is based on Named Entity\nRecognition but formulated as a Classification problem. The task\nwas modeled by [34], annotations are based on the OntoNotes 5.0\ncorpus [38] and contain 18 entity categories.\nCoreference Resolution. The Coreference task requires the model\nto predict whether two mentions within a text refer to the same\nentity. The task was built from the OntoNotes corpus and enhanced\nwith negative samples by [34].\n1The source code is available at: https://github.com/bvanaken/explain-BERT-QA\nRelation Classification. In Relation Classification the model has\nto predict which relation type connects two known entities. The\ntask was constructed by [34] with samples taken from the SemEval\n2010 Task 8 dataset consisting of English web text and nine direc-\ntional relation types.\nQuestion Type Classification. A fundamental part of answering\na question is to correctly identify its question type. For this Edge\nProbing task we use the Question Classification dataset constructed\nby Li and Roth [19] based on the TREC-10 QA dataset [37]. It in-\ncludes 500 fine-grained types of questions within the larger groups\nof abbreviation, entity, description, human, location and numeric\nvalue. We use the whole question as input to the model with its\nquestion type as label.\nSupporting Facts. The extraction of Supporting Facts is essential\nfor Question Answering tasks, especially in the multi-hop case. We\nexamine what BERT\u2019s token transformations can tell us about the\nmechanism behind identifying important context parts.\nTo understand at which stage this distinction is done, we con-\nstruct a probing task for identifying Supporting Facts. The model\nhas to predict whether a sentence contains supporting facts re-\ngarding a specific question or whether it is irrelevant. Through\nthis task we test the hypothesis that token representations contain\ninformation about their significance to the question.\nBoth HotpotQA and bAbI contain information about sentence-\nwise Supporting Facts for each question. SQuAD does not require\nmulti-hop reasoning, we therefore consider the sentence containing\nthe answer phrase the Supporting Fact. We also exclude all QA-pairs\nthat only contain one context sentence. We construct a different\nprobing task for each dataset in order to check their task-specific\nability to recognize relevant parts. All samples are labeled sentence-\nwise with true if they are a supporting fact or false otherwise.\nProbing Setup. Analogue to the authors of [34], we embed input\ntokens for each probing task sample with our fine-tuned BERT\nmodel. Contrary to previous work, we do this for all layers (N = 12\nfor BERT-base and N = 24 for BERT-large), using only the output\nembedding from n-th layer at step n. The concept of Edge Probing\ndefines that only tokens of \"labeled edges\" (e.g. tokens of two related\nentities for Relation Classification) within a sample are considered\nfor classification. These tokens are first pooled for a fixed-length\nrepresentation and afterwards fed into a two-layer Multi-layer\nPerceptron (MLP) classifier, that predicts label-wise probability\nscores (e.g. for each type of relation). A schematic overview of this\nsetting is shown in Figure 1. We perform the same steps on pre-\ntrained BERT-base and BERT-large models without any fine-tuning.\nThis enables us to identify which abilities the model learns during\npre-training or fine-tuning.\n4\nDATASETS AND MODELS\n4.1\nDatasets\nOur aim is to understand how BERT works on complex downstream\ntasks. Question Answering (QA) is one of such tasks that require a\ncombination of multiple simpler tasks such as Coreference Resolu-\ntion and Relation Modeling to arrive at the correct answer. We take\n SQuAD\nbAbI\nQuestion\nWhat is a common punishment in the UK and Ireland?\nWhat is Emily afraid of?\nAnswer\ndetention\ncats\nContext\nCurrently detention is one of the most common pun-\nishments in schools in the United States, the UK, Ire-\nland, Singapore and other countries. It requires the\npupil to remain in school at a given time in the school\nday (such as lunch, recess or after school); or even to attend\nschool on a non-school day, e.g. \"Saturday detention\" held at\nsome schools. During detention, students normally have to\nsit in a classroom and do work, write lines or a punishment\nessay, or sit quietly.\nWolves are afraid of cats.\nSheep are afraid of wolves.\nMice are afraid of sheep.\nGertrude is a mouse.\nJessica is a mouse.\nEmily is a wolf.\nCats are afraid of sheep.\nWinona is a wolf.\nTable 1: Samples from SQuAD dataset (left) and from Basic Deduction task (#15) of the bAbI dataset (right). Supporting Facts\nare printed in bold. The SQuAD sample can be solved by word matching and entity resolution, while the bAbI sample requires\na logical reasoning step and cannot be solved by simple word matching. Figures in the further analysis will use these examples\nwhere applicable.\nthree current Question Answering datasets into account, namely\nSQUAD [31], bAbI [39] and HotpotQA [40]. We intentionally choose\nthree very different datasets to diversify the results of our analysis.\nSQuAD. As one of the most popular QA tasks the SQuAD dataset\ncontains 100,000 natural question-answer pairs on 500 Wikipedia\narticles. A new version of the dataset called SQuAD 2.0 [30] ad-\nditionally includes unanswerable questions. We use the previous\nversion SQuAD 1.1 for our experiments to concentrate on the base\ntask of span prediction. In 2018 an ensemble of fine-tuned BERT\nmodels has outperformed the Human Baseline on this task. The\ndataset is characterised by questions that mainly require to resolve\nlexical and syntactic variations.\nHotpotQA. This Multihop QA task contains 112,000 natural question-\nanswer pairs. The questions are especially designed to combine\ninformation from multiple parts of a context. We focus on the dis-\ntractor-task of HotpotQA, in which the context is composed of both\nsupporting and distracting facts with an average size of 900 words.\nAs the pre-trained BERT model is restricted to an input size of 512\ntokens, we reduce the amount of distracting facts by a factor of\n2.7. We also leave out yes/no-questions (7% of questions) as they\nrequire additional specific architecture, diluting our analysis.\nbAbI. The QA bAbI tasks are a set of artificial toy tasks developed\nto further understand the abilities of neural models. The 20 tasks\nrequire reasoning over multiple sentences (Multihop QA) and are\nmodeled to include Positional Reasoning, Argument Relation Ex-\ntraction and Coreference Resolution. The tasks strongly differ from\nthe other QA tasks in their simplicity (e.g. vocabulary size of 230\nand short contexts) and the artificial nature of sentences.\n4.2\nBERT and GPT-2\nIn this section we briefly discuss the models our analysis is based\non, BERT [8] and GPT-2 [29]. Both of these models are Trans-\nformers that extend and improve on a number of different recent\nideas. These include previous Transformer models [36][28], Semi-\nSupervised Sequence Learning [5], ELMo [26] and ULMFit [13].\nSQuAD\nHotpotQA Distr.\nHotpotQA SP\nbAbI\nBaseline\n77.2\n66.0\n66.0\n42.0\nBERT\n87.9\n56.8\n80.4\n93.4\nGPT-2\n74.9\n54.0\n64.6\n99.9\nTable 2: Results from fine-tuning BERT on QA tasks. Base-\nlines are: BIDAF [32] for SQuAD, the LSTM Baseline for bAbI\nfrom [39] and the HotpotQA baseline from [40] for the two\nHotpot tasks.\nBoth have a similar architecture, and they each represent one half\nof the original Encoder-Decoder Transformer [36]. While GPT-2,\nlike its predecessor, consists of only the decoder half, BERT uses a\nbidirectional variant of the original encoder. Each consists of a large\nnumber of Transformer blocks (12 for small GPT-2 and bert-base,\n24 for bert-large), that in turn consist of a Self-Attention module,\nFeed Forward network, Layer Normalization and Dropout. On top\nof these encoder stacks we add a Sequence Classification head for\nthe bAbI dataset and a Span Prediction head for the other datasets.\nFigure 1 depicts how these models integrate into our probing setup.\n4.3\nApplying BERT to Question Answering\nWe base our training code on the Pytorch implementation of BERT\navailable at [14]. We use the publicly available pre-trained BERT\nmodels for our experiments. In particular, we study the monolin-\ngual models bert-base-uncased and bert-large. For GPT-2 the small\nmodel (117M Parameters) is used, as a larger model has not yet\nbeen released. However, we do not apply these models directly, and\ninstead fine-tune them on each of our datasets.\nTraining Modalities. Regarding hyperparameters, we tune the\nlearning rate, batch size and learning rate scheduling according to\na grid search and train each model for 5 epochs with evaluations on\nthe development set every 1000 iterations. We then select the model\nof the best evaluation for further analysis. The input length chosen\nis 384 tokens for the bAbI and SQuAD tasks and the maximum of\n Figure 2: Probing Task results of BERT-base models in\nmacro averaged F1 (Y-axis) over all layers (X-axis). Fine-\ntuning barely affects accuracy on NEL, COREF and REL in-\ndicating that those tasks are already sufficiently covered\nby pre-training. Performances on the Question Type task\nshows its relevancy for solving SQuAD, whereas it is not re-\nquired for the bAbI tasks and the information is lost.\nFigure 3: Probing Task results of BERT-large models in\nmacro averaged F1 (Y-axis) over all layers (X-axis). Perfor-\nmance of HotpotQA model is mostly equal to the model\nwithout fine-tuning, but information is dropped in last lay-\ners in order to fit the Answer Selection task.\n512 tokens permitted by the pre-trained models\u2019 positional embed-\nding for the HotpotQA tasks. For bAbI we evaluate both models\nthat are trained on a single bAbI task and also a multitask model,\nthat was trained on the data of all 20 tasks. We further distinguish\nbetween two settings: Span prediction, which we include for better\ncomparison with the other datasets, and Sequence Classification,\nwhich is the more common approach to bAbI. In order to make\nspan prediction work, we append all possible answers to the end of\nthe base context, since not all answers can be found in the context\nby default. For HotpotQA, we also distinguish between two tasks.\nIn the HotpotQA Support Only (SP) task, we use only the sentences\nlabeled as Supporting Facts as the question context. This simpli-\nfies the task, but more importantly it reduces context length and\nincreases our ability to distinguish token vectors. Our HotpotQA\nDistractor task is closer to the original HotpotQA task. It includes\ndistracting sentences in the context, but only enough to not exceed\nthe 512 token limit.\n5\nRESULTS AND DISCUSSION\nTraining Results. Table 2 shows the evaluation results of our best\nmodels. Accuracy on the SQuAD task is close to human perfor-\nmance, indicating that the model can fulfill all sub-tasks required\nto answer SQuAD\u2019s questions. As expected the tasks derived from\nHotpotQA prove much more challenging, with the distractor setting\nbeing the most difficult to solve. Unsurprisingly too, bAbI was easily\nsolved by both BERT and GPT-2. While GPT-2 performs signifi-\ncantly worse in the more difficult tasks of SQuAD and HotpotQA,\nit does considerably better on bAbi reducing the validation error\nto nearly 0. Most of BERT\u2019s error in the bAbI multi-task setting\ncomes from tasks 17 and 19. Both of these tasks require positional\nor geometric reasoning, thus it is reasonable to assume that this is\na skill where GPT-2 improves on BERT\u2019s reasoning capabilities.\nPresentation of Analysis Results. The qualitative analysis of\nvector transformations reveals a range of recurring patterns. In\nthe following, we present these patterns by two representative\nsamples from the SQuAD and bAbI task dataset described in Table\n1. Examples from HotpotQA can be found in the supplementary\nmaterial as they require more space due to the larger context.\nResults from probing tasks are displayed in Figures 2 and 3.\nWe compare results in macro-averaged F1 over all network layers.\nFigure 2 shows results from three models of BERT-base with twelve\nlayers: Fine-tuned on SQuAD,on bAbI tasks and without fine-tuning.\nFigure 3 reports results of two models based on BERT-large with\n24 layers: Fine-tuned on HotpotQA and without fine-tuning.\n5.1\nPhases of BERT\u2019s Transformations\nThe PCA representations of tokens in different layers suggest that\nthe model is going through multiple phases while answering a\nquestion. We observe these phases in all three selected QA tasks de-\nspite their diversity. These findings are supported by results of the\napplied probing tasks. We present the four phases in the following\nparagraphs and describe how our experimental results are linked.\n(1) Semantic Clustering. Early layers within the BERT-based mod-\nels group tokens into topical clusters. Figures 4a and 5a reveal this\nbehaviour and show the second layer of each model. Resulting\nvector spaces are similar in nature to embedding spaces from e.g.\nWord2Vec [24] and hold little task-specific information. Therefore,\nthese initial layers reach low accuracy on semantic probing tasks,\nas shown in Figures 2 and 3. BERT\u2019s early layers can be seen as\nan implicit replacement of embedding layers common in neural\nnetwork architectures.\n(2) Connecting Entities with Mentions and Attributes. In the\nmiddle layers of the observed networks we see clusters of entities\nthat are less connected by their topical similarity. Rather, they\nare connected by their relation within a certain input context.\nThese task-specific clusters appear to already include a filtering of\nquestion-relevant entities. Figure 4b shows a cluster with words\nlike countries, schools, detention and country names, in which \u2019de-\ntention\u2019 is a common practice in schools. This cluster helps to solve\nthe question \"What is a common punishment in the UK and Ireland?\".\nAnother question-related cluster is shown in Figure 5b. The main\n (a) SQuAD Phase 1: Semantic Clustering. We observe a topical cluster with\n\u2019school\u2019-related and another with \u2019country\u2019-related tokens.\n(b) SQuAD Phase 2: Entity Matching. The marked cluster contains matched to-\nkens \u2019detention\u2019, \u2019schools\u2019 and the countries that are applying this practice.\n(c) SQuAD Phase 3: Question-Fact Matching. The question tokens form a\ncluster with the Supporting Fact tokens.\n(d) SQuAD Phase 4: Answer Extraction. The answer token \u2019detention\u2019 is separated\nfrom other tokens.\nFigure 4: BERT\u2019s Transformation Phases for the SQuAD example from Table 1. Answer token: Red diamond-shaped. Question\nTokens: Orange star-shaped. Supporting Fact tokens: Dark Cyan. Prominent clusters are circled. The model passes through\ndifferent phases in order to find the answer token, which is extracted in the last layer (#11).\nchallenge within this sample is to identify the two facts that Emily is\na wolf and Wolves are afraid of cats. The highlighted cluster implies\nthat Emily has been recognized as a relevant entity that holds a\nrelation to the entity Wolf. The cluster also contains similar entity\nmentions e.g. the plural form Wolves. We observe analogous clusters\nin the HotpotQA model, which includes more cases of coreferences.\nThe probing results support these observations. The model\u2019s abil-\nity to recognize entities (Named Entity Labeling), to identify their\nmentions (Coreference Resolution) and to find relations (Relation\nRecognition) improves until higher network layers. Figure 6 visu-\nalizes these abilities. Information about Named Entities is learned\nfirst, whereas recognizing coreferences or relations are more diffi-\ncult tasks and require input from additional layers until the model\u2019s\nperformance peaks. These patterns are equally observed in the re-\nsults from BERT-base models and BERT-large models.\n(3) Matching Questions with Supporting Facts. Identifying rel-\nevant parts of the context is crucial for QA and Information Re-\ntrieval in general. In traditional pipeline models this step is often\nachieved by filtering context parts based on their similarity to the\nquestion [17]. We observe that BERT models perform a compara-\nble step by transforming the tokens so that question tokens are\nmatched onto relevant context tokens. Figures 4c and 5c show two\nexamples in which the model transforms the token representation\nof question and Supporting Facts into the same area of the vector\n (a) bAbI Phase 1: Semantic Clustering. Names and animals are clustered.\n(b) bAbI Phase 2: Entity Matching. The determining relation between the entities\n\u2019Emily\u2019 and \u2019Wolf\u2019 is resolved in a cluster.\n(c) bAbI Phase 3: Question-Fact Matching. In this case the question tokens\nmatch with a subset of Supporting Facts (\u2019Wolves are afraid of cats\u2019). The\nsubset is decisive of the answer.\n(d) bAbI Phase 4: Answer Extraction. The answer token \u2019cats\u2019 is separated from\nother tokens.\nFigure 5: BERT\u2019s Transformation Phases for the bAbI example from Table 1. The phases are equal to what we observe in\nSQuAD and HotpotQA samples: The formed clusters in the first layers show general language abilities, while the last layers\nare more task-specific.\nspace. Some samples show this behaviour in lower layers. How-\never, results from our probing tasks show that the models hold the\nstrongest ability to distinguish relevant from irrelevant information\nwrt. the question in their higher layers. Figure 2 demonstrates how\nthe performance for this task increases over successive layers for\nSQuAD and bAbI. Performance of the fine-tuned HotpotQA model\nin Figure 3 is less distinct from the model without fine-tuning and\ndoes not reach high accuracy.2 This inability indicates why the\nBERT model does not perform well on this dataset as it is not able\nto identify the correct Supporting Facts.\n2Note that the model only predicts the majority class in the first five layers and thereby\nreaches a decent accuracy without really solving the task.\nThe vector representations enable us to tell which facts a model\nconsidered important (and therefore matched with the question).\nThis helps retracing decisions and makes the model more transparent.\n(4) Answer Extraction. In the last network layers we see that\nthe model dissolves most of the previous clusters. Here, the model\nseparates the correct answer tokens, and sometimes other possible\ncandidates, from the rest of the tokens. The remaining tokens form\none or multiple homogeneous clusters. The vector representation\nat this point is largely task-specific and learned during fine-tuning.\nThis becomes visible through the performance drop in general NLP\nprobing tasks, visualized in Figure 6. We especially observe this\n Figure 6: Phases of BERT\u2019s language abilities. Higher saturation denotes higher accuracy on probing tasks. Values are normal-\nized over tasks on the Y-axis. X-axis depicts layers of BERT. NEL: Named Entity Labeling, COREF: Coreference Resolution, REL:\nRelation Classification, QUES: Question Type Classification, SUP: Supporting Fact Extraction. All three tasks exhibit similar\npatterns, except from QUES, which is solved earlier by the HotpotQA model based on BERT-large. NEL is solved first, while\nperformance on COREF and REL peaks in later layers. Distinction of important facts (SUP) happens within the last layers.\nloss of information in last-layer representations in the large BERT-\nmodel fine-tuned on HotpotQA, as shown in Figure 3. While the\nmodel without fine-tuning still performs well on tasks like NEL or\nCOREF, the fine-tuned model loses this ability.\nAnalogies to Human Reasoning. The phases of answering ques-\ntions can be compared to the human reasoning process, including\ndecomposition of input into parts [1]. The first phase of semantic\nclustering represents our basic knowledge of language and the sec-\nond phase how a human reader builds relations between parts of the\ncontext to connect information needed for answering a question.\nSeparation of important from irrelevant information (phase 3) and\ngrouping of potential answer candidates (phase 4) are also known\nfrom human reasoning. However, the order of these steps might\ndiffer from the human abstraction. One major difference is that\nwhile humans read sequentially, BERT can see all parts of the input\nat once. Thereby it is able to run multiple processes and phases\nconcurrently depending on the task at hand. Figure 6 shows how\nthe tasks overlap during the answering process.\n5.2\nComparison to GPT-2\nIn this section we compare our insights from the BERT models\nto the GPT-2 model. We focus on the qualitative analysis of to-\nken representations and leave the application of probing tasks for\nfuture work. One major difference between GPT-2\u2019s and BERT\u2019s\nhidden states is that GPT-2 seems to give particular attention to the\nfirst token of a sequence. While in our QA setup this is often the\nquestion word, this also happens in cases where it is not. During\ndimensionality reduction this results in a separation of two clusters,\nnamely the first token and all the rest. This problem holds true\nfor all layers of GPT-2 except for the Embedding Layer, the first\nTransformer block and the last one. For this reason we mask the\nfirst token during dimensionality reduction in further analysis.\nFigure 7 shows an example of the last layer\u2019s hidden state for\nour bAbI example. Like BERT, GPT-2 also separates the relevant\nSupporting Facts and the question in the vector space. Additionally,\nFigure 7: bAbI Example of the Answer Extraction phase in\nGPT-2. Both the question and Supporting Fact are extracted,\nbut the correct answer is not fully separated as in BERT\u2019s\nlast layers. Also a potential candidate Supporting Fact in\n\"Sheep are afraid of Wolves\" is separated as well.\nGPT-2 extracts another sentence, which is not a Supporting Fact, but\nis similar in meaning and semantics. In contrast to BERT, the correct\nanswer \"cats\" is not particularly separated and instead simply left\nas part of its sentence. These findings in GPT-2 suggest that our\nanalysis extends beyond the BERT architecture and hold true for\nother Transformer networks as well. Our future work will include\nmore probing tasks to confirm this initial observation.\n5.3\nAdditional Findings\nObservation of Failure States. One important aspect of explain-\nable Neural Networks is to answer the questions of when, why, and\nhow the network fails. Our visualizations are not only able to show\n Figure 8: BERT SQuAD example of a falsely selected answer\nbased on the matching of the wrong Supporting Fact. The\npredicted answer \u2019lectures\u2019 is matched onto the question as\na part of this incorrect fact (magenta), while the actual Sup-\nporting Fact (cyan) is not particularly separated.\nsuch failure states, but even the rough difficulty of a specific task\ncan be discerned by a glance at the hidden state representations.\nWhile for correct predictions the transformations run through the\nphases discussed in previous sections, for wrong predictions there\nexist two possibilities: If a candidate answer was found that the\nnetwork has a reasonable amount of confidence in, the phases will\nlook very similar to a correct prediction, but now centering on\nthe wrong answer. Inspecting early layers in this case can give\ninsights towards the reason why the wrong candidate was chosen,\ne.g. wrong Supporting Fact selected, misresolution of coreferences\netc. An example of this is shown in Figure 8, where a wrong answer\nis based on the fact that the wrong Supporting Fact was matched\nwith the question in early layers.\nIf network confidence is low however, which is often the case\nwhen the predicted answer is far from the actual answer, the trans-\nformations do not go through the phases discussed earlier. The\nvector space is still transformed in each layer, but tokens are mostly\nkept in a single homogeneous cluster. In some cases, especially\nwhen the confidence of the network is low, the network maintains\nPhase (1), \u2019Semantic Clustering\u2019 analogue to Word2Vec, even in\nlater layers. An example is depicted in the supplementary material.\nImpact of Fine-tuning. Figures 2 and 3 show how little impact\nfine-tuning has on the core NLP abilities of the model. The pre-\ntrained model already holds sufficient information about words and\ntheir relations, which is the reason it works well in multiple down-\nstream tasks. Fine-tuning only applies small weight changes and\nforces the model to forget some information in order to fit specific\ntasks. However, the model does not forget much of the previously\nlearned encoding when fitting the QA task, which indicates why\nthe Transfer Learning approach proves successful.\nFigure 9: BERT SQuAD example Layer 7. Tokens are color-\ncoded by sentence. This visualization shows that tokens are\nclustered by their original sentence membership suggesting\nfar reaching importance of the positional embedding.\nMaintained Positional Embedding. It is well known that the\npositional embedding is a very important factor in the performance\nof Transformer networks. It solves one major problem that Trans-\nformers have in comparison with RNNs, that they lack sequential\ninformation [36]. Our visualizations support this importance and\nshow that even though the positional embedding is only added once\nbefore the first layer, its effects are maintained even into very late\nlayers depending on the task. Figure 9 demonstrates this behavior\non the SQuAD dataset.\nAbilities to resolve Question Type. The performance curves re-\ngarding the Question Type probing task illustrate another inter-\nesting result. Figure 2 demonstrates that the model fine-tuned on\nSQuAD outperforms the base model from layer 5 onwards. This in-\ndicates the relevancy of resolving the question type for the SQuAD\ntask, which leads to an improved ability after fine-tuning. The\nopposite is the case for the model fine-tuned on the bAbI tasks,\nwhich loses part of its ability to distinguish question types during\nfine-tuning. This is likely caused by the static structure of bAbI\nsamples, in which the answer candidates can be recognized by sen-\ntence structure and occurring word patterns rather than by the\nquestion type. Surprisingly, we see that the model fine-tuned on\nHotpotQA does not outperform the model without fine-tuning in\nFigure 3. Both models can solve the task in earlier layers, which\nsuggests that the ability to recognize question types is pre-trained\nin BERT-large.\n6\nCONCLUSION AND FUTURE WORK\nOur work reveals important findings about the inner functioning\nof Transformer networks. The impact of these findings and how\nfuture work can build upon them is described in the following:\n Interpretability. The qualitative analysis of token vectors reveals\nthat there is indeed interpretable information stored within the\nhidden states of Transformer models. This information can be used\nto identify misclassified examples and model weaknesses. It also\nprovides clues about which parts of the context the model consid-\nered important for answering a question - a crucial part of decision\nlegitimisation. We leave the development of methods to further\nprocess this information for future work.\nTransferability. We further show that lower layers might be more\napplicable to certain problems than later ones. For a Transfer Learn-\ning task, this means layer depth should be chosen individually\ndepending on the task at hand. We also suggest further work re-\ngarding skip connections in Transformer layers to examine whether\ndirect information transfer between non-adjacent layers (that solve\ndifferent tasks) can be of advantage.\nModularity. Our findings support the hypothesis that not only do\ndifferent phases exist in Transformer networks, but that specific\nlayers seem to solve different problems. This hints at a modularity\nthat can potentially be exploited in the training process. For exam-\nple, it could be beneficial to fit parts of the network to specific tasks\nin pre-training, instead of using an end-to-end language model task.\nOur work aims towards revealing some of the internal processes\nwithin Transformer-based models. We suggest to direct further\nresearch at thoroughly understanding state-of-the-art models and\nthe way they solve downstream tasks, in order to improve on them.\nACKNOWLEDGMENTS\nOur work is funded by the European Unions Horizon 2020 research\nand innovation programme under grant agreement No. 732328\n(FashionBrain) and by the German Federal Ministry of Education\nand Research (BMBF) under grant agreement No. 01UG1735BX\n(NOHATE) and No. 01MD19003B (PLASS).\nREFERENCES\n[1] Lotfi A Zadeh. 1997. Zadeh, L.A.: Toward a Theory of Fuzzy Information Granu-\nlation and Its Centrality in Human Reasoning and Fuzzy Logic. Fuzzy Sets and\nSystems. ELSEVIER Fuzzy Sets and Systems 90 (1997).\n[2] Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass.\n2017. What do Neural Machine Translation Models Learn about Morphology?.\nIn Proceedings of ACL 2017.\n[3] Pierre Comon. 1994. Independent component analysis, A new concept? Signal\nProcessing 36 (1994).\n[4] Alexis Conneau and Douwe Kiela. 2018. SentEval: An Evaluation Toolkit for\nUniversal Sentence Representations. In Proceedings of LREC 2018.\n[5] Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised Sequence Learning. In\nProceedings of NIPS 2015.\n[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive Language Models Beyond a\nFixed-Length Context. CoRR (2019).\n[7] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz\nKaiser. 2018. Universal Transformers. In Proceedings of SMACD 2018.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nCoRR (2018).\n[9] F. K. Do\u0161ilovi\u0107, M. Br\u010di\u0107, and N. Hlupi\u0107. 2018. Explainable artificial intelligence:\nA survey. In MIPRO 2018.\n[10] Karl Pearson F.R.S. 1901. LIII. On lines and planes of closest fit to systems of\npoints in space. The London, Edinburgh, and Dublin Philosophical Magazine and\nJournal of Science 2 (1901).\n[11] Yoav Goldberg. 2019. Assessing BERT\u2019s Syntactic Abilities. CoRR (2019).\n[12] Riccardo Guidotti, Anna Monreale, Franco Turini, Dino Pedreschi, and Fosca\nGiannotti. 2018. A Survey Of Methods For Explaining Black Box Models. ACM\nComput. Surv. (2018).\n[13] Jeremy Howard and Sebastian Ruder. 2018. Fine-tuned Language Models for Text\nClassification. CoRR (2018).\n[14] Huggingface. 2018.\npytorch-pretrained-BERT.\n(2018).\nhttps://github.com/\nhuggingface/pytorch-pretrained-BERT\n[15] Dieuwke Hupkes, Sara Veldhoen, and Willem H. Zuidema. 2017. Visualisation\nand \u2019diagnostic classifiers\u2019 reveal how recurrent and recursive neural networks\nprocess hierarchical structure. In Proceedings of IJCAI 2018.\n[16] Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Pro-\nceedings of NAACL 2019.\n[17] Dan Jurafsky and James H. Martin. 2009. Speech and Language Processing: An In-\ntroduction to Natural Language Processing, Computational Linguistics, and Speech\nRecognition, Chapter 23. Prentice Hall series in artificial intelligence, Vol. 2. Pren-\ntice Hall, Pearson Education International.\n[18] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding Neural Networks\nthrough Representation Erasure. CoRR (2016).\n[19] Xin Li and Dan Roth. 2002. Learning Question Classifiers. In Proceedings of\nCOLING 2002.\n[20] Zachary Chase Lipton. 2016. The Mythos of Model Interpretability. ACM Queue\n(2016).\n[21] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, and Noah A.\nSmith. 2019. Linguistic Knowledge and Transferability of Contextual Represen-\ntations. In Proceedings of NAACL 2019.\n[22] Stuart P. Lloyd. 1982. Least squares quantization in PCM. IEEE Trans. Information\nTheory (1982).\n[23] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018.\nThe Natural Language Decathlon: Multitask Learning as Question Answering.\nCoRR (2018).\n[24] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Esti-\nmation of Word Representations in Vector Space. In Workshop Track Proceedings\nof ICLR 2013.\n[25] Tasha Nagamine, Michael Seltzer, and Nima Mesgarani. 2015. Exploring How\nDeep Neural Networks Form Phonemic Categories. In Proceedings of INTER-\nSPEECH 2015.\n[26] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. In Proceedings of NAACL-HLT 2018.\n[27] Yifan Qiao, Chenyan Xiong, Zheng-Hao Liu, and Zhiyuan Liu. 2019. Understand-\ning the Behaviors of BERT in Ranking. CoRR (2019).\n[28] Alec Radford. 2018. Improving Language Understanding by Generative Pre-\nTraining. OpenAI Blog (2018).\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners. OpenAI\nBlog (2019).\n[30] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don\u2019t Know:\nUnanswerable Questions for SQuAD. In Proceedings of ACL 2018.\n[31] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100, 000+ Questions for Machine Comprehension of Text. In Proceedings\nof EMNLP 2016.\n[32] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. [n.\nd.]. Bidirectional Attention Flow for Machine Comprehension. In Proceedings of\nICLR 2017.\n[33] Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does String-Based Neural MT\nLearn Source Syntax?. In Proceedings of EMNLP 2016.\n[34] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy,\nNajoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie\nPavlick. 2019. What do you learn from context? Probing for sentence structure\nin contextualized word representations. In Proceedings of ICLR 2019.\n[35] Laurens van der Maaten. 2009. Learning a Parametric Embedding by Preserving\nLocal Structure. In Proceedings of AISTATS 2009.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Proceedings of NIPS 2017.\n[37] Ellen Voorhees. 2001. Overview of TREC 2001. In Proceedings of TREC 2001.\n[38] Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin,\nSameer Pradhan, Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A Large\nTraining Corpus for Enhanced Processing. Springer, Heidelberg.\n[39] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2016. Towards\nAI-Complete Question Answering: A Set of Prerequisite Toy Tasks. In Proceedings\nof ICLR 2016.\n[40] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan\nSalakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for\nDiverse, Explainable Multi-hop Question Answering. In Proceedings of EMNLP\n2018.\n[41] Quan-shi Zhang and Song-chun Zhu. 2018. Visual interpretability for deep\nlearning: a survey. Frontiers of IT & EE (2018).\n"}, "Cascaded Semantic and Positional Self-Attention Network for Document Classification": {"authors": ["Juyong Jiang", "Jie Zhang", "Kai Zhang"], "title": "Cascaded Semantic and Positional Self-Attention Network for Document Classification", "url": "https://arxiv.org/pdf/2009.07148.pdf", "abstract": "Transformers have shown great success in learning representations for language modelling. However, an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (word orders). In this work, we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network (CSPAN) in the context of document classification. The CSPAN uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner, and then adaptively combine them together through a residue connection. Compared with commonly used positional encoding schemes, CSPAN can exploit the interaction between semantics and word positions in a more interpretable and adaptive manner, and the classification performance can be notably improved while simultaneously preserving a compact model size and high convergence rate. We evaluate the CSPAN model on several benchmark data sets for document classification with careful ablation studies, and demonstrate the encouraging results compared with state of the art.", "arxiv_id": "2009.07148", "published_date": "2020-09-15", "year": 2020, "introduction": "", "conclusion": "", "full_text": "1 \n \n \nAbstract \nTransformers have shown great success in \nlearning representations for language \nmodelling. However, an open challenge \nstill remains on how to systematically \naggregate semantic information (word \nembedding) with positional (or temporal) \ninformation (word orders). In this work, we \npropose a new architecture to aggregate the \ntwo sources of information using cascaded \nsemantic and positional self-attention \nnetwork (CSPAN) in the context of \ndocument classification. The CSPAN uses \na semantic self-attention layer cascaded \nwith Bi-LSTM to process the semantic and \npositional information in a sequential \nmanner, and then adaptively combine them \ntogether through a residual connection. \nCompared with commonly used positional \nencoding schemes, CSPAN can exploit the \ninteraction between semantics and word \npositions in a more interpretable and \nadaptive manner, and the classification \nperformance can be notably improved \nwhile simultaneously preserving a compact \nmodel size and high convergence rate. We \nevaluate the CSPAN model on several \nbenchmark \ndata \nsets \nfor \ndocument \nclassification with careful ablation studies, \nand demonstrate the encouraging results \ncompared with state of the art.  \n1 \nIntroduction \nDocument classification is one of the fundamental \nproblems in natural language processing, which is \naimed at assigning one or multiple labels to a  \n(typically)  short text paragraph. Wide applications \ncan be found in sentiment analysis (Moraes et al., \n2013; Tang et al., 2015)\uff0csubject categorization \n                                                           \n* Corresponding author. \n(Wang et al., 2012),spam email detection (Sahami \net al., 1998) and doc1ument ranking (Wang et al., \n2014). In recent years, deep neural networks have \nshown great potential in document classification \nand updated state-of-the-art performance. Popular \napproaches include Recurrent neural networks   \n(RNN) (Yogatama et al., 2017), convolutional \nneural networks (CNN) (Zhang et al., 2015) and \nAttention-based methods (Transformers) (Gong et \nal., 2019; Adhikari et al., 2019), or a mixture of \nthem.  \nDifferent lines of methods have their respective \npros and cons. For example, RNNs are highly \neffective models for exploiting word orders in \nlearning useful representations, thanks to the \niterative update of the hidden states that depend on \nboth the semantics of the current word and that of \nhistorical words (or a concise summary of them), \nand the long-range dependency made possible \nthrough LSTMs (Yang et al., 2016; Stephen et al., \n2018; Adhikari et al., 2019). Of course, the \nsequential processing nature makes it less efficient \ncomputationally. CNNs have gained huge success \nin image procesing and classification and were \nrecently introduced to NLP domains like document \nclassification (Zhang et al., 2015; Lei et al.,2015; \nConneau et al., 2016; Kim and Yang, 2018; Kim, \n2014).The \nlocal \nconvolutional \noperator \nis \nsensitive to word orders but only partially and \nlimited by the size of the kernel, and so long-term \nrelations may need many layers and therefore be \nchallenging. Transformers, different from both, \nfully exploit the   modelling power of self-attention \nmechanism (Shen et al., 2018; Gao et al., 2018; \nZheng et al., 2018) and have significantly \nimproved state of the art in many NLP tasks such \nas machine translation (Vaswani et al., 2017), \nCascaded Semantic and Positional Self-Attention \nNetwork for Document Classification  \n \nJuyong Jiang1, Jie Zhang2, Kai Zhang3* \n1College of Internet of Things Engineering, Hohai University, Nanjing, China \n2Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China \n3Department of Computer & Information Sciences, Temple University, PA, USA  \n1jiangjuyong@hhu.edu.cn \n2jzhang080@gmail.com, 3zhang.kai@temple.edu \n \n \n 2 \n \n \nlanguage understanding (Devlin et al., 2018) and \nlanguage modeling (Dai et al., 2019), etc.  \nDespite \nthe \ngreat \nsuccesses, \nhow \nto \nsystematically aggregate the semantic information \n(word embedding) with the positional information \n(word orders) is still an open challenge in \ntransformers. A common practice is the positional \nencoding (Vaswani et al., 2017), which encodes the \nposition of the \ud835\udc61 th word as a \ud835\udc51 -dimensional \nsinusoidal vector, as  \n \n\ud835\udc5d\ud835\udc61,2\ud835\udc56 = \ud835\udc60\ud835\udc56\ud835\udc5b(\ud835\udc61/100002\ud835\udc56/\ud835\udc51) , \n(1) \n                     \ud835\udc5d\ud835\udc61,2\ud835\udc56+1 = \ud835\udc50\ud835\udc5c\ud835\udc60(\ud835\udc61/100002\ud835\udc56/\ud835\udc51) . \n(2) \nThe positional vector of each word is then added \nto the \ud835\udc51 -dimensional word embedding vector, so \nthat subsequent predictors can numerically utilize \nthe temporal information.  However, empirically, \nadding positional vectors to the word vectors \nbrings little performance gains in document \nclassification, compared with when no positional \nencoding is adopted at all (See Section 3.4 Table 5 \nfor detailed empirical results).  \nThere are two reasons which we believe are \nrelated to the low performance gains from using \npositional encodings. First, such a strategy leads to \nan interaction (inner product) between the \nsemantic and temporal component that is hard to \ninterpret. To see this, let \ud835\udc65\ud835\udc56  and \ud835\udc5d\ud835\udc56  be the word \nvector and position vector for the \ud835\udc56th word. Then \nthe attention score between \ud835\udc56th and \ud835\udc57th word will be \ncomputed as (before normalization)  \n\ud835\udc52\ud835\udc56\ud835\udc57 = \u2329\ud835\udc65\ud835\udc56 + \ud835\udc5d\ud835\udc56, \ud835\udc65\ud835\udc57 + \ud835\udc5d\ud835\udc57\u232a \n                     = \u2329\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57\u232a + \u2329\ud835\udc5d\ud835\udc56, \ud835\udc5d\ud835\udc57\u232a + \u2329\ud835\udc65\ud835\udc56, \ud835\udc5d\ud835\udc57\u232a \n                                       + \u2329\ud835\udc5d\ud835\udc56, \ud835\udc65\ud835\udc57\u232a  \n(3) \nwhere \u2329\u2219,\u2219\u232a denotes the inner product between two \nvectors, and without loss of generality we have \nassumed identity transforms in generating the key \nand query views of each word.  \nObviously, as the inner product between a word \nvector and a positional vector, \u2329\ud835\udc65\ud835\udc56, \ud835\udc5d\ud835\udc57\u232a  and \u2329\ud835\udc5d\ud835\udc56, \ud835\udc65\ud835\udc57\u232a \ndo not bear meaningful interpretation. Therefore  \nthese two terms could very likely hamper the \nsemantic attention term \u2329\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57\u232a  and the positional \nattention term \u2329\ud835\udc5d\ud835\udc56, \ud835\udc5d\ud835\udc57\u232a by behaving like noise, such \nas deflating an important attention or exaggerating \na marginal one. This can negatively affect the \nlearned representations through the self-attention \nmechanism. Indeed, similar observations were \nmade in (Yan et al., 2019), where the authors show \nthat the self-attention mechanism, when mixed \nwith the positional vectors, can no longer \neffectively quantify the relative positional distance \nbetween the words (namely the positional attention \nterm \u2329\ud835\udc5d\ud835\udc56, \ud835\udc5d\ud835\udc57\u232a is perturbed in an undesired manner). \nSecond, the relative weights of the word vector \nand the position vector (in their summation) is \nhard-coded, leading to a fixed combination, while \nin practice the relative importance of the semantic \nand positional components in affecting the \nsimilarity among the words can definitely be more \ncomplex.  \nIn order to solve these challenges with positional \nencoding, we explore a new architecture in \ncombining the semantic and temporal information \nin document classification, called \u201ccascaded \nsemantic and positional self-attention network\u201d \n(CSPAN). There are three main characteristics of \nthe proposed architecture. First, instead of \ncombining the word vectors with positional vectors \nfrom scratch, we choose to first explore the two \nsources of information with their respective \nprocessing layers, namely, a self-attention layer \nthat works only on the semantic space, and a Bi-\nLSTM layer which further incorporates the \ntemporal order information in the updated word \nrepresentations. Second, these two layers are \ncascaded so that sematic information and the \ntemporal information can be finally combined \nthrough the use of a residual connection; this not \nonly avoids non-interpretable operations defined \nbetween word vectors and positional vectors, but \nalso serves as an adaptive transformation in \ncombining the two information sources.  Third, a \nmulti-query attention scheme is adopted to extract  \nmulti-faceted, \nfixed \ndimensional \ndocument \nfeatures, which makes the resultant model highly \ncompact and memory efficient. \nThe CSPAN model is shown to effectively \nimprove performance of document classification in \ncomparison to several state-of-the-art methods \nincluding transformer-styled architecture. In the \nmeantime, it demonstrates very compact model \nsize and fast convergence rate during the training \nprocess, which is particularly desirable for large \nproblems. We also conducted careful ablation \nstudies to further quantify the performance gains of \neach component of the CSPAN model.  \nOur study demonstrates the importance of the \nway semantic and temporal information are \naggregated in capturing the structures and meaning \nof documents, which we will continue exploring in \nthe more challenging language modelling tasks \n 3 \n \n \nsuch as sequence tagging (Huang et al., 2015), \nnatural language inference  (Chen et al., 2016) and \nmodeling sentence pairs (Tan et al., 2018) in our \nfuture research. \n2 \nMethod  \nThe overall architecture of the proposed CSPAN \nmodel is shown in Figure 1. It is a highly compact \nmodel with three basic building blocks.  \n First, we use a self-attention block to update the \nword representations in each document. Here, the \nembedding of each word will be collectively \naffected by all other words with related semantics \nin the same document. Note that we will not look \ninto any positional information in this stage. \nInstead, the temporal information will be taken into \naccount in the next block, after the word \nrepresentations have been fully updated through \nsemantic self-attention alone. As we shall see, such \na sequential processing pipeline allows more \nflexible combination of the semantic and positional \ninformation.  \nSecond, the updated word embeddings are fed \ninto a Bi-LSTM layer, so that the relative position \nof the words are naturally exploited to further \nrefine the word representations specific to the \norganization of each document. In the meantime, a \nresidual connection is adopted to combine the \nsemantic representation derived from the self-\nattention block, together with the output derived \nfrom the Bi-LSTM block; we call this ``Semantic \nand Positional Residual Connection\u2019\u2019, because it \ncombines the semantic information (out of self-\nattention block) with the positional information \n(out of the Bi-LSTM block) using residual \nconnections. As we shall see, such a combination \nis more flexible than directly combining word \nvector with positional vector as in existing \npositional encoding schemes.  \nThird, we adopt a multiple-query attention in the \nfinal block to extract fixed-dimensional document \nfeatures for final classification. Compared with \nmulti-head attention, the multi-query attention can \nsignificantly reduce the number of parameters in \nthe network, while giving promising classification \nresults. We describe the details of different \nstructures and components of our model in the \nfollowing sections. \n2.1 \nSemantic Self-Attention \nSelf-attention as proposed by (Vaswani et al., 2017) \ncalculates attention weight between each pair of \nobjects to capture global correlations and improve \nrepresentation learning. We apply this framework \nin computing the word representations since it can \ncapture long-range dependencies. However, we do \nmake a number of important rectifications which \nprove to be quite useful in improving the \nperformance of document classification. \nFirst, rather than using three independent \ntransformation matrices corresponding to the key, \nvalue, and query views for each word, we discard \nthese transformations, and use the original word \nvectors in all the three views. The reason is that we \nwant to activate a full, pairwise interaction between \nthe words in the original word embedding space \nand then apply transformations in subsequent (Bi-\nLSTM) layer, in order to maximally preserve the \npower of self-attention based representation \nlearning. In comparison, if one chooses to apply \ntransformation (e.g. dimensionality reduction in \nmost cases), then chances are that the semantic \ninformation encoded in the word vectors might \nsuffer certain losses before entering the next layer. \nEmpirically, we have observed that implementing \nself-attention in the full-dimensional word vectors \nleads to better performance than that on the lower \ndimensional, transformed word-vectors. \n Second,  rather than considering the use of the \npositional information in self-attention, we choose \nto implement self-attention only based on the \nsemantic information, and consider the positional  \ninformation in subsequent information processing \nblocks. This in contrast to current practices in \nWord \nEmbedding\nSelf Attention\nNorm\nBi-LSTM\nFC\nSoftmax\nOutput \nProbabiliies\nNorm\n\uf02b\n Self Attention\nMulti-Query\nSoft Attention\nm\nPositional \nRepresentation\nSemantic\nRepresentation\n \nFigure 1: The architecture of the proposed CSPAN \nmodel. \n 4 \n \n \nwhich the semantic information and positional \ninformation of each word is used together in \ncalculating the self-attention coefficients. The \nreason is that directly adding the word vector and \npositional vector can lead to noisy fluctuations in \nattention scores, as has been discussed in the \nintroduction. Therefore, the semantic information \nwill first be processed alone, and then subject to the \npositional information through subsequent LSTM \nlayer, which is a more natural way of injecting \npositional information.  \nGiven these two design principles, our self-\nattention block can be described as follows. Let the \ninput text sequence be \ud835\udc37 = (\ud835\udc641, \ud835\udc642, \u2026 , \ud835\udc64\ud835\udc3f)  of  \ud835\udc3f \nelements where \ud835\udc64\ud835\udc56 \u2208 \u211d\ud835\udc51   is the i-th word \nembedding. Self-attention compares each element \n\ud835\udc64\ud835\udc56  to every other element \ud835\udc64\ud835\udc57  in the sequence \nfollowed by layer normalization. As a result, a new \nsequence \ud835\udc46 = (\ud835\udc601, \ud835\udc602, \u2026 , \ud835\udc60\ud835\udc3f)  of the same length is \nconstructed, in which each element  \ud835\udc60\ud835\udc56 \u2208 \u211d\ud835\udc51 is a \nweighted average of all elements \ud835\udc64\ud835\udc56  in the input \nsequence, as \n \n\ud835\udc46 =  \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc37, \ud835\udc37, \ud835\udc37) \n  = \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 (\n\ud835\udc37\ud835\udc37\ud835\udc47\n\u221a\ud835\udc51 ) \ud835\udc37  \n(4) \nHere, the original word embedding matrix \ud835\udc37 \u2208\n\u211d\ud835\udc3f\u00d7\ud835\udc51  appears three times because we do not \ndifferentiate among the key, value and query views.  \nThe term \ud835\udc37\ud835\udc37\ud835\udc47 is used to generate a weight matrix \nbased on the inner-product similarity of the \nelements in the sequence. After normalization and \nre-scaling, the weight matrix is multiplied with \ud835\udc37 \nto generate the new sequence representation  \ud835\udc46 . \nThe self-attention can enhance the semantic \nrepresentation of word embeddings and capture \nboth the local and long-range dependencies.   \n2.2 \nSemantic and Positional Residual Connection \nIn the second block, we apply a Bi-LSTM layer to \ninject \ntemporal \ninformation \nin \nthe \nword \nrepresentations computed via the self-attention \nblock. The Bi-LSTM is a powerful model in \nhandling sequential data, and is known to capture \nlong-term dependencies due to the use of the gating \nmechanism (Graves and Schmidhuber, 2005).  \nTherefore this layer is supposed to further improve \nthe word representations obtained from the self-\nattention layer, which proceeds as \n \n\u210e\u20d7 \ud835\udc61 = \ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40\n\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7 (\ud835\udc60\ud835\udc61)  \n(5) \n \n\u210e\u20d6\u20d7\ud835\udc61 = \ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40\n\u20d6\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7\u20d7(\ud835\udc60\ud835\udc61)  \n(6) \n \n\u210e\ud835\udc61 = [\u210e\u20d7 \ud835\udc61 ,  \u210e\u20d6\u20d7\ud835\udc61]  \n(7) \n \n\ud835\udc43 = \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc3b, \ud835\udc3b, \ud835\udc3b)  \n(8) \nHere, the word vectors obtained through the \nself-attention layer, \ud835\udc60\ud835\udc56\u2032\ud835\udc60 \u2208 \u211d\ud835\udc51  are fed into a single-\nlayer Bi-LSTM, and then the hidden state of the \nLSTM in the forward and backward directions are \nconcatenated as \u210e\ud835\udc61 = [\u210e\u20d7 \ud835\udc61  , \u210e\u20d6\u20d7\ud835\udc61] . Finally, another \nself-attention layer is used to enhance the \nrepresentations \ud835\udc3b = [\u210e1, \u210e2, \u2026 , \u210e\ud835\udc3f], followed by a \nlayer-wise normalization to obtain the position-\naware representations  \ud835\udc43 = (\ud835\udc5d1, \ud835\udc5d2, \u2026 , \ud835\udc5d\ud835\udc3f). \nAlthough LSTMs are known to handle long-\nrange dependencies, it can still be challenging in \nlong documents. Therefore, following the custom \nin transformers (Vaswani et al., 2017), we use a \nresidual connection that combines the output of the \nself-attention layer with that of the Bi-LSTM layer, \ncomputed as shown below. \n \n\ud835\udc39\ud835\udc61\n\ud835\udc60\ud835\udc5d = \ud835\udc60\ud835\udc61 + \ud835\udc5d\ud835\udc61  \n(9) \nHere, \ud835\udc60\ud835\udc61 \u2208 \u211d\ud835\udc51  represents the output of first \nbuilding block (Semantic self-attention), \ud835\udc5d\ud835\udc61 \u2208 \u211d\ud835\udc51 \nstands for the output of second building blocks (Bi-\nLSTM). To guarantee that the two vectors can be \nadded together, the hidden-state dimension of the \nBi-LSTM is chosen as half of the input dimension, \ni.e., \ud835\udc51/2, so that the concatenated hidden state from \nthe forward and backward direction (7) has the \nsame dimension as the input word vectors. By \ncombining the semantic and positional information, \nwe obtain a final, high-level representation of each \ndocument. \nThe residual connection (He et al., 2016) has \nshown to be highly useful in facilitating an \neffective backpropagation so that the learning \nprocess approaches a better model. In our context, \nthe residual connection has an interesting \ninterpretation of combining sematic and positional \ninformation in an adaptive manner. Note that the \noutput of the self-attention layer is all about the \nsemantic component of the words; on the other \nhand, the output of the Bi-LSTM layer can be \ndeemed as word representations that incorporated \nthe positional information, thanks to the sequential \nprocessing nature of the Bi-LSTM. Besides, since \nthe output of the Bi-LSTM layer, its hidden state, \nis a transformation of the input word vectors, we \ncan then consider the output of the residual \nconnection as an adaptive combination of the \nsemantic components and positional components. \nThis not only avoids the non-interpretability of \n 5 \n \n \ndirectly combining word vector with position \nvectors, but also successfully adjusts their relative \nimportance \nthrough \nthe \nlearning \nof \nthe \ntransformation matrices in the Bi-LSTM model. \nWe speculate that this is an important reason why \nthe proposed architecture can effectively improve \nthe classification performance.     \n2.3 \nMulti-Query Soft Attention \nIn the final block, we learn a number of query \nvectors in the space of \ud835\udc39\ud835\udc61\n\ud835\udc60\ud835\udc5d (9) so that each query \ncan capture a certain aspect of the meaning of the \ndocument, in the form of a fixed-dimensional \nfeature (context) vector. This is in contrast to the \nsingle-query attention where only a single query \nvector is learned to summarize the content of a \ndocument  (Yang et al., 2016). It is worthwhile to \nnote that the multi-query attention in extracting \ndocument features can be computationally more \neffective than multi-head attention. In the latter \ncase, one attention head is associated with a \nindependent set of transformation matrices, \ntherefore the model size can be quite large. In \ncomparison, in our approach only multiple query \nvectors need to be learned in the same latent space \nof word representations, which has a much smaller \nmemory footprint.  \nMore Specifically, the multi-query attention is \ndefined as follows.  \n \n\ud835\udc62\ud835\udc61 = \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udc39\ud835\udc61\n\ud835\udc60\ud835\udc5d\ud835\udc4a\u210e + \ud835\udc4f\u210e)  \n(10) \n \n\ud835\udefc\ud835\udc56\ud835\udc61 =\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc62\ud835\udc61\n\ud835\udc47\ud835\udc44\ud835\udc56)\n\u2211 \ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc62\ud835\udc61\n\ud835\udc47\ud835\udc44\ud835\udc56)\n\ud835\udc61\n  \n(11) \n \n\ud835\udc39\ud835\udc56\n\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e = \u2211 \ud835\udefc\ud835\udc56\ud835\udc61\n\ud835\udc61\n\ud835\udc39\ud835\udc61\n\ud835\udc60\ud835\udc5d  \n(12) \n \n\ud835\udc39\u0303\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc391\n\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e, \u2026 , \ud835\udc39\ud835\udc5a\n\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e)\ud835\udc4a\ud835\udc53  (13) \nThat is, we first feed the \ud835\udc39\ud835\udc61\n\ud835\udc60\ud835\udc5d \u2208 \u211d\ud835\udc51  through a \none-layer MLP to get  \ud835\udc62\ud835\udc61 \u2208 \u211d\ud835\udc51  as a hidden \nrepresentation of  \ud835\udc39\ud835\udc61\n\ud835\udc60\ud835\udc5d \u2208 \u211d\ud835\udc51 , then we measure the \nimportance of the word as the similarity of \ud835\udc62\ud835\udc61 with \na query vector  \ud835\udc44\ud835\udc56 \u2208 \u211d\ud835\udc51  and get a normalized \nimportance weight  \ud835\udefc\ud835\udc56 \u2208 \u211d\ud835\udc3f  through a softmax \nfunction. The multi-query matrix is randomly \ninitialized and jointly learned during the training \nprocess. After that, we compute the  \ud835\udc39\ud835\udc56\n\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e \u2208 \u211d\ud835\udc51 as \na weighted sum of the  \ud835\udc39\ud835\udc61\n\ud835\udc60\ud835\udc5d \u2208 \u211d\ud835\udc51  based on the \nweighting. Finally, we concatenate all  \ud835\udc39\ud835\udc56\n\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e \nvectors and then use a fusion matrix \ud835\udc4a\ud835\udc53 \u2208 \u211d\ud835\udc5a\ud835\udc51\u00d7\ud835\udc51 \nto get a high-level representation of each document. \nHere we discuss in more detail the memory \nfootprint of the proposed multi-query attention, in \ncomparison and commonly used multi-head \nattention. Let the dimension of the residual \nconnection be \ud835\udc51; the number of query vectors be \n\ud835\udc5a. Then the model space complexity is \ud835\udc42(\ud835\udc5a\ud835\udc51 +\n\ud835\udc512). In comparison, if one adopts the multi-head \nattention with \ud835\udc5a attention heads, then the model \nspace complexity will be \ud835\udc42(\ud835\udc5a\ud835\udc512)  since each \nattention head will have its own transformation \nparameters. As can be seen, the memory saving is \nalmost proportional to the dimensionality; the \nhigher the word vector dimensions, the more \nsignificant the memory saving. This will be a \ndesired property for real-world applications. It is \nalso worthwhile to note that the CSPAN model \nonly has 3 blocks, while the standard transformer \nhas a cascade of 6 layers of self-attention each of \nwhich may require an independent set of \ntransformation matrices. \n2.4 \nClassification Layer  \nIn the final layer we apply a softmax classifier on \nthe document representation  \ud835\udc39\u0303\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e  to get a \npredicted label  \ud835\udc66\u0302 , where  \ud835\udc66\u0302 \u2208 \ud835\udc4c  and \ud835\udc4c  is the class \nlabel set, i.e., \n \n\ud835\udc66\u0302 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc5d(\ud835\udc4c|\ud835\udc39\u0303\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e)  \n(14) \nwhere \n         \ud835\udc5d(\ud835\udc4c|\ud835\udc39\u0303\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e) = \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc4a\ud835\udc5c\ud835\udc39\u0303\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e + \ud835\udc4f\ud835\udc5c)  (15) \nHere, \ud835\udc4a\ud835\udc5c and  \ud835\udc4f\ud835\udc5c are the transformation matrix \nand the bias term, respectively. Therefore, we can \nuse the negative log-likelihood to define the loss \nfunction as follows: \n \n\ud835\udc3f = \u2212 log \ud835\udc5d(\ud835\udc66\u0302|\ud835\udc39\u0303\ud835\udc60\ud835\udc5d\ud835\udc5a\ud835\udc5e)  \n(16) \n3 \nExperiments \nIn this section, we will report a number of \nexperimental results on 4 benchmark datasets for \ndocument classification, together with careful \nablation studies to illustrate the effectiveness of the \nbuilding blocks of the proposed method.   \n3.1 \nDatasets and Methods \nWe evaluate the effectiveness of the proposed  \nCSPAN model on four document classification \ndatasets as in (Zhang et al., 2015).  The detailed \nstatistics of the data sets are shown in Table 1. \nAG\u2019s News. Topic classification over four \ncategories of internet news articles composed of \ntitles plus description classified into: World, \nSports, Business and Sci/Tech. The number of \n 6 \n \n \ntraining samples for each class is 30,000 and test-\ning 1900. \nYelp Review Polarity. The same dataset of text \nreviews from Yelp Dataset Challenge in 2015, \nexcept that a coarser sentiment definition is \nconsidered: 1 and 2 are negative, and 4 and 5 as \npositive. The polarity dataset has 280,000 training \nsamples and 19,000 test samples in each polarity. \nYelp Review Full. The dataset is obtained from \nthe Yelp Dataset Challenge in 2015 on sentiment \nclassification of polarity star labels ranging from 1 \nto 5. The full dataset has 130,000 training samples \nand 10,000 testing samples in each star. \nYahoo! Answer. Topic classification over ten \nlargest main categories from Yahoo Answers \nComprehensive Questions and Answers version \n1.0: Society & Culture, Science & Mathematics, \nHealth, Education & Reference, Computers & \nInternet, Sports, Business & Finance, Enter-\ntainment & Music, Family & Relationships and \nPolitics & Government. The document we use \nincludes question titles, question contexts and best \nanswers. Each class contains 140,000 training \nsamples and 5,000 testing samples. \nMethods. We have included altogether eleven \ncompeting methods from (Zhang et al., 2015) and \n(Gong et al., 2019). For our approach, we have two \nversions: the CSPAN (base) using single-layer Bi-\nLSTM and 16 query vectors, and  CSPAN (big) \nusing three hidden layers in Bi-LSTM and 128 \nquery vectors. We trained the base models for 30 \nepochs and the big models for 60 epochs. \n3.2 \nModel configuration and training  \nIn the experiments, we use 300-dimensional GloVe \n6B pre-trained word embedding (Pennington et al., \n2014) to initialize the word embedding at \nhttps://nlp.stanford.edu/projects/glove. We choose \n150 hidden units for the Bi-LSTM models. The \nAdam Optimizer (Kingma et al., 2014) with \nlearning rate of 1e-3 and weight decay of 1e-4 is \nused to train the model parameters. The size of \nmini-batch is set to 64 and the number of multi-\nquery to 16. We train all neural networks for 30 \nepochs and the learning rate divides by 10 at 20 and \n25 epochs. All of our experiments are performed \non NVIDIA TITAN RTX GPUs, with PyTorch \n1.1.0 as the backend framework. \n3.3 \nResults and analysis \nThe experimental results on all data sets are shown \nin Table 2. The results of the competing methods \nare directly cited from the respective papers as \nlisted in Table 2.  \nFrom Table 2 we can see that CSPAN model \nachieves the best performance on all the 4 datasets \nof AG\u2019s News, Yelp P, Yelp F. and Yahoo datasets \n(rows 12/ 13), which demonstrates its effectiveness  \nin document classification. Particularly, CSPAN \nconsistently outperforms the baseline deep \nlearning networks using RNN/CNN, such as \nLSTM, CNN-char and CNN-word by a substantial \nmargin on all datasets (rows 1, 2 and 3).  \nCompared to the CSPAN (base), the CSPAN (big) \ngives a comparable or slightly better performance \non all the datasets. This observation shows that the \nCSPAN actually prefers simpler models against \nhighly complex ones, which is an advantage for \nlarge problems. \n3.4 \nAblation Study \nComponent-wise gains. To investigate the impact \nof each of the key components of CSPAN model for \ndocument classification, we conducted an ablation \nstudy on the AG\u2019s News dataset. Firstly, we \nvalidate the impact of each component, including \nsemantic self-attention, semantic and positional \nresidual connection, and multi-query soft attention. \nThe results are shown in Table 3. \nThe standard Bi-LSTM baseline provides a test \naccuracy of 89.36. As we expected, integrating \nsemantic self-attention significantly improved the \nclassification performance with test accuracy of \n92.61. It shows that using self-attention can \nDataset \nClasses \nTrain \nTest \nAverage #s \nMax #s \nAverage #w \nMax #w \nAG\u2019s News \n4 \n120,000 \n7,600 \n1.3 \n15 \n46.6 \n277 \nYelp Review Polarity \n2 \n560,000 \n38,000 \n8.4 \n119 \n161.4 \n1345 \nYelp Review Full \n5 \n650,000 \n50,000 \n8.4 \n151 \n163.3 \n1418 \nYahoo! Answers \n10 \n1,400,000 \n60,000 \n    5.7 \n    515 \n115.9 \n2746 \nTable 1:  Detailed statistics of the datasets:  #s denotes the number of sentences (average and maximum per \ndocument),  #w denotes the number of words (average and maximum per document). \n \n \n 7 \n \n \nenhance the semantic. Furthermore, integrating \nresidual connection improves the classification \nperformance from 92.61 to 93.03. Finally, when \nmulti-query attention is adopted, the classification \nperformance is significantly improved with an \noverall gain of 4.32% over the baseline.  \nModel Size. As mentioned in (Adhikari et al., \n2019), increasingly complex network components \nand modeling techniques are accompanied by \nsmaller and smaller improvements in effectiveness \non standard benchmark datasets. We have observed \nsimilar trend in CSPAN, as shown in Table 4. \nFrom Table 4, we can see that when the number \nof hidden layer in Bi-LSTM is set to 3, the \nperformance can be worse than 1-layer or 2-layer \nBi-LSTMS (the latter with even less query vectors). \nIn other words, a compact Bi-LSTM is preferred. \nOn the other hand, the optimal number of query \nvectors seems to be around 16 for 1-layer Bi-\nLSTM; more query vectors than this brings limited \nor even negative performance gains. \nFusion Methods. We also conducted extensive \ncomparative studies on the performance of \ndifferent ways in combining the semantic and the \npositional information, as shown in Figure 2.  \nFrom Table 5,we can see that directly com-\nbining the positional vector with the word vector \n(fusion method (b), a \u201clight-weight\u201d transformer)  \nbrings an improvement of 0.33% compared with \nthe baseline (method (a), without any positional \ninformation). In addition, using relative positional \n \nMethods \nAGNews \nYelp P. \nYelp F. \nYahoo \nZhang et al., 2015 \nLSTM \n86.06 \n94.74 \n58.17 \n70.84 \n \nCNN-char \n89.13 \n94.46 \n62.02 \n69.98 \n \nCNN-word \n91.45 \n95.11 \n60.48 \n70.94 \nGong et al., 2019 \nDeep CNN \n91.27 \n95.72 \n64.26 \n73.43 \n \nFastText \n92.50 \n95.70 \n63.90 \n72.30 \n \nHAN \n92.36 \n95.59 \n63.32 \n75.80 \n \nSASEM \n91.50 \n94.90 \n63.40 \n- \n \nDiSAN \n92.51 \n94.39 \n62.08 \n76.15 \n \nLEAM \n92.45 \n95.31 \n64.09 \n77.42 \n \nSWEM \n92.24 \n93.76 \n61.11 \n73.53 \n \nHLAN \n92.89 \n95.83 \n63.78 \n77.55 \nThis paper \nCSPAN (base) \n93.68 \n96.11 \n65.93 \n77.61 \n \nCSPAN (big) \n93.62 \n96.18 \n65.95 \n77.75 \nTable 2:  Test accuracy of competing methods on benchmark document classification tasks, in percentage. \n \n \nLayers \n(BiLSTM) \nQuery \nMemory(MB) \nAccuracy \n1 \n1 \n1557 \n92.84 \n1 \n8 \n1641 \n92.95 \n1 \n16 \n1739 \n93.68 \n2 \n8 \n1665 \n93.05 \n2 \n16 \n1765 \n92.88 \n2 \n32 \n1961 \n93.04 \n3 \n32 \n1997 \n92.92 \n3 \n64 \n2401 \n92.71 \n3 \n128 \n3201 \n93.14 \nTable 4:  Impact of model size. \n \n \nComponent \nAccuracy \nStandard Bi-LSTM(baseline) \n89.36 \n+ self-att \n92.61 \n+ residual \n93.03 \n+ multi-query \n93.68 \nTable 3:  Impact of each building block in the \nproposed CSPAN model on AG\u2019s News dataset. \n \n \n# \nMethods \nAccuracy \n(a) \nEmbedding \n92.38 \n(b) \nEmbedding + Position \n92.71 \n(c) \nEmbedding + Relative-Position \n92.39 \n(d) \nEmbedding + Bi-LSTM \n93.03 \n(e) \nEmbedding // Bi-LSTM \n93.68 \nTable 5:  Different ways in combining the semantic and \nthe positional information and their accuracy on AG\u2019s \nNews dataset. \n \n \n \n 8 \n \n \nencoding schemes (Shaw et al., 2018) (fusion \nmethod (c)) leads to almost the same result as the \nbaseline method. If we use Bi-LSTM directly on \nthe input word vectors, i.e., a parallel combination \nscheme of the semantic and positional information \n(fusion method (d)), the performance gain \napproaches 0.65%. Finally, the proposed fusion \nscheme in CSPAN (fusion method (e)), i.e., \nsequential processing of semantic and positional \ninformation equipped with a residual connection, \nthe performance gain is around 1.30%. This \ncomparative study clearly demonstrates the \nadvantage of the proposed CSPAN model in \ncombining semantic and positional information.  \nComputational Considerations. It is usually \nbelieved that transformers are computationally \nefficient by virtue of the parallel processing \npipeline \nassociated \nwith \nthe \nself-attention \nmechanism. However, empirically, we find that the \nlarge model size and extensive, pairwise self-\nattention cost can significantly slow down the \ncomputation. For example, standard transformers \nhave 6 layers of self-attention in the encoding stage \nalone, leading to a huge set of transformation \nmatrix parameters \ud835\udc4a\ud835\udc44, \ud835\udc4a\ud835\udc3e, \ud835\udc4a\ud835\udc49 and the cost of \nback-propagation can be huge. On the other hand, \n\ud835\udc42(\ud835\udc5b2) time and space are needed in each layer in \ncomputing the self-attention among a document of \n\ud835\udc5b words. Therefore, standard transformer is time \nconsuming in our experimental evaluations and \ntypically won\u2019t converge until after tens or even \n100 epochs even on the smallest data set (AG\u2019s \nNews). This is why we implemented and compared \nwith the \u201clight-weight\u201d version of transformers in \nour experiments (e.g., method (b) in Figure 2). The \nproposed CSPAN model, on the other hand, is \nmore compact and approaches a satisfactory result \nin just a few epochs, and the time taken for each \nepoch is also much less than standard transformers. \nTherefore, our approach is computationally very \nefficient, especially for classification of short or \nmedian-length documents.  \n4 \nConclusion \nWe presented the cascaded semantic and positional \nself-attention to aggregate semantic and positional \ninformation \nin \ndocument \nclassification. \nIt \novercomes the limitation of existing positional \nencoding schemes, and shows encouraging \nperformance against state-of-the-art methods using \ntransformers and CNNs. In the meantime, it has a \ncompact model size and is computational efficient. \nOur studies demonstrate the importance of \nproperly aggregating semantic and positional \ncomponents, and we will further extend it more \nchallenging NLP tasks in our future research.   \nAcknowledgments \nJie Zhang is supported by NSFC 61973086, \nShanghai Municipal Science and Technology \nMajor Project (No.2018SHZDZX01) and ZJ Lab. \nReferences  \nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and \nJimmy Lin. 2019. Docbert: Bert for document \nclassification. arXiv preprint arXiv:1904.08398. \nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and \nJimmy Lin. 2019.  Rethinking complex neural \nnetwork architectures for document classification. \nIn NAACL-HLT, volume 1, pages 4046-4051. \nAlexis Conneau, Holger Schwenk, Lo\u0131c Barrault, and \nYann Lecun. 2016. Very deep convolutional \nnetworks for natural language processing. arXiv \npreprint arXiv:1606.017812. \nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui \nBi-LSTM\n\uf02b\n\uf02b\nBi-LSTM\n(d)\n(e)\n(a)\nPositional \nEncoding\n\uf02b\n(b)\nRelative \nPosition\n(c)\n\uf02b\nSelf Attention\nSelf Attention\nSelf Attention\nSelf Attention\nSelf Attention\nSelf Attention\nWord Embedding\nWord Embedding\nWord Embedding\nWord Embedding\nWord Embedding\n \nFigure 2: Different schemes of combining the semantic and position information for a comparative study, where \n(b) corresponds to a \u201clight-weight\u201d-transformer, and  (e) is the proposed architecture. \n 9 \n \n \n  Jiang, and Diana Inkpen. 2016. Enhanced lstm for \nnatural \nlanguage \ninference. \narXiv \npreprint \narXiv:1609.06038. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2018. Bert: Pre-training of deep \nbidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805. \nZihang Dai, Zhilin Yang, Yiming Yang, Jaime \nCarbonell, Quoc V. Le, and Ruslan Salakhutdinov. \n2019. Transformer-xl: Attentive language models \nbeyond a fixed-length context. arXiv preprint \narXiv:1901.02860. \nShang Gao, Arvind Ramanathan, and Georgia Tourassi. \n2018. Hierarchical convolutional attention net-\nworks for text classification. The Third Workshop \non Representation Learning for NLP. \nAlex Graves and J\u00fcrgen Schmidhuber. 2005. \nFramewise \nphoneme \nclassification \nwith \nbi-\ndirectional LSTM and other neural network \narchitectures. Neural networks, 18(5-6): 602-610. \nChangjin Gong, Kaize Shi, and Zhendong Niu. 2019. \nHierarchical Text-Label Integrated Attention Net-\nwork for Document Classification. Proceedings of \nthe 2019 3rd High Performance Computing and \nCluster Technologies Conference. \nZhiheng Huang, Wei Xu, and Kai Yu. 2015. \nBidirectional LSTM-CRF models for sequence tag-\nging. arXiv preprint arXiv:1508.01991. \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian \nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR, pages 770-778. \nTaehoon Kim, and Jihoon Yang. 2018. Abstractive text \nclassification using sequence-to-convolution neural \nnetworks. arXiv preprint arXiv:1805.07745. \nYoon Kim. 2014. Convolutional neural networks for \nsentence classification. arXiv: 1408.5882. \nDiederik P. Kingma, and Jimmy Ba. 2014. Adam: A \nmethod for stochastic optimization. arXiv preprint \narXiv:1412.6980. \nTao Lei, Regina Barzilay, and Tommi Jaakkola. 2015. \nMolding cnns for text: non-linear, non-consecutive \nconvolutions. arXiv preprint arXiv:1508.04112. \nRodrigo Moraes, Jo\u00e3O Francisco Valiati, and Wilson P. \nGavi\u00e3O Neto. 2013. Document-level sentiment \nclassification: An empirical comparison between \nSVM and ANN. Expert Systems with Applications, \n40(2): 621-633. \nStephen Merity, Nitish Shirish Keskar, and Richard \nSocher. 2018. Regularizing and optimizing LSTM \nlanguage models. In ICLR. \nJeffrey Pennington, Richard Socher, and Christopher D. \nManning. 2014. Glove: Global vectors for word \nrepresentation. In EMNLP. \nMehran Sahami, Susan Dumais, David Heckerman, \nand Eric Horvitz. 1998. A Bayesian approach to \nfiltering junk e-mail. In Learning for Text Cate-\ngorization: Papers from the 1998 workshop, \nvolume 62, pages 98-105. \nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. \n2018. Self-attention with relative position repre-\nsentations. arXiv preprint arXiv:1803.02155. \nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and \nChengqi Zhang. 2018. Bi-directional block self-\nattention for fast and memory-efficient sequence \nmodeling. arXiv preprint arXiv:1804.00857. \nDuyu Tang, Bing Qin, and Ting Liu. 2015. Document \nmodeling with gated recurrent neural network for \nsentiment classification. In EMNLP. \nChuanqi Tan, Furu Wei, Wenhui Wang, Weifeng Lv, \nand Ming Zhou. 2018. Multiway Attention \nNetworks for Modeling Sentence Pairs. In IJCAI. \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob \nUszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz \nKaiser, and Illia Polosukhin. 2017. Attention is all \nyou need. In NIPS, pages 5998-6008. \nMingqiang Wang, Mengting Liu, Shi Feng, Daling \nWang, and Yifei Zhang. 2014.  A novel calibrated \nlabel ranking based method for multiple emotions \ndetection in Chinese microblogs. In Natural \nLanguage Processing and Chinese Computing, \npages 238-250.  \nSida Wang, and Christopher D. Manning. 2012. \nBaselines and bigrams: Simple, good sentiment and \ntopic classification. In ACL. \nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, \nAlex Smola, and Eduard Hovy. 2016. Hierarchical \nattention networks for document classification. In \nNAACL-HLT, pages1480-1489. \nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu. \n2019. TENER: Adapting Transformer Encoder for \nName \nEntity \nRecognition. \narXiv \npreprint  \narXiv:1911.04474. \nDani Yogatama, Chris Dyer, Wang Ling, and Phil \nBlunsom. 2017. Generative and discriminative text \nclassification with recurrent neural networks. arXiv \npreprint arXiv:1703.01898. \nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015. \nCharacter-level convolutional networks for text \nclassification. In NIPS. \nJianming Zheng, Fei Cai, Taihua Shao, and Honghui \nChen. 2018. Self-interaction attention mechanism-\nbased text representation for document classi-\nfication. Applied Sciences, 8(4): 613.  \n"}, "Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End": {"authors": ["Yanran Chen", "Steffen Eger"], "title": "Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End", "url": "https://arxiv.org/pdf/2212.10522.pdf", "abstract": "We consider the end-to-end abstract-to-title generation problem, exploring seven recent transformer based models (including ChatGPT) fine-tuned on more than 30k abstract-title pairs from NLP and machine learning venues. As an extension, we also consider the harder problem of generating humorous paper titles. For the latter, we compile the first large-scale humor annotated dataset for scientific papers in the NLP/ML domains, comprising almost 2.5k titles. We evaluate all models using human and automatic metrics. Our human evaluation suggests that our best end-to-end system performs similarly to human authors (but arguably slightly worse). Generating funny titles is more difficult, however, and our automatic systems clearly underperform relative to humans and often learn dataset artefacts of humor. Finally, ChatGPT, without any fine-tuning, performs on the level of our best fine-tuned system.", "arxiv_id": "2212.10522", "published_date": "2022-12-20", "year": 2022, "introduction": "Introduction Computer-assisted writing is an important and longstanding use case of natural language processing (NLP) and natural language generation (NLG). A popular early scenario involved machine translation, where the success of an MT system was measured in the number of human edits required to transform system output into an adequate translation (Krings, 2001); the HTER metric, which measures this, has remained an important metric until recently (Specia and Farzindar, 2010; Specia et al., 2021). The recent success of large-scale language models (LLMs), such as the GPT generation of NLG models, has made the goal even more realistic and promises full-scale automatic text generation, without any human intervention. 1Our paper title is a (modi\ufb01ed) merge of a funny and unfunny title suggested by ChatGPT. Our paper logo is drawn by DALL-E. In this work, we concern ourselves with automatic text generation in the scienti\ufb01c domain. Sample scenarios in this general context involve (semi)automatically generating reviews for scienti\ufb01c papers (Yuan et al., 2022), e.g., as a response to high reviewing load in the face of exploding submission numbers; and generating captions for tables that require reasoning capabilities (Moosavi et al., 2021). Our goal is much more modest: we ask whether language models can generate adequate titles given a human authored abstract as an input; we refer to this task as A2T (abstract-to-title generation). Title generation is important as titles are the \ufb01rst access point to paper; a good title may thus attract more readers and consequently increase a paper\u2019s impact, e.g., in terms of citation numbers (Falagas et al., 2013). We approach the problem as a standard sequenceto-sequence text generation problem, where we \ufb01ne-tune pre-trained language models on more than 30k abstract-title pairs from the ML and NLP communities. Besides generating titles per-se, we also aim for generating humorous title, an inherently dif\ufb01cult problem due to small sample size \u2014 in our corpus, less than 5% of human titles are estimated to be funny. Generating funny titles may be relevant, as a funny title may attract more readers and thus garner (even) more citations (Heard et al., 2022). Our contributions: \u2022 To our knowledge, we provide the \ufb01rst publicly available humor annotated dataset for scienti\ufb01c titles in the NLP and ML domain, encompassing 2,441 humor annotated titles annotated by 2 annotators with decent levels of agreement (kappa \u223c0.65). \u2022 We explore 6 recently popular text generation systems on the A2T task, \ufb01nding one of them to be competitive to human titles, according to automatic and human evaluation involving 15 arXiv:2212.10522v1  [cs.CL]  20 Dec 2022 ", "conclusion": "Conclusion We considered the abstract-to-title generation problem using end-to-end models. To do so, we trained six recent text-to-text generation systems on more than 30k NLP and ML papers. We evaluated the systems using an array of state-of-the-art automatic metrics as well as human evaluation. Our evaluation indicates that some current text generation models can generate titles with similar quality than humans, but human authors are apparently still superior. We also considered the humorous title generation problem as an extension, compiling the \ufb01rst dataset in the NLP/ML domain in this context, comprising almost 2.5k titles annotated by two annotators with acceptable agreement. We \ufb01nd that 11As the prompt may play a role, we evaluate another 20 instances from EMNLP 2022 handbook with ChatGPT using a different prompt and obtain the same results, indicating that the minor \ufb02aw in prompt words for ChatGPT may not affect its ability to perform the A2T generation task. ", "full_text": "Transformers Go for the LOLs:\nGenerating (Humourous) Titles from Scienti\ufb01c Abstracts End-to-End\nYanran Chen, Steffen Eger\nNatural Language Learning Group (NLLG)\nFaculty of Technology\nBielefeld University\nychen@techfak.uni-bielefeld.de\nsteffen.eger@uni-bielefeld.de\nAbstract\nWe consider the end-to-end abstract-to-title\ngeneration problem, exploring seven recent\ntransformer based models (including Chat-\nGPT) \ufb01ne-tuned on more than 30k abstract-\ntitle pairs from NLP and machine learning\nvenues.\nAs an extension, we also consider\nthe harder problem of generating humorous pa-\nper titles. For the latter, we compile the \ufb01rst\nlarge-scale humor annotated dataset for scien-\nti\ufb01c papers in the NLP/ML domains, compris-\ning almost 2.5k titles. We evaluate all mod-\nels using human and automatic metrics. Our\nhuman evaluation suggests that our best end-\nto-end system performs similarly to human au-\nthors (but arguably slightly worse). Generat-\ning funny titles is more dif\ufb01cult, however, and\nour automatic systems clearly underperform\nrelative to humans and often learn dataset arte-\nfacts of humor. Finally, ChatGPT, without any\n\ufb01ne-tuning, performs on the level of our best\n\ufb01ne-tuned system.1\n1\nIntroduction\nComputer-assisted writing is an important and long-\nstanding use case of natural language processing\n(NLP) and natural language generation (NLG). A\npopular early scenario involved machine transla-\ntion, where the success of an MT system was mea-\nsured in the number of human edits required to\ntransform system output into an adequate trans-\nlation (Krings, 2001); the HTER metric, which\nmeasures this, has remained an important metric\nuntil recently (Specia and Farzindar, 2010; Spe-\ncia et al., 2021). The recent success of large-scale\nlanguage models (LLMs), such as the GPT gen-\neration of NLG models, has made the goal even\nmore realistic and promises full-scale automatic\ntext generation, without any human intervention.\n1Our paper title is a (modi\ufb01ed) merge of a funny and\nunfunny title suggested by ChatGPT. Our paper logo is drawn\nby DALL-E.\nIn this work, we concern ourselves with auto-\nmatic text generation in the scienti\ufb01c domain. Sam-\nple scenarios in this general context involve (semi-\n)automatically generating reviews for scienti\ufb01c pa-\npers (Yuan et al., 2022), e.g., as a response to high\nreviewing load in the face of exploding submission\nnumbers; and generating captions for tables that re-\nquire reasoning capabilities (Moosavi et al., 2021).\nOur goal is much more modest: we ask whether\nlanguage models can generate adequate titles given\na human authored abstract as an input; we refer to\nthis task as A2T (abstract-to-title generation). Title\ngeneration is important as titles are the \ufb01rst access\npoint to paper; a good title may thus attract more\nreaders and consequently increase a paper\u2019s impact,\ne.g., in terms of citation numbers (Falagas et al.,\n2013).\nWe approach the problem as a standard sequence-\nto-sequence text generation problem, where we\n\ufb01ne-tune pre-trained language models on more than\n30k abstract-title pairs from the ML and NLP com-\nmunities. Besides generating titles per-se, we also\naim for generating humorous title, an inherently\ndif\ufb01cult problem due to small sample size \u2014 in\nour corpus, less than 5% of human titles are esti-\nmated to be funny. Generating funny titles may be\nrelevant, as a funny title may attract more readers\nand thus garner (even) more citations (Heard et al.,\n2022).\nOur contributions:\n\u2022 To our knowledge, we provide the \ufb01rst publicly\navailable humor annotated dataset for scienti\ufb01c\ntitles in the NLP and ML domain, encompassing\n2,441 humor annotated titles annotated by 2 an-\nnotators with decent levels of agreement (kappa\n\u223c0.65).\n\u2022 We explore 6 recently popular text generation\nsystems on the A2T task, \ufb01nding one of them\nto be competitive to human titles, according to\nautomatic and human evaluation involving 15\narXiv:2212.10522v1  [cs.CL]  20 Dec 2022\n annotators.\n\u2022 We analyze the problem and \ufb01nd that the A2T\ntask is to some degree ill-posed as a good title\nmay leverage more than the abstract alone (we\nargue that the problem framing is still a legitimate\nand ef\ufb01cient approximation).\n\u2022 For humor generation, we \ufb01nd that our models\nclearly underperform relative to humans and in-\nstead often learn dataset artefacts.\n\u2022 We \ufb01nally analyze ChatGPT on a small scale and\n\ufb01nd that it may be competitive (albeit slightly\nweaker) to our best \ufb01ne-tuned model without any\ntask-speci\ufb01c \ufb01ne-tuning at all.\nWe release data and code under https://\ngithub.com/cyr19/A2T.\n2\nRelated Work\nTitle generation and evaluation\nMishra et al.\n(2021) perform A2T with pre-trained GPT-2 \ufb01ne-\ntuned on arxiv papers and subsequent (rule-based)\nmodules of title selection and re\ufb01nement. We com-\npare many more text generation models for the\ntask, use better evaluation (excluding low-quality\noutdated surface-level metrics such as BLEU; in-\ncluding more comprehensive human evaluation),\ndo not make use of rule-based selection and also\nconsider humor in title generation. Putra and Kho-\ndra (2017) classify sentences from paper abstracts\ninto rhetorical categories, retain those relating to\nmethods and results and then generate titles using\ntemplates. They further note the relationship be-\ntween the task of summarization (Nenkova et al.,\n2011) and A2T, as a title can be seen as a summary\nof the research paper. We also leverage the relation-\nship to summarization by considering pre-trained\nmodels \ufb01ne-tuned on summarization datasets. In\ncontrast to Putra and Khodra (2017) and Mishra\net al. (2021), we only consider end-to-end models\nthat do not involve error-prone pipelines.\nBeyond title generation, related \ufb01elds of text\ngeneration for science are related work generation\n(Li et al., 2022) and, outside of science, headline\ngeneration e.g. for news (Tan et al., 2017). Tan\net al. (2017) use a coarse-to-\ufb01ne approach which\n\ufb01rst identi\ufb01es important sentences and then con-\nverts them into a headline. In this way, the model is\nnot confused by \u2018too much\u2019 irrelevant information.\nIn A2T, the \ufb01rst summarization step is apparently\nnot necessary, as the abstract is already a summary\nof the scienti\ufb01c paper. Yuan et al. (2022) explore\nautomatically generating reviews for scienti\ufb01c arti-\ncles.\nHow titles should be (and are) structured has\nbeen researched for a long time, e.g., Hartley\n(2005); Lewison and Hartley (2005).\nHartley\n(2008) gives a categorization of title types, dis-\ntinguishing 13 title classes, e.g., those that state\nresults vs. methods.\nHumor identi\ufb01cation and generation\nHumor\ndetection is a niche area in NLP but nonethe-\nless with a rich history. For example, Mihalcea\nand Strapparava (2006) distinguish funny from\nnon-funny sentences (heuristically scraped from\nthe Web) using features and traditional classi\ufb01ers.\nSimpson et al. (2019) focus on ef\ufb01ciently anno-\ntating humor and inducing classi\ufb01ers from crowd-\nsourced data. Recently, Peyrard et al. (2021) show\nthat transformers are strong at distinguishing funny\nfrom non-funny sentences on minimal pairs of satir-\nical news headlines. In the scienti\ufb01c domain, Heard\net al. (2022) annotate a dataset of more than 2k ti-\ntles from ecology using a \ufb01ne-grained Likert scale.\nThe majority were labeled as non-funny and anno-\ntators exhibited low agreements.\nThere is considerably less work on humor gen-\neration. As one exception, He et al. (2019) gener-\nate puns by a retrieve-and-edit approach based on\nword2vec, thus circumventing the problem of little\ntraining data for puns.\n3\nData\nWe use the dataset released by Beese et al. (2022),\nwhich contains title-abstract pairs and correspond-\ning meta-information such as the publication year\nand venue. Beese et al. (2022) extracted the data\nfrom two sources: ACL Anthology (from 1984\nto 2021) and machine learning conferences (from\n1989 to 2021); we refer to the datasets from these\ntwo sources as NLP and ML respectively, following\nthe original paper.\nFiltering\n(1) We restrict the data to the main con-\nference papers (e.g., EMNLP, ACL), to ensure data\nquality. (2) As Figure 1 shows, most abstracts\nhave less than 400 words; by introspection, we \ufb01nd\nthat extremely long abstracts often contain extra\nsections other than abstracts, due to limitations of\nAllen AI\u2019s Science Parse2 which was used to auto-\nmatically extract the paper structures by Beese et al.\n2https://github.com/allenai/\nscience-parse/\n Figure 1: Histogram of the length of abstracts. We de-\n\ufb01ne the length of a text as the length of the list split by\nspaces for each text.\n(2022). As a consequence, we limit the data to ab-\nstracts of length smaller than 400. (3) In addition,\nwe only leverage the data of papers published after\nthe year 2000 (which form the majority anyway).\nAfter \ufb01ltering, 32,952 abstract-title pairs remain in\nour dataset.\nTitle\nLabel\nLearning to learn by gradient descent by\ngradient descent (Andrychowicz et al.,\n2016)\nFUNNY\nCancerEmo: A Dataset for Fine-Grained\nEmotion Detection (Sosea and Caragea,\n2020)\nFUNNYmedium\nGlobal Encoding for Abstractive Summa-\nrization (Lin et al., 2018)\n\u00acFUNNY\nTable 1: Example of the annotated titles.\nHumor Annotation + Classi\ufb01cation\nTo gener-\nate humorous titles from the abstracts, in addition\nto the abstract-title pairs, we need labels regarding\nthe humorousness of the titles. Thus, we train hu-\nmor classi\ufb01ers to automatically label the titles as\nFUNNY, FUNNYmedium, and \u00acFUNNY (denoted\nas 2, 1 and 0 respectively; examples see Table 1).\nTwo co-authors participated in the annotation. We\nmade no guidelines and asked annotators to refer\nto their intuition for a notion of humor. To measure\nannotation quality, we calculate Cohen\u2019s Kappa\nagreement (Cohen, 1960) between them several\ntimes during the whole procedure.\nStage 1: The two annotators initially anno-\ntated 1,730 titles, obtaining 0.650 Kappa agree-\nment on the commonly annotated 300 titles, where\n1,603 titles were labeled as \u00acFUNNY, 106 as\nFUNNYmedium, and 21 as FUNNY. Since funny\ntitles (FUNNY or FUNNYmedium) make up only\n\u223c7.3% of the annotated data, we randomly gen-\nerate 11 different data splits, where the train set\nof each split consists of 100 funny or medium\nfunny titles and 200 not funny titles (all randomly\ndrawn), while the remaining 27 funny titles and 27\nnot funny titles compose the dev set. From those\ndata splits, we train 11 classi\ufb01ers to construct an\nensemble classi\ufb01er (checkpoints selected based on\nthe macro F1 on each dev set). To evaluate the clas-\nsi\ufb01er performance, the two annotators annotated\nanother 315 titles jointly, obtaining 0.639 Kappa\nagreement. Our best ensemble classi\ufb01er leverages\nthe sum of the label values assigned by the 11 indi-\nvidual classi\ufb01ers to predict humorousness, yielding\n4.8% improvement of macro F1 compared to the in-\ndividual classi\ufb01ers (62.4% vs. 57.6%). We relegate\ndetails to Section B.\nStage 2: To \ufb01nd more funny title candidates to\nannotate, the two annotators annotated the funni-\nest 396 titles in the original dataset from Beese\net al. (2022), predicted by the ensemble classi\ufb01er\ndeveloped in Stage 1; 75.8% (300 titles) were la-\nbeled as FUNNY or FUNNYmedium, which is sub-\nstantially higher than the proportion of funny titles\nin the annotated data of Stage 1 (7.3%), bringing us\n300 more funny titles. Thus, the annotated data\nexpands to 2,441 titles (1, 730 + 315 + 396 =\n2, 441), where 1,893 are labeled as \u00acFUNNY, 492\nas FUNNYmedium and 56 as FUNNY. Subsequently,\nwe re-train 11 classi\ufb01ers on newly generated 11\ndata splits from the expanded data of 2,441 titles;\nnow the train set of each split is composed of 400\nfunny or medium funny titles and 800 not funny\ntitles. As before, the 11 classi\ufb01ers are then used to\nconstruct an ensemble classi\ufb01er by means of the\noptimal ensemble strategy identi\ufb01ed in Stage 1.\nWe test the classi\ufb01ers from both stages on a held-\nout test set containing 197 titles annotated by the\ntwo annotators, who obtain 0.649 Kappa on this\nevaluation data. The macro F1 scores of those clas-\nsi\ufb01ers are presented in Table 3. As FUNNY titles\nare rare in the whole dataset, we also evaluate the\nclassi\ufb01ers on the corresponding binary classi\ufb01ca-\ntion task, where FUNNY and FUNNYmedium are\nmerged to one category. We observe that: (1) en-\nsemble classi\ufb01er performs better than the individual\nones. (2) Classi\ufb01ers from Stage 2 are superior com-\npared to the ones from Stage 1, indicating larger\nsize of the training data is bene\ufb01cial. (3) The best\nthree-way classi\ufb01er achieves only \u223c58% macro\nF1, whereas it has \u223c88% macro F1 on the binary\n Abstract\nTitle\nLabel\nThe move from hand-designed features to learned features in machine learning has been\nwildly successful.[...] We demonstrate this on a number of tasks, including simple convex\nproblems, training neural networks, and styling images with neural art.\nLearning to learn by\ngradient descent by\ngradient descent\nFUNNY\nTable 2: Example of the instance in our dataset. Each instance contains the abstract, title, and the label regarding\nthe title\u2019s humor degree for a certain paper. This instance contains the data for paper Andrychowicz et al. (2016).\nthree-way\nbinary\nStage 1\nIndividuals\n52.2%\n81.5%\nEnsemble\n54.1%\n85.1%\nStage 2\nIndividuals\n55.1%\n84.7%\nEnsemble\n57.7%\n88.1%\nTable 3: Average macro F1 over the 11 individual clas-\nsi\ufb01ers and macro F1 of the ensemble classi\ufb01ers from\nboth stages on the held-out test set (where the two an-\nnotators obtain 0.649 kappa agreement). We bold the\nhighest macro F1 for both classi\ufb01cation tasks.\nclassi\ufb01cation, implying that the three-way classi-\n\ufb01cation is hard, but the classi\ufb01ers can handle the\nbinary classi\ufb01cation task. Besides, we see a con-\nsistent improvement of human annotation quality:\nthe two annotators achieve 0.01-0.1 higher Kappa\nagreement when their annotations are down-scaled\nto binary ones (see Table 14 in Appendix B). As\na consequence, we use the ensemble classi\ufb01er\nfrom Stage 2 as the humor classi\ufb01er in further\nexperiments.\nHumor label\nTotal\nSource\n\u00acFUNNY\nFUNNY\nNLP\nML\ntrain\n30,741\n1,011\n31,752\n16,141\n15,611\ndev\n400\n200\n600\n480\n120\ntest\n400\n200\n600\n480\n120\ntotal\n31,541\n1,411\n32,952\n17,101\n15,851\nTable 4: Distribution of the source (NLP or ML) and\nhumor labels (FUNNY or \u00acFUNNY) of the instances\nin our dataset.\nFinal Dataset\nTo obtain the \ufb01nal dataset, we\nuse our humor classi\ufb01er to automatically label\nthe rest of the data (not annotated). Considering\nthe dif\ufb01culty of the three-way classi\ufb01cation task\nfor both human annotators and automatic classi-\n\ufb01ers, we only consider two humor levels in further\ngeneration experiments: (1) FUNNY (for funny\nand medium funny titles) and (2) \u00acFUNNY (for\nnot funny titles). Therefore, we collect 31,541\ninstances (>95%) with FUNNY and 1,411 with\n\u00acFUNNY titles, where each instance now consists\nof the abstract-title pair and the humor label for the\ntitle, as illustrated in Table 2. Subsequently, we\nsplit the resulting data to train, dev, and test sets,\nensuring that (1) the data with human-annotated\ntitles remains in the train set, as the humor classi-\n\ufb01er trained and evaluated on it will be used as an\nautomatic humor evaluator; (2) 80% of the data in\ndev/test is from NLP and 20% from ML because our\nannotators are more knowledgable for NLP papers,\nand (3) the ratio of FUNNY data to \u00acFUNNY data\nis 1:2.3 As \u00acFUNNY data is only a small portion\nof the whole dataset, we only keep 600 instances in\nthe dev/test sets, whereas the remaining data serves\nas the training data. In Table 4, we summarize the\nstatistics of the \ufb01nal dataset.\nExamples of human annotated funny instances\nare in the appendix. We note that FUNNYmedium of-\nten contain acronyms for datasets or models, while\nFUNNY typically contain a \u2018genuine\u2019 humor com-\nponent.\n4\nTitle Generation\nIn the \ufb01rst phrase of the experiments, we explore\nwhether existing state-of-the-art Seq2Seq models\nmanage to generate human-level titles from ab-\nstracts. Hence, we do not include humor constraints\nin the inputs of the generation systems.\n4.1\nBaseline Models\nWe experiment with the following six generation\nmodels: (i) BART base (BARTbase) (Lewis et al.,\n2020), (ii) GPT2 (GPT2) (Radford et al., 2019),\n(iii) T5 small (Raffel et al., 2020) (T5), and (iv)\nPEGASUS large (Zhang et al., 2019) \ufb01netuned on\nExtreme Summarization (XSUM) dataset (Narayan\net al., 2018) (PEGASUSxsum). Noting the similarity\nbetween text summarization and our A2T genera-\ntion task, we additionally inspect two BART large\nmodels \ufb01netuned on (v) XSUM (BARTxsum) and\n(vi) CNN dailymail (CNNDM) (See et al., 2017)\n3This aims to more easily compare the system-generated\nfunny titles with the human-generated ones and does not relate\nto controlling the quality of titles in the test set.\n (BARTcnn), respectively.4 XSUM and CNNDM\ncontain document-summary pairs, where XSUM\nhas one-sentence summaries, while each summary\nin CNNDM consists of multiple sentences.\n4.2\nFine-tuning\nFor all baseline models, we continue \ufb01ne-tuning\nthem on the abstract-title pairs from our dataset5\nwith AdamW Optimizer (Loshchilov and Hutter,\n2019) and linear learning rate scheduler, and sub-\nsequently use beam search (Vijayakumar et al.,\n2016) as the sampling strategy to generate the out-\nput candidates. The optimal checkpoint for each\nmodel is selected based on the ROUGE1/2/L (Lin,\n2004) scores on the dev set. We relegate the (hy-\nper)parameter settings for training and beam search\nto Appendix C.\n4.3\nEvaluation\nWe assess the performance of the generation sys-\ntems on 230 abstracts using both automatic evalu-\nation metrics and human evaluation. Besides the\nsix automatic generation systems, we also include\nthe human-generated titles in the evaluation; the\nrespective system is denoted as \u2018HUMAN\u2019.\n4.3.1\nAutomatic Evaluation\nAs there are no A2T task-speci\ufb01c evaluation met-\nrics, we use the following existing evaluation met-\nrics designed for other NLG tasks such machine\ntranslation or summarization: BERTScore (Zhang\net al., 2020), MoverScore (Zhao et al., 2019),\nCOMET (Rei et al., 2020), BARTScore (Yuan et al.,\n2021), MENLI (Chen and Eger, 2022). In the eval-\nuation, we employ all considered metrics in both\nreference-based and -free settings. In the reference-\nbased setup, the metrics compare the system ti-\ntles with the original ones (human-generated titles),\nwhile in the reference-free evaluation, the system\ntitles are directly compared to the abstracts. Fur-\nther, we examine whether those evaluation metrics\nare reliable to evaluate the titles\u2019 quality, based on\nthe human evaluation results.\n4.3.2\nHuman Evaluation\nThe human evaluation is conducted in the reference-\nfree setup: 15 annotators6 were asked to select two\n4We obtain all model checkpoints from Hugging Face\nhttps://huggingface.co/models.\n5The dataset used here is slightly different from the one\nintroduced above, as we recreate the dataset for humor title\ngeneration. This does not affect the evaluation of humor\ngeneration.\n6Most annotators are Master students, with an additional\nsenior researcher and two Bachelor students.\nbest and two worst titles among six titles from\ndifferent systems (including HUMAN), given the ab-\nstract. Each instance (an abstract and its six titles)\nwas evaluated by two to \ufb01ve annotators. The aver-\nage percentage agreement over all annotator pairs\nis \u223c50%, implying that each two annotators agree\non one selection among the two selected best/worst\ntitles, on average.\nThen, we use best-worst scaling to obtain the\n\ufb01nal human score for each title. The \ufb01nal human\nscore (BWS) is calculated as:\nBWS = Nbest \u2212 Nworst\nNannotators\n(1)\nwhere Nbest/worst refers to the number of times that\nthe title was selected as one of the best/worst two\ntitles and Nannotators indicates the number of an-\nnotators responsible for that instance. Therefore,\nBWS ranges from -1 to 1.\n4.3.3\nResults\n(a) Reference-based Evaluation\nsystem\nMoverS\nBERTS\nCOMET\nBARTS\nMENLI\nBARTxsum\n0.410\n0.912\n-0.283\n-3.816\n0.076\nPEGASUSxsum\n0.404\n0.906\n-0.371\n-3.964\n0.005\nBARTbase\n0.405\n0.907\n-0.373\n-3.986\n0.036\nGPT2\n0.400\n0.902\n-0.461\n-4.114\n-0.020\nT5\n0.381\n0.898\n-0.501\n-4.177\n-0.025\nBARTcnn\n0.282\n0.907\n-0.634\n-3.747\n0.133\nHUMAN\n1.000\n1.000\n1.072\n-0.608\n0.898\n(b) Reference-free Evaluation\nsystem\nBWS\nMoverS\nBERTS\nBARTS\nCOMET\nMENLI\nBARTxsum\n0.197\n-0.025\n0.889\n-2.583\n0.060\n-0.214\nPEGASUSxsum\n0.022\n-0.036\n0.887\n-2.819\n0.060\n-0.263\nBARTbase\n0.015\n-0.034\n0.887\n-2.709\n0.059\n-0.226\nGPT2\n-0.013\n-0.087\n0.881\n-3.090\n0.060\n-0.285\nT5\n-0.039\n-0.055\n0.889\n-2.735\n0.057\n-0.265\nBARTcnn\n-0.384\n0.046\n0.880\n-2.982\n0.047\n-0.159\nHUMAN\n0.181\n-0.062\n0.873\n-3.508\n0.061\n-0.029\nTable 5: Evaluation Results of the baseline models. We\nunderlie the best performance among all generation sys-\ntems including human. We bold the best performance\namong all automatic generation systems excluding hu-\nman.\nWe present the reference-based evaluation re-\nsults in Table 5(a). HUMAN obtains the highest\nscores using all metrics, which is unsurprising, as\nthe metrics use the human-generated titles as the\nanchor text. Among the six automatic generation\nsystems, BARTxsum is best, being selected by 3 out\nof 5 evaluation metrics, followed by BARTcnn.\nTable 5(b) shows the reference-free evaluation\nresults (including human evaluation). In contrast\n to reference-based evaluation, only two evaluation\nmetrics (COMET and MENLI) select HUMAN as\nthe best generation system. BARTxsum is still the\nbest among the six automatic generation systems,\nobtaining best results on 4 out of 6 evaluation met-\nrics (including BWS). Surprisingly, it outperforms\nHUMAN even in the human evaluation (0.197 vs.\n0.181 BWS). Nevertheless, as Figure 2(a) shows,\nHUMAN was still most frequently selected as among\nthe two best titles (23.2%) among all generation\nsystems, whereas the best neural generation system\nBARTxsum was selected in 16.9% of the cases as\none of the best two titles. However, we observe that\nHUMAN was also more often selected as among the\ntwo worst titles (14.1% vs. 9.3% BARTxsum; see\nFigure 2(b)), explaining why BARTxsum is better\nthan HUMAN in human evaluation. We analyzed\ncases in which the human title was selected as\namong the two worst. Introspection shows that\nthis is mostly due to words in the title which do not\nappear in the abstract. As a consequence, human\nannotators may believe that the model is hallucinat-\ning.\nFigure 2: Distribution of generation systems of the ti-\ntles selected as the BEST/WORST ones in human eval-\nuation; percentages indicate the proportion of the gen-\neration systems being selected over all selections.\nOverall, we thus believe that there is a (slight)\nmismatch in our task de\ufb01nition: human authors\nmay leverage the whole paper when designing their\ntitles, not only the abstracts. However, paper2title\ngeneration would not only be a challenge for the\ntext generation models (which are often limited\nin text length) but also for the human annotation\nprocess. We argue that framing the problem as\nabstract2title generation is a simpli\ufb01cation with\noverall good tradeoffs between problem complexity\nand model and annotator capacity.\n4.4\nAnalysis of Evaluation Metrics\n4.4.1\nCorrelation\nTo inspect the reliability of the used metrics, we\ncalculate Pearson correlation with system-level hu-\nMetric\n230 instances\n35 instances\nref-based\nref-free\nref-free\nBARTS\n0.389\n-0.044\n0.177\nBERTS\n0.442\n0.079\n0.389\nMoverS\n0.575\n-0.677\n-0.457\nMENLI\n0.345\n0.139\n-0.051\nCOMET\n0.580\n0.929\n0.618\nA2TMetric\n-\n-\n0.821\nTable 6: Pearson correlation of evaluation metrics with\nsystem-level human judgements for the 230 instances\n(1380 titles; left block) and the 35 instances (210 titles;\nright block). We bold the highest correlation in each\nblock.\nman judgements, i.e., average BWS per system, on\nthe 1380 titles (230 instances \u00d7 6 titles = 1380\ntitles). From Table 6 (left block), we observe that:\n(1) most metrics perform better in the ref-based\nsetup compared to ref-free, except for COMET;\n(2) only ref-free COMET correlates well with hu-\nman judgements (0.929 vs. -0.7-0.58), indicating\nthat the majority of the examined metrics are not\nadequate for A2T generation.\n4.4.2\nA2TMetric\nBased on the above \ufb01ndings, we develop the \ufb01rst\nsupervised A2T generation-speci\ufb01c evaluation\nmetric, using the human judgments collected in\nthe evaluation for the 230 instances. Since hu-\nman is included in the evaluation as a generation\nsystem, and the metrics will later be used to evalu-\nate system-generated humorous titles, which may\nvastly differ from the original ones, we argue that a\nref-free metric will better suit our needs.\nDataset\nWe split the data of 230 instances to train\n(170 instances), dev (25 instances), and test (35 in-\nstances) set. We note that many titles receive a\nBWS of 0 when the number of annotators is small\n(because they were never selected as the best or\nworst two titles), which may be problematic to\ndirectly train a regression model. Besides, the hu-\nman evaluation was similar to the ranking process.\nTherefore, we convert BWS in the train and dev set\nto relative-ranking judgements (Ma et al., 2018).\nI.e., if two titles for one abstract obtain different\nBWS, this title pair is considered as one relative-\nranking judgement. Each instance then contains\none abstract, a \u201cbetter\u201d title, a \u201cworse\u201d title, and the\nscore difference between the two titles in addition.\nAs a consequence, our train set consists of 2245\ninstances, whereas the dev set has 309 instances.\n Framework\nWe adopt a similar framework to\nthe ranking-based variant of COMET to train the\nA2T metrics but in a ref-free setup. During train-\ning, the model optimizes the embedding space so\nthat (1) the sentence embedding of the abstract\n(a) is closer to that of the \u201cbetter\u201d title (t+) than\nto that of the \u201cworse\u201d title (t\u2212) (using the Triplet\nMargin loss (Schroff et al., 2015)) and (2) the dif-\nference between d(a, t+) and d(a, t\u2212) is close to\nthe difference in BWS human scores for the two\ntitles (using the MSE loss), where d(u, v) refers\nto the Euclidean distance between u and v. Dur-\ning predicting, the metrics calculate the Euclidean\ndistance between the sentence embeddings of the\nabstract and the title.\nEvaluation\nAs Table 6 (right block) shows, our\nbest A2TMetric substantially outperforms the exist-\ning ref-free metrics on the test set (0.821 vs. -0.457-\n0.618 Pearson). Besides, we notice that the correla-\ntion of COMET drops from 0.929 to 0.618 when\nevaluated only on a portion of the data (test set),\nwhich may indicate that the BWS human scores\nare inconsistent across different batches of evalua-\ntion data, especially when the number of annotators\nchanges. We use the A2TMetric reported here in\nfurther experiments.\n5\nHumorous Title Generation\nIn the second phase of the experiments, we\nuse the optimal model identi\ufb01ed previously, i.e.,\nBARTxsum, to generate titles with constraints on\nhumor level. The input of the generation systems is\nformulated as \u201chumor level [SEP] abstract\u201d, where\nhumor level is either 0 (for \u00acFUNNY) or 1 (for\nFUNNY).\nFine-tuning\nWe \ufb01ne-tune generation systems\nhere using AdamW optimizer with linear learn-\ning rate scheduler and obtain the titles with beam\nsearch as in Section 4.2:\n(1) we \ufb01ne-tune a\nBARTxsum on the abstract-title pairs in the train\nset with humor constraints. (2) We continue \ufb01ne-\ntuning the model from (1) on self-generated pseudo\ndata.\nThe motivation of (2) is that we observe that the\nsystems tend to ignore the humor constraints in\nthe input and generate identical titles for different\nconstraints in the initial experiment. We assume\nthat to expose the systems to titles with different\nhumor levels for the same abstract during training\ncan encourage the models to pay more attention to\nthe humor constraints. To obtain the pseudo data,\nwe perform the following steps:\n(i) We generate titles for abstracts in the train set\nbut with \u201copposite\u201d humor constraints com-\npared to the original titles. For instance, if the\noriginal title for a certain abstract has label\n\u201c0\u201d for humor, we then generate a pseudo title\nwith constraint \u201c1\u201d for this abstract, utilizing\nthe model obtained from (1).\n(ii) We keep only the pseudo titles having the cor-\nrect humor labels assigned by the humor clas-\nsi\ufb01er.\n(iii) We \ufb01lter out the titles labeled as FUNNY con-\ntaining extremely frequent N-grams, in order\nto encourage the systems to generate more\ndiverse titles.7\n(iv) We \ufb01nally merge the \ufb01ltered pseudo data with\nthe original data. Thus, in the training data of\n(2), each abstract has two titles, one with label\nFUNNY and the other with \u00acFUNNY.\nThe training hyperparameters can be found in Ap-\npendix D. To monitor the models\u2019 ability to gen-\nerate titles on correct humor levels, we use macro\nF1 between the expected humor labels (i.e., the\nhumor constraints given to the inputs) and the hu-\nmor labels assigned to the generated titles by the\nhumor classi\ufb01er as the performance indicator, with\nwhich on the dev set we select the optimal model\ncheckpoints of the two systems.\n5.1\nEvaluation\n5.1.1\nAutomatic Evaluation\nBased on the analysis results for the automatic eval-\nuation metrics in Section 4.4, we only leverage\nCOMET and our supervised metric A2TMetric\nhere to evaluate the titles\u2019 quality. To evaluate the\nsystems\u2019 ability to generate titles on correct hu-\nmor levels, we use the following three metrics: (1)\nF1macro between the expected humor labels and\nthose assigned by the humor classi\ufb01er. (2) System\naccuracy of generating titles on correct humor lev-\nels, denoted as ACCFUNNY and ACC\u00acFUNNY. (3)\nThe ratio of the cases that the systems generate the\n7In initial experiments, we found that using the not \ufb01ltered\npseudo data can lead the system to generate many titles with\nidentical non-sense n-grams. In a particular case, half of the\ngenerated titles labeled as FUNNY have the text \u201cWhat\u2019s in a\nname:\u201d.\n F1macro\nACC\u00acFUNNY\nACCFUNNY\nRatioSAME\nBARTxsum\n0.647\n94.5%\n40.2%\n6.5%\nBARTxsum+pseudo\n0.856\n93.6%\n77.8%\n4.7%\nTable 7: Automatic evaluation for the systems\u2019 ability\nto generate titles with correct humor constraints. We\nbold the best performance.\nMetric\nCOMET\nA2TMetric\nhumor constraint\n\u00acFUNNY\nFUNNY\n\u00acFUNNY\nFUNNY\nBARTxsum\n0.0598\n0.0582\n-2.3014\n-2.3173\nBARTxsum+pseudo\n0.0593\n0.0541\n-2.3130\n-2.3736\nHUMAN\n0.0586\n-2.3566\nTable 8: Automatic evaluation for titles\u2019 quality. We\nbold the best performance assessed by each metric.\n\u201cHumor constraint\u201d refers to the constraints given to the\ninput of the generation systems.\nsame titles for both humor constraints to all genera-\ntion cases (RatioSAME); lower ratio indicates better\nperformance.\nWe generate titles with constraint on both humor\nlevels for all abstracts in the test set. Thus, we\ncompute the automatic evaluation on 1200 titles in\ntotal.\nResults\nFrom Table 7, we observe that: (1) af-\nter continued training on the pseudo data, the\nsystem (BARTxsum+pseudo) achieves substan-\ntially higher F1macro (from 0.647 to 0.856) and\nACCFUNNY (from 40.2% to 77.8%), and slightly\nbetter RatioSAME (from 6.5% to 4.7%), implying\nthe effectiveness of this training procedure. (2)\nHowever, ACC\u00acFUNNY drops slightly compared\nto BARTxsum (94.5% vs. 93.6%), indicating that\nboth systems have good performance on generat-\ning \u00acFUNNY titles and the \ufb01ne-tuning on pseudo\ndata only bene\ufb01ts to improve the system\u2019s ability\nto generate FUNNY titles.\nWe then present the quality evaluation results\nin Table 8. Both BART neural systems can obtain\nbetter results than HUMAN, which is in line with\nthe observation in Section 4.3.3, especially when\ngenerating \u00acFUNNY titles, achieving higher scores\nthan HUMAN on both evaluation metrics (0.0593 to\n0.0598 vs. 0.0586 on COMET, and -2.3130 to -\n2.3014 vs. -2.3566 on A2TMetric). However, we\nobserve a consistent performance drop after train-\ning on the pseudo data (values in the \ufb01rst row vs.\nthose in the second row). Further, we also note\nthat the system generated \u00acFUNNY titles have bet-\nter quality than the FUNNY ones (values in the\nleft column vs. those in the right column for each\nevaluation metric).\n5.1.2\nHuman Evaluation\nWe randomly sample 50 abstracts from the test\nset with controls on the source of the papers (80%\nfrom NLP and 20% from ML) and on the humor\nlabel of the original titles (50% FUNNY and 50%\n\u00acFUNNY), to generate the titles for human evalua-\ntion. Each of two annotators evaluates 25 instances\nseparately; each instance contains one abstract and\nits \ufb01ve titles: 1 original title + 4 system titles (2\ngeneration systems \u00d7 2 humor levels).8 During\nevaluation, the annotators rank the 5 titles on two\ncriteria: general quality and humor degree, based\non the abstract, being unaware of generation sys-\ntems and humor constraints; soft ranking is allowed\nhere, i.e., the annotators can assign identical ranks\nto multiple titles if they can not differentiate the\ntitles according to the evaluation criterion.\nResults\nIn Table 9, we compare the two BART-\nbased generation systems. Similar to automatic\nevaluation, we observe (1) a general quality drop\nbut a performance boost regarding humor genera-\ntion after training on the pseudo data and (2) that\nthe \u00acFUNNY titles have better quality compared\nto the FUNNY ones.\nsystem\nhumor constraint\nhumor\nquality\nBARTxsum\n\u00acFUNNY\n3.06\n2.48\nFUNNY\n1.80\n3.00\nBARTxsum+pseudo\n\u00acFUNNY\n3.20\n2.88\nFUNNY\n1.54\n3.46\nTable 9: Average rank of the system titles for all ab-\nstracts in the human evaluation of general quality and\nhumor degree; smaller values denotes higher ranks.\n\u201cHumor constraint\u201d refers to the constraints given to the\ninput of the generation systems.\nFurther, we compare the system titles with the\noriginal human generated titles in Table 10. HUMAN\nis consistently ranked \ufb01rst almost across all evalua-\ntion criteria: human generated titles are funnier\n(1.52 vs. 1.72 to 1.88) and have better quality\n(2.60 vs. 2.72 to 3.20 for FUNNY and 2.08 vs.\n2.32 to 2.68 for \u00acFUNNY titles) than the system\ngenerated ones; the latter observation contradicts\nwith the conclusion from the automatic evaluation,\nwhere the system-generated \u00acFUNNY titles obtain\nhigher scores compared to the human generated.\n8We calculate the inter-annotator agreement between the\ntwo annotators on other 31 instances, obtaining 0.523 Kappa\nfor humor and 0.605 Kappa for quality ranking.\n humor constraint/label\nFUNNY\n\u00acFUNNY\nsystem\nhumor\nquality\nhumor\nquality\nBARTxsum\n1.88\n2.72\n3.04\n2.32\nBARTxsum+pseudo\n1.72\n3.20\n2.96\n2.68\nHUMAN\n1.52\n2.60\n2.52\n2.08\nTable 10: Average rank of the system titles for the\nabstracts with original titles labeled as FUNNY and\n\u00acFUNNY separately in the human evaluation of gen-\neral quality and humor degree; smaller values denotes\nhigher ranks. \u201cHumor constraint/label\u201d refers to the\nconstraints given to the input of the generation systems\nand the humor labels of the original titles.\nOn introspection, we \ufb01nd that the models often\nover\ufb01t to example patterns seen in the training data\nwhen generating humorous titles which may not \ufb01t\nin the context of a speci\ufb01c paper\u2019s title, e.g., \u201cDon\u2019t\ndo X\u201d where X is irrelevant to the current paper\nand/or directly taken from the training data.\n6\nComparison with ChatGPT\nsystem\nhumor constraint\nhumor rank\nquality rank\nBARTxsum\nFUNNY\n1.66\n2.73\n\u00acFUNNY\n2.77\n2.31\nChatGPT\nFUNNY\n1.54\n3.95\n\u00acFUNNY\n3.31\n2.45\nTable 11: Average ranks of the titles for the 48 ab-\nstracts from EMNLP 2022 handbook in the human\nevaluation of quality and humorousness; smaller values\ndenote higher ranks. We bold the highest ranks.\nWe compare our \ufb01ne-tuned BARTxsum (without\ntraining on pseudo data) with the recent popular\nChatGPT9 model.\nFirstly, we use the two models to generate funny\nand not funny titles for 48 abstracts from the recent\nEMNLP 2022 handbook10 which ChatGPT could\nnot have seen in its training data. The prompt used\nfor ChatGPT here is \u201cI want a funny title and a not-\nfunny title for the following abstract: [abstract]\u201d;\nthen the model will return two titles with an indica-\ntion of which is the funny or not funny title. The\nranking-based human evaluation conducted here is\nidentical to that described in Section 5.1.2. The two\nevaluators initially assess 10 instances, obtaining\nKappa scores of 0.783 and 0.347 for humorousness\n9https://chat.openai.com/\n10We\nmanually\ncollect\nthe\nabstracts\nand\ntitles\nfrom\nhttps://drive.google.com/file/d/\n1OlPv6QBeo62VVTughj2jkiLeyHd1WnUt/view\nand quality ranking, respectively, indicating poten-\ntially low agreement on quality ranking. However,\nwhen we only consider the best and worst titles\nselected, the percentage agreement on quality as-\nsessment rises to a satisfactory level (69.2%). We\nthen use the average ranks over the two evaluators\nas the \ufb01nal ranks for the titles in the \ufb01rst 10 in-\nstances. Subsequently, each evaluator separately\nassesses another 19 instances.\nThe average rank per system with humor con-\nstraint is presented in Table 11. We observe that\nChatGPT generates funnier but lower-quality ti-\ntles compared to BARTxsum. Nevertheless, in the\nquality ranking of \u00acFUNNY titles, compared to\nChatGPT, BARTxsum is better in 23 out of 43\ncases, while it is worse in 20 out of 43 cases (in 5\nout of 48 cases the titles from these two systems\nrank equally.). Hence, we conclude that ChatGPT\nwithout any \ufb01ne-tuning may already perform sim-\nilarly to our \ufb01ne-tuned BARTxsum, acknowledg-\ning the small-scale of our human annotation, how-\never.11\nEvaluation results suggest that system-generated\nfunny titles tend to rank lower in the quality eval-\nuation, compared to the not funny ones. Exam-\nples of the system-generated titles with high humor\nranks but low quality ranks are shown in Appendix\nE. As before, we note that those titles frequently\nincorporate phrases unrelated to the content and\ndemonstrate de\ufb01ciencies in terms of information\ncorrectness and coverage.\n7\nConclusion\nWe considered the abstract-to-title generation prob-\nlem using end-to-end models. To do so, we trained\nsix recent text-to-text generation systems on more\nthan 30k NLP and ML papers. We evaluated the\nsystems using an array of state-of-the-art automatic\nmetrics as well as human evaluation. Our evalu-\nation indicates that some current text generation\nmodels can generate titles with similar quality than\nhumans, but human authors are apparently still\nsuperior. We also considered the humorous title\ngeneration problem as an extension, compiling the\n\ufb01rst dataset in the NLP/ML domain in this context,\ncomprising almost 2.5k titles annotated by two an-\nnotators with acceptable agreement. We \ufb01nd that\n11As the prompt may play a role, we evaluate another 20\ninstances from EMNLP 2022 handbook with ChatGPT using\na different prompt and obtain the same results, indicating that\nthe minor \ufb02aw in prompt words for ChatGPT may not affect\nits ability to perform the A2T generation task.\n our systems struggle with generating humorous ti-\ntles and instead over\ufb01t to frequent patterns in the\ndata, indicating much scope for future research.\nReferences\nMarcin Andrychowicz, Misha Denil, Sergio Gomez,\nMatthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. 2016.\nLearning to learn by gradient descent by gradient de-\nscent. Advances in neural information processing\nsystems, 29.\nDavid\nBalduzzi,\nMarcus\nFrean,\nLennox\nLeary,\nJP\nLewis,\nKurt\nWan-Duo\nMa,\nand\nBrian\nMcWilliams. 2017.\nThe shattered\ngradients\nproblem: If resnets are the answer, then what is the\nquestion?\nDominik Beese, Beg\u00fcm Altunba\u00b8s, G\u00f6rkem G\u00fczeler,\nand Steffen Eger. 2022. Detecting stance in scien-\nti\ufb01c papers: Did we get more negative recently?\nYanran Chen and Steffen Eger. 2022. Menli: Robust\nevaluation metrics from natural language inference.\nArXiv, abs/2208.07316.\nJacob Cohen. 1960.\nA coef\ufb01cient of agreement for\nnominal scales. Educational and psychological mea-\nsurement, 20(1):37\u201346.\nMatt Crane. 2018. Questionable answers in question\nanswering research: Reproducibility and variability\nof published results. Transactions of the Association\nfor Computational Linguistics, 6:241\u2013252.\nAleksandra Edwards, Jose Camacho-Collados, H\u00e9l\u00e8ne\nDe Ribaupierre, and Alun Preece. 2020.\nGo sim-\nple and pre-train on domain-speci\ufb01c corpora: On\nthe role of training data for text classi\ufb01cation. In\nProceedings of the 28th International Conference\non Computational Linguistics, pages 5522\u20135529,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nMatthew E Falagas, Angeliki Zarkali, Drosos E Kara-\ngeorgopoulos, Vangelis Bardakas, and Michael N\nMavros. 2013. The impact of article length on the\nnumber of future citations: a bibliometric analysis of\ngeneral medicine journals. PLoS One, 8(2):e49476.\nJonas Geiping, Liam Fowl, W. Ronny Huang, Woj-\nciech Czaja, Gavin Taylor, Michael Moeller, and\nTom Goldstein. 2020.\nWitches\u2019 brew: Industrial\nscale data poisoning via gradient matching.\nJames Hartley. 2005. To attract or to inform: What are\ntitles for?\nJournal of Technical Writing and Com-\nmunication, 35(2):203\u2013213.\nJames Hartley. 2008. Academic writing and publishing:\nA practical handbook. Routledge.\nHe He, Nanyun Peng, and Percy Liang. 2019. Pun gen-\neration with surprise. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1734\u20131744, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nStephen B. Heard, Chloe A. Cull, and Easton R. White.\n2022. If this title is funny, will you cite me? citation\nimpacts of humour and other features of article titles\nin ecology and evolution. bioRxiv.\nHermann Hild, Johannes Feulner, and Wolfram Men-\nzel. 1991. Harmonet: A neural net for harmonizing\nchorales in the style of j. s. bach. In Advances in\nNeural Information Processing Systems, volume 4.\nMorgan-Kaufmann.\nPei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Min-\nlie Huang. 2020. SentiLARE: Sentiment-aware lan-\nguage representation learning with linguistic knowl-\nedge.\nIn Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP), pages 6975\u20136988, Online. Associa-\ntion for Computational Linguistics.\nHans P Krings. 2001. Repairing texts: Empirical in-\nvestigations of machine translation post-editing pro-\ncesses, volume 5. Kent State University Press.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871\u20137880, Online. Association\nfor Computational Linguistics.\nGrant Lewison and James Hartley. 2005. What\u2019s in a\ntitle? numbers of words and the presence of colons.\nScientometrics, 63(2):341\u2013356.\nPengcheng Li, Wei Lu, and Qikai Cheng. 2022. Gen-\nerating a related work section for scienti\ufb01c papers:\nan optimized approach with adopting problem and\nmethod information. Scientometrics, 127(8):4397\u2013\n4417.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74\u201381.\nJunyang Lin, Xu Sun, Shuming Ma, and Qi Su. 2018.\nGlobal encoding for abstractive summarization. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 163\u2013169, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019.\nDecoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\n Qingsong Ma, Ond\u02c7rej Bojar, and Yvette Graham. 2018.\nResults of the WMT18 metrics shared task: Both\ncharacters and embeddings achieve good perfor-\nmance. In Proceedings of the Third Conference on\nMachine Translation: Shared Task Papers, pages\n671\u2013688, Belgium, Brussels. Association for Com-\nputational Linguistics.\nRada Mihalcea and Carlo Strapparava. 2006.\nLearn-\ning to laugh (automatically): Computational models\nfor humor recognition. Computational Intelligence,\n22(2):126\u2013142.\nPrakhar Mishra, Chaitali Diwan, Srinath Srinivasa, and\nG Srinivasaraghavan. 2021. Automatic title gener-\nation for text with pre-trained transformer language\nmodel. In 2021 IEEE 15th International Conference\non Semantic Computing (ICSC), pages 17\u201324. IEEE.\nNa\ufb01se Sadat Moosavi, Andreas R\u00fcckl\u00e9, Dan Roth,\nand Iryna Gurevych. 2021.\nScigen: a dataset for\nreasoning-aware text generation from scienti\ufb01c ta-\nbles. In Thirty-\ufb01fth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks\nTrack (Round 2).\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don\u2019t give me the details, just the summary!\nTopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium.\nAni Nenkova, Kathleen McKeown, et al. 2011. Auto-\nmatic summarization. Foundations and Trends\u00ae in\nInformation Retrieval, 5(2\u20133):103\u2013233.\nUlrike Pad\u00f3. 2016. Get semantic with me! the useful-\nness of different feature types for short-answer grad-\ning. In Proceedings of COLING 2016, the 26th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 2186\u20132195, Osaka, Japan.\nThe COLING 2016 Organizing Committee.\nGiorgio Patrini, Richard Nock, Paul Rivera, and\nTiberio Caetano. 2014. (almost) no label no cry. In\nAdvances in Neural Information Processing Systems,\nvolume 27. Curran Associates, Inc.\nCharuta Pethe and Steve Skiena. 2019.\nThe trumpi-\nest trump? identifying a subject\u2019s most character-\nistic tweets.\nIn Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1653\u20131663, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMaxime Peyrard, Beatriz Borges, Kristina Gligoric,\nand Robert West. 2021. Laughing heads: Can trans-\nformers detect what makes a sentence funny?\nIn\nProceedings of the Thirtieth International Joint Con-\nference on Arti\ufb01cial Intelligence, IJCAI 2021, Vir-\ntual Event / Montreal, Canada, 19-27 August 2021,\npages 3899\u20133905. ijcai.org.\nJan Wira Gotama Putra and Masayu Leylia Khodra.\n2017. Automatic title generation in scienti\ufb01c arti-\ncles for authorship assistance: a summarization ap-\nproach. Journal of ICT Research and Applications,\n11(3):253\u2013267.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020.\nExploring\nthe limits of transfer learning with a uni\ufb01ed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1\u201367.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don\u2019t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 784\u2013\n789, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nSudha Rao and Joel Tetreault. 2018.\nDear sir or\nmadam, may I introduce the GYAFC dataset: Cor-\npus, benchmarks and metrics for formality style\ntransfer. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 129\u2013140,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685\u20132702, Online. Associa-\ntion for Computational Linguistics.\nAlan Ritter, Stephen Soderland, Doug Downey, and\nOren Etzioni. 2008. It\u2019s a contradiction \u2013 no, it\u2019s not:\nA case study using functional relations. In Proceed-\nings of the 2008 Conference on Empirical Methods\nin Natural Language Processing, pages 11\u201320, Hon-\nolulu, Hawaii. Association for Computational Lin-\nguistics.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015.\nFacenet: A uni\ufb01ed embedding for\nface recognition and clustering. In 2015 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 815\u2013823.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073\u2013\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\n Edwin Simpson, Erik-L\u00e2n Do Dinh, Tristan Miller, and\nIryna Gurevych. 2019.\nPredicting humorousness\nand metaphor novelty with Gaussian process prefer-\nence learning. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL 2019), pages 5716\u20135728.\nTiberiu Sosea and Cornelia Caragea. 2020.\nCancer-\nEmo: A dataset for \ufb01ne-grained emotion detection.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8892\u20138904, Online. Association for Computa-\ntional Linguistics.\nLucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva,\nChrysoula Zerva, Zhenhao Li, Vishrav Chaudhary,\nand Andr\u00e9 FT Martins. 2021. Findings of the wmt\n2021 shared task on quality estimation. In Proceed-\nings of the Sixth Conference on Machine Translation,\npages 684\u2013725.\nLucia Specia and Atefeh Farzindar. 2010. Estimating\nmachine translation post-editing effort with HTER.\nIn Proceedings of the Second Joint EM+/CNGL\nWorkshop: Bringing MT to the User: Research on\nIntegrating MT in the Translation Industry, pages\n33\u201343, Denver, Colorado, USA. Association for Ma-\nchine Translation in the Americas.\nJiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017.\nFrom neural sentence summarization to headline\ngeneration: A coarse-to-\ufb01ne approach.\nIn IJCAI,\nvolume 17, pages 4109\u20134115.\nRaphael Tang, Jaejun Lee, Ji Xin, Xinyu Liu, Yao-\nliang Yu, and Jimmy Lin. 2020. Showing your work\ndoesn\u2019t always work. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2766\u20132772, Online. Association\nfor Computational Linguistics.\nLifu Tu, Richard Yuanzhe Pang, Sam Wiseman, and\nKevin Gimpel. 2020. ENGINE: Energy-based infer-\nence networks for non-autoregressive machine trans-\nlation.\nIn Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2819\u20132826, Online. Association for Computa-\ntional Linguistics.\nAshwin K. Vijayakumar, Michael Cogswell, Ram-\nprasaath R. Selvaraju, Qing Sun, Stefan Lee, David J.\nCrandall, and Dhruv Batra. 2016.\nDiverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models. CoRR, abs/1610.02424.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pappa-\ngari, R. Thomas McCoy, Roma Patel, Najoung Kim,\nIan Tenney, Yinghui Huang, Katherin Yu, Shuning\nJin, Berlin Chen, Benjamin Van Durme, Edouard\nGrave, Ellie Pavlick, and Samuel R. Bowman. 2019.\nCan you tell me how to get past sesame street?\nsentence-level pretraining beyond language model-\ning.\nIn Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 4465\u20134476, Florence, Italy. Association for\nComputational Linguistics.\nWilliam Yang Wang and Kathleen McKeown. 2010.\n\u201cgot you!\u201d:\nAutomatic vandalism detection in\nWikipedia\nwith\nweb-based\nshallow\nsyntactic-\nsemantic modeling.\nIn Proceedings of the 23rd\nInternational Conference on Computational Lin-\nguistics (Coling 2010), pages 1146\u20131154, Beijing,\nChina. Coling 2010 Organizing Committee.\nXinrun Wang, Bo An, Martin Strobel, and Fookwai\nKong. 2018. Catching captain jack: Ef\ufb01cient time\nand space dependent patrols to combat oil-siphoning\nin international waters. In Proceedings of the Thirty-\nSecond AAAI Conference on Arti\ufb01cial Intelligence\nand Thirtieth Innovative Applications of Arti\ufb01cial In-\ntelligence Conference and Eighth AAAI Symposium\non Educational Advances in Arti\ufb01cial Intelligence,\nAAAI\u201918/IAAI\u201918/EAAI\u201918. AAAI Press.\nJoachim Wermter and Udo Hahn. 2006. You can\u2019t beat\nfrequency (unless you use linguistic knowledge) \u2013\na qualitative evaluation of association measures for\ncollocation and term extraction. In Proceedings of\nthe 21st International Conference on Computational\nLinguistics and 44th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 785\u2013792,\nSydney, Australia. Association for Computational\nLinguistics.\nWeizhe Yuan, Pengfei Liu, and Graham Neubig. 2022.\nCan we automate scienti\ufb01c reviewing?\nJournal of\nArti\ufb01cial Intelligence Research, 75:171\u2013212.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263\u201327277.\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,\nSicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi\nZeng, Xiangyu Dong, Ruoyu Zhang, Hongchao\nFang, Penghui Zhu, Shu Chen, and Pengtao Xie.\n2020.\nMedDialog: Large-scale medical dialogue\ndatasets.\nIn Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 9241\u20139250, Online. Associa-\ntion for Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2019.\nPegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nTianyi Zhang, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert.\nIn International\nConference on Learning Representations.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563\u2013578, Hong\nKong, China. Association for Computational Lin-\nguistics.\n FUNNY\n(Almost) No Label No Cry (Patrini et al., 2014)\nThe Trumpiest Trump? Identifying a Subject\u2019s Most Characteristic Tweets (Pethe and Skiena, 2019)\nQuestionable Answers in Question Answering Research: Reproducibility and Variability of Published Results (Crane, 2018)\nKnow What You Don\u2019t Know: Unanswerable Questions for SQuAD (Rajpurkar et al., 2018)\nDear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\n(Rao and Tetreault, 2018)\nCan You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling (Wang et al.,\n2019)\nHARMONET: A Neural Net for Harmonizing Chorales in the Style of J. S. Bach (Hild et al., 1991)\nShowing Your Work Doesn\u2019t Always Work (Tang et al., 2020)\n\"Got You!\": Automatic Vandalism Detection in Wikipedia with Web-based Shallow Syntactic-Semantic Modeling (Wang and\nMcKeown, 2010)\nIt\u2019s a Contradiction - no, it\u2019s not: A Case Study using Functional Relations (Ritter et al., 2008)\nFUNNYmedium\nMedDialog: Large-scale Medical Dialogue Datasets (Zeng et al., 2020)\nCatching Captain Jack: Ef\ufb01cient Time and Space Dependent Patrols to Combat Oil-Siphoning in International Waters (Wang\net al., 2018)\nThe Shattered Gradients Problem: If resnets are the answer, then what is the question? (Balduzzi et al., 2017)\nGo Simple and Pre-Train on Domain-Speci\ufb01c Corpora: On the Role of Training Data for Text Classi\ufb01cation (Edwards et al.,\n2020)\nSentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge (Ke et al., 2020)\nGet Semantic With Me! The Usefulness of Different Feature Types for Short-Answer Grading (Pad\u00f3, 2016)\nWitches\u2019 Brew: Industrial Scale Data Poisoning via Gradient Matching (Geiping et al., 2020)\nENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation (Tu et al., 2020)\nYou Can\u2019t Beat Frequency (Unless You Use Linguistic Knowledge) - A Qualitative Evaluation of Association Measures for\nCollocation and Term Extraction (Wermter and Hahn, 2006)\nOntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres (Zhu et al., 2021)\nTable 12: Examples of funny titles in the annotated data.\nYilun Zhu, Sameer Pradhan, and Amir Zeldes. 2021.\nOntoGUM: Evaluating contextualized SOTA coref-\nerence resolution on 12 more genres. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers), pages 461\u2013467,\nOnline. Association for Computational Linguistics.\nA\nExamples of funny titles\nTable 12 shows 20 titles labeled as FUNNY or\nFUNNYmedium by human annotators.\nB\nHumor annotation + classi\ufb01ation\nThe two annotators \ufb01rst annotated the same 230\ntitles independently, obtaining only 0.397 Kappa\nagreement, which indicates a relatively bad anno-\ntation quality. To improve the inter-agreement be-\ntween the annotators, they then discussed the rea-\nsons leading to disagreement. Subsequently, they\nannotated another 300 titles independently, achiev-\ning a decent 0.650 Kappa for a task as subjective\nas humor. As a consequence, we use the maxi-\nmal label value among the two annotations for\neach title as its \ufb01nal label for the 300 titles, i.e., if\none annotator labels a title with 1 (FUNNYmedium\n), while the other labels with 0 (\u00acFUNNY), we\nassign label 1 to the title. Each annotator then la-\nbeled 600 different titles separately, bringing 1,730\n(230 + 300 + 600 \u00d7 2 = 1730) annotated titles in\ntotal, where 1,603 titles labeled as \u00acFUNNY, 106\nas FUNNYmedium and 21 as FUNNY.\nAs the funny titles (labeled as Funny ) are very\nfew compared to the not funny ones (labeled with\n0), we generate 11 different data splits, where the\ntrain set of each split consists of 100 funny titles\nand 200 not funny ones (randomly sampled from\nthe 1730 titles), while the remaining 27 funny ti-\ntles and other 27 not funny ones compose the dev\nset. From the 11 different data splits, we obtain\n11 classi\ufb01ers (checkpoints selected based on the\nmacro F1 on each dev set).We then evaluate the\nensembles of the 11 classi\ufb01ers on 315 newly an-\nnotated titles by the two annotators, who obtain\n0.639 Kappa agreement this time. With this step,\nwe study the optimal ensemble of the classi\ufb01ers.\nand also obtain more funny titles from the whole\ndata by annotating the funniest titles selected by\nthe ensemble classi\ufb01ers. We design two types of\nensemble classi\ufb01ers:\n\u2022 EnsMV, which relies on the majority vote of\nthe 11 classi\ufb01ers. Speci\ufb01cally, each title re-\nceives 11 labels from the 11 classi\ufb01ers: if\n the number of \u00acFUNNY labels exceeds 5,\nthe title is labeled as \u00acFUNNY; if not, the\ntitle is labeled as FUNNY when the num-\nber of FUNNY labels exceeds the number of\nFUNNYmedium labels, otherwise it is labeled\nas FUNNYmedium.\n\u2022 EnsSUMi,j, which depends on the sum of\nthe label values. The sum of the label val-\nues for each title ranges from 0 (11 classi\ufb01ers\n\u00d7 0 for \u00acFUNNY) to 22 (11 classi\ufb01ers \u00d7 2\nfor FUNNY). We then select a threshold i for\nFUNNYmedium and j for FUNNY: if sum < i,\nthe title is labeled as \u00acFUNNY; otherwise it\nis labeled as FUNNYmedium (when sum < j)\nor FUNNY (when sum \u2265 j).\nIndividuals\nEnsembles\nEnsMV\nEnsSUM7,16\nF1\n57.6%\n61.4%\n62.4%\nTable 13: Average macro F1 over the 11 individual clas-\nsi\ufb01ers and macro F1 of the ensemble classi\ufb01ers from\nstage 1 on the evaluation data of 315 titles (where the\ntwo annotators obtain 0.639 kappa). We bold the high-\nest macro F1 score.\nTable 13 shows the evaluation results of Stage\n1; we only present the performance of EnsSUMi,j\nwith optimal i and j here, i.e., EnsSUM7,16. We\nobserve that: (1) both ensembles perform better\nthan the individual ones (+4-5% macro F1) and (2)\nEnsSUM7,16 is slightly better than EnsMV (62.4%\nvs. 61.4% macro F1).\nKappa\n#titles\nthree-way\nbinary\nStage 1\n230\n0.397\n0.513\n300\n0.650\n0.754\n315\n0.639\n0.709\nStage 2\n197\n0.649\n0.661\nTable 14: Kappa agreements between the two annota-\ntors on several data pieces. \u201c#titles\u201d refers to the num-\nber of titles in a certain piece of data. We bold the\nhigher Kappa on the same data.\nC\nParameters for title generation\nTable 15 displays the hyperparameter used in Sec-\ntion 4.2 for training the six models and Table 16\nshows the parameters used for beam search.\nD\nParameters for humor generation\nWe train BARTxsum on our train set using the\nAdamW optimizer with weight decay 0.01 and\nlearning rate 4e-05 for 5 epochs. Then we con-\ntinue to train it on the pseudo data for one epoch\nto obtain BARTxsum+pseudo. We use the default\nsettings in Huggingface\u2019s Trainer API for the other\nhyperparameters.\nE\nExamples of low-quality\nsystem-generated funny titles\nTable 17 shows 10 system-generated low-quality\nfunny titles, of which 5 are from ChatGPT and\n5 from BARTxsum. They obtain good results in\nthe humor ranking but bad results in the quality\nranking according to our human evaluation.\n learning rate\nbatch size\nepochs\ngradient accumulation steps\nBARTxsum\n3e-05\n3\n3\n8\nPEGASUSxsum\n6e-04\n3\n3\n8\nBARTbase\n3e-04\n8\n3\n8\nGPT2\n3e-04\n2\n3\n8\nT5\n3e-04\n8\n3\n8\nBARTcnn\n3e-04\n4\n3\n8\nTable 15: Training hyperparameter for title generation. We use the AdamW optimizer with a weight decay of 0.01\nand keep the other settings as default in Huggingface\u2019s Trainer API.\nmax length\n30\nmin length\n3\nrepetition penalty\n2\nlength penalty\n10\nnum beams\n5\nnum return sequences\n5\nTable 16: Parameter settings for beam search.\nBARTxsum\nDon\u2019t Invite Adversaries to Poison Your Data: Exploiting Federated Learning for Adversarial Backdoor Attacks\nDon\u2019t Take the Easy Way Out: Generating Adversarial Negative Responses with Large-Scale Language Models for Dialogue\nSelection\nDon\u2019t Give Up on Style: Learn to Generate Stylistically-Diverse Summaries with Multiple Decoders\nCKD: Curriculum Knowledge Distiller for Cross-Lingual Sentiment Analysis with Emoji\nSuccessive Prompting: Learning to Break Down Complex Questions into As Simple As Possible\nChatGPT\nGraphin\u2019 It Up: A Humorous Guide to Generative Knowledge Construction\nTiny Tasks, Big Results: A Hilarious Guide to Few-Shot Relation Extraction\nRevealing the Magic Behind Transformer Language Models: A Lighthearted Investigation\nAsk and You Shall Receive: A Whimsical Approach to Automatic Question Generation\nFederated Learning: The More You Poison, the More You Win!\nTable 17: Examples of system-generated low-quality funny titles, which obtain high humor ranks but low quality\nranks in the human evaluation.\n"}, "On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers": {"authors": ["Tianchu Ji", "Shraddhan Jain", "Michael Ferdman", "Peter Milder", "H. Andrew Schwartz", "Niranjan Balasubramanian"], "title": "On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers", "url": "https://arxiv.org/pdf/2106.01335.pdf", "abstract": "How much information do NLP tasks really need from a transformer's attention mechanism at application-time (inference)? From recent work, we know that there is sparsity in transformers and that the floating-points within its computation can be discretized to fewer values with minimal loss to task accuracies. However, this requires retraining or even creating entirely new models, both of which can be expensive and carbon-emitting. Focused on optimizations that do not require training, we systematically study the full range of typical attention values necessary. This informs the design of an inference-time quantization technique using both pruning and log-scaled mapping which produces only a few (e.g. $2^3$) unique values. Over the tasks of question answering and sentiment analysis, we find nearly 80% of attention values can be pruned to zeros with minimal ($< 1.0\\%$) relative loss in accuracy. We use this pruning technique in conjunction with quantizing the attention values to only a 3-bit format, without retraining, resulting in only a 0.8% accuracy reduction on question answering with fine-tuned RoBERTa.", "arxiv_id": "2106.01335", "published_date": "2021-06-02", "year": 2021, "introduction": "Introduction While the verdict is still out on which large language model will prove best, at this point in time, all contenders rely on multi-headed attention over multiple layers. Many have investigated whether attention (the output of the softmax, \u03b1) itself is qualitatively sensible (e.g., correlating with linguistic aspects) (Vig and Belinkov, 2019; Clark et al., 2019; Voita et al., 2018, 2019; Kovaleva et al., 2019; Rogers et al., 2020) or how useful it is for interpreting models (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Brunner et al., 2020; Rogers et al., 2020). Others have focused on inducing sparsity in the attention: whether some of the structural components (the softmax function, attention heads and layers) introduce attention sparsity (Correia et al., 2019; Michel et al., 2019; Voita et al., 2019; Sajjad et al., 2020), if the model tends to focus on a small amount of tokens (Clark et al., 2019; Ramsauer et al., 2020), and the interpretability of such sparsity (Chen et al., 2020; Rogers et al., 2020). Yet, little is known about our ability to induce sparsity or reduce its values at applicationtime, and what role the inherent sparsity could play in building inference-time ef\ufb01cient transformers. This work focuses on a systematic study of the quantitative distribution of the attention values across the layers and heads as well as the potential for reducing the information content of attention values during inference at application-time1. We consider two popular pretrained transformer models: BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) over tasks of Masked Language Modeling as well as question answering and sentiment analysis. We explore the attention distributions on the different models and tasks, and quantitatively pro\ufb01le the sparse attention that commonly exists in the transformer model. Motivated by the high levels of inherent sparsity in these distributions, we design a pruning and quantization technique and test the limits of information necessary from attention. We \ufb01nd that most attention values can be pruned (i.e. set to zero) and the remaining non-zero values can be mapped to a small number of discrete-levels (i.e. unique values) without any signi\ufb01cant impact on accuracy. Approximately 80% of the values can be set to zero without signi\ufb01cant impact on the accuracy for QA and sentiment analysis tasks. Further, when we add quantization utilizing a logscaling, we \ufb01nd a 3-bit discrete representation is suf\ufb01cient to achieve accuracy within 1% of using the full \ufb02oating points of the original model. 1Our analyzing code and data are available at https://github.com/StonyBrookNLP/spiqa ", "conclusion": "Conclusion We demonstrated that pruning near-zero values and large reductions in the number of bits needed for attention, even at application time without retraining or \ufb01ne-tuning, is possible with little loss of accuracy. This suggests attention plays a very coarse role in model accuracy at inference-time, yielding opportunities to run transformers more ef\ufb01ciently over applications. While quantization during training had previously shown promise (down to three bits, for most weights of the transformer), we observed the same reduction potential on attention values at application-time, allowing their representation to be reduced down to three bits (or even two for sentiment) with little effort (e.g., without retraining or using a dynamic quantization range). This shows it is feasible to implement ef\ufb01cient transformers by leveraging heavily sparse and quantized attention values, suggesting the possibility of building specialized hardware (e.g., FPGA and ASIC accelerators) to optimize the transformer\u2019s evaluation on-the-\ufb02y. Acknowledgments We would like to express our appreciation to Adithya V. Ganesan who assisted with our experiments. This material is based upon work supported by the National Science Foundation under Grant Nos. 2007362 and 1918225. The experiments were conducted with equipment purchased through NSF Grant No. OAC-1919752. ", "full_text": "On the Distribution, Sparsity, and Inference-time Quantization of\nAttention Values in Transformers\nTianchu Ji1, Shraddhan Jain2, Michael Ferdman2,\nPeter Milder1, H. Andrew Schwartz2, and Niranjan Balasubramanian2\n1,2Stony Brook University\n1{tianchu.ji, peter.milder}@stonybrook.edu\n2{shrjain, mferdman, has, niranjan}@cs.stonybrook.edu\nAbstract\nHow much information do NLP tasks really\nneed from a transformer\u2019s attention mecha-\nnism at application-time (inference)?\nFrom\nrecent work, we know that there is sparsity\nin transformers and that the \ufb02oating-points\nwithin its computation can be discretized to\nfewer values with minimal loss to task accu-\nracies.\nHowever, this requires retraining or\neven creating entirely new models, both of\nwhich can be expensive and carbon-emitting.\nFocused on optimizations that do not require\ntraining, we systematically study the full range\nof typical attention values necessary. This in-\nforms the design of an inference-time quanti-\nzation technique using both pruning and log-\nscaled mapping which produces only a few\n(e.g. 23) unique values. Over the tasks of ques-\ntion answering and sentiment analysis, we \ufb01nd\nnearly 80% of attention values can be pruned\nto zeros with minimal (< 1.0%) relative loss\nin accuracy. We use this pruning technique in\nconjunction with quantizing the attention val-\nues to only a 3-bit format, without retraining,\nresulting in only a 0.8% accuracy reduction on\nquestion answering with \ufb01ne-tuned RoBERTa.\n1\nIntroduction\nWhile the verdict is still out on which large lan-\nguage model will prove best, at this point in time,\nall contenders rely on multi-headed attention over\nmultiple layers. Many have investigated whether\nattention (the output of the softmax, \u03b1) itself is\nqualitatively sensible (e.g., correlating with lin-\nguistic aspects) (Vig and Belinkov, 2019; Clark\net al., 2019; Voita et al., 2018, 2019; Kovaleva\net al., 2019; Rogers et al., 2020) or how useful it\nis for interpreting models (Jain and Wallace, 2019;\nWiegreffe and Pinter, 2019; Brunner et al., 2020;\nRogers et al., 2020). Others have focused on in-\nducing sparsity in the attention: whether some of\nthe structural components (the softmax function,\nattention heads and layers) introduce attention spar-\nsity (Correia et al., 2019; Michel et al., 2019; Voita\net al., 2019; Sajjad et al., 2020), if the model tends\nto focus on a small amount of tokens (Clark et al.,\n2019; Ramsauer et al., 2020), and the interpretabil-\nity of such sparsity (Chen et al., 2020; Rogers et al.,\n2020). Yet, little is known about our ability to\ninduce sparsity or reduce its values at application-\ntime, and what role the inherent sparsity could play\nin building inference-time ef\ufb01cient transformers.\nThis work focuses on a systematic study of\nthe quantitative distribution of the attention values\nacross the layers and heads as well as the potential\nfor reducing the information content of attention\nvalues during inference at application-time1. We\nconsider two popular pretrained transformer mod-\nels: BERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019) over tasks of Masked Language Mod-\neling as well as question answering and sentiment\nanalysis. We explore the attention distributions on\nthe different models and tasks, and quantitatively\npro\ufb01le the sparse attention that commonly exists\nin the transformer model. Motivated by the high\nlevels of inherent sparsity in these distributions,\nwe design a pruning and quantization technique\nand test the limits of information necessary from\nattention.\nWe \ufb01nd that most attention values can be pruned\n(i.e. set to zero) and the remaining non-zero values\ncan be mapped to a small number of discrete-levels\n(i.e. unique values) without any signi\ufb01cant impact\non accuracy. Approximately 80% of the values\ncan be set to zero without signi\ufb01cant impact on\nthe accuracy for QA and sentiment analysis tasks.\nFurther, when we add quantization utilizing a log-\nscaling, we \ufb01nd a 3-bit discrete representation is\nsuf\ufb01cient to achieve accuracy within 1% of using\nthe full \ufb02oating points of the original model.\n1Our\nanalyzing\ncode\nand\ndata\nare\navailable\nat\nhttps://github.com/StonyBrookNLP/spiqa\n 2\nMethod\nTo analyze attention distribution we \ufb01rst plot his-\ntograms of attention values for BERT (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019) models. We\nalso compute a sparsity distribution using the pro-\nportion of the attention values smaller than a given\nthreshold. For attention pruning, we \ufb01nd attention\nvalues that are below a speci\ufb01ed threshold and re-\nplace them with zero. We experiment with different\nthresholds. For quantization to k-bits we map the\ncontinuous attention values to one of 2k real val-\nues2. We use two methods: (i) Linear - Bin the\nattention values to 2k quantiles and set the mid-\npoint of each as the quantized value. (ii) Log - Bin\nthe log transformed attention values and pick the\nmid-point of each on the log scale as the quantized\nvalue. The quantization methods are explained in\ndetail in Appendix E.\nWe apply these inference-time (i.e. no training)\ntechniques on three tasks: masked language mod-\neling, question answering and sentiment analysis.\nFor QA we used BERT3 and RoBERTa4 models\n\ufb01ne-tuned on SQuAD v1.1 (Rajpurkar et al., 2016).\nFor sentiment analysis we used RoBERTa5 \ufb01ne-\ntuned on the SST-2 dataset (Socher et al., 2013).\nFor both these tasks we report accuracy on the\ncorresponding development sets. For the Masked\nLanguage Modeling (MLM) task we report pseudo-\nperplexity (Salazar et al., 2020) computed on the\nHuggingface Wikipedia dataset6.\n3\nEvaluation\nAttention distribution and sparsity.\nA thor-\nough quantitative analysis on the attention distri-\nbution could help build ef\ufb01cient transformers by\nproviding useful information, such as the degree\nof sparsity and the range of the attention values.\nWe plot the histogram of each token\u2019s attention to\nall the others (\u03b1i) and provide three examples of\nthe heads in Figure 1 to investigate the density of\nthe attention values, how differently the tokens at-\ntend to others in the same attention head, and how\nsparse a token/head/layer\u2019s attention can be. We\n\ufb01nd that, for most of the heads, attention forms\na lognormal-like distribution similar to Figure 1a.\n2Note here we use full precision \ufb02oating point rather than\na k-bit value since our main goal is to see how many discrete\nlevels of attention is needed.\n3http://huggingface.co/csarron/bert-base-uncased-squad-v1\n4http://huggingface.co/csarron/roberta-base-squad-v1\n5http://huggingface.co/textattack/roberta-base-SST-2\n6https://huggingface.co/datasets/wikipedia\nOn some heads, some of the attention for query to-\nken (\u03b1i) have more tiny attention values (\u03b1ij) and\ninduce more sparsity than others (like in Figure 1c).\nWe also observe entire heads with high sparsity, in\nwhich nearly all tokens only slightly attend to oth-\ners (like in Figure 1b). Our observation con\ufb01rms\nthe existence of sparsity in the attention heads.\nA key motivation for us is to quantitatively char-\nacterize sparsity, especially in terms of how much\npotential there is in reducing the information con-\ntent in attention values. To this end, we speci\ufb01cally\nmeasure the proportion of small attention values\nby counting the number of \u03b1ij that sum up to 0.5\nin each \u03b1i. This indicates that most heads focus\nstrongly on fewer than 10 tokens on average (de-\ntails in Appendix A), leading to notable sparsity\nand suggesting large potential for conveying the\nsame information as continuous attention values\nusing fewer discrete levels.\nBeyond these, we occasionally observe outlier\nattention histograms (like the outliers between\n[10\u22124, 10\u22121] in Figure 1b). We also found notice-\nable differences on the attention histograms from\nlayer to layer. These \ufb01ndings are related to the\nworks on the syntactic heads/special tokens (Voita\net al., 2019; Kovaleva et al., 2019; Voita et al., 2018;\nClark et al., 2019; Rogers et al., 2020)) and the dif-\nferences of the layers/heads (Correia et al., 2019;\nClark et al., 2019). We discuss how our \ufb01ndings\nrelate to them in Appendices B and C.\nLimited effect of near-zero attention values dur-\ning inference.\nThe inherent sparsity we observed\nmotivates us to explore the sparsity of attention at\ninference-time\u2014how much attention can be pruned\nduring inference, without impacting the model ac-\ncuracy? By setting up a series of pruning thresh-\nolds, we clamp different proportions of the atten-\ntion to zero and examine how attention sparsity\naffects the accuracy, on both pretrained and \ufb01ne-\ntuned models. The results shown in Figure 2 in-\ndicate that the sparsity can grow above 80% with\nonly a 0.1%\u20131.3% drop in accuracy. Speci\ufb01cally,\nthe pretrained BERT model achieves 99.9% of\nthe original performance with 87% of the sparsity\non Masked Language Modeling. By comparing\nRoBERTa\u2019s accuracy on different tasks, we \ufb01nd\nthat sentiment analysis suffers more from increased\nsparsity, suggesting that different models are dif-\nferentially sensitive to the induced sparsity. Our\nresults quantitatively show how much sparsity can\nbe induced in all the attention values without losing\n (a) Layer 1 Head 4\n(b) Layer 2 Head 3\n(c) Layer 12 Head 11\nFigure 1: Normalized histograms (in blue) and cumulative histograms (in red) for every token\u2019s attention to others\n(\u03b1i) at different heads in the pretrained RoBERTa model, starting from 10\u22128. The histograms show different\npatterns of attention distribution. E.g., in (b) many tokens\u2019 attention form an evenly distributed histogram from\n10\u22128 to 1, and most of the \u03b1i have 80%\u2013100% of all the attention values (\u03b1ij) \u2264 10\u22128. This indicates a higher\nlevel of sparsity compared to (a) and (c). The \u201csparsity distribution\u201d bar on the right shows the density of \u03b1i to\neach level of sparsity. E.g., the red cell with \u201c0.96\u201d between 0.9\u20131.0 in (b) means 96% of all \u03b1i have sparsity\nbetween 90%\u2013100%, whereas the sparsity is the proportion of \u03b1ij in \u03b1i that are less than 10\u22128.\n0.00\n0.25\n0.50\n0.75\n1.00\nsparsity\n0\n20\n40\n60\n80\n100\nEM score\nQA\nRoBERTa SQuAD\nBERT SQuAD\n0.00\n0.25\n0.50\n0.75\n1.00\nsparsity\n0\n20\n40\n60\n80\n100\naccuracy\nSA\nRoBERTa SST-2\n0.00\n0.25\n0.50\n0.75\n1.00\nsparsity\n0\n5\n10\n15\n20\n25\npseudo-perplexity\nMLM\nRoBERTa MLM\nBERT MLM\nFigure 2: Exact Match score (for QA), Accuracy (for SA) and pseudo-perplexity (for MLM) under different levels\nof sparsity that we induce, showing that on these models and tasks \u223c80% of the sparsity can be induced with\nlimited performance drop. X-axis values denotes the induced sparsity levels measured as the proportion of the\nattention values less than a speci\ufb01ed threshold.\naccuracy, suggesting that one can expect to prune\nup to 80% of the attention values without retrain-\ning.\nQuantizing pruned attention.\nQuantization is\noften used to compress transformer models for\nhigher computational and memory ef\ufb01ciency. Re-\ncently Prato et al. (2020) showed that for machine\ntranslation, attention values in transformers can\nbe quantized with only a small impact on accu-\nracy. While their results suggest that full precision\nattention values may not be necessary for high ac-\ncuracy, it is unclear if one can retain the accuracy in\ninference-time quantization in general settings i.e.,\nwithout retraining. Bhandare et al. (2019); Shen\net al. (2020); Prato et al. (2020) have proved the im-\nportance of meticulously selecting the range of the\nquantization when pursuing higher accuracy. Intu-\nitively, pruning the tiny attention values will lead\nto a narrower quantization range with more precise\nvalue representatives. For example, if all \u03b1 < 10\u22123\nare pruned before 3-bit quantization, all numbers\nwe need to quantize will land in [10\u22123, 1] rather\nthan [0, 1], with the 8 quantiles of the quantization\nlocated more densely; this forms a higher resolu-\ntion within the quantization range compared to the\nnon-pruned version. Since we observed that prun-\ning most of the attention values during inference\nhas minimal effect on the accuracy when removing\nonly the tiny attention values (\u03b1 < 10\u22123 in our\ncase), we hypothesize that properly pruning atten-\ntion values will help increase the accuracy of the\nquantized model.\nTo verify the pruning hypothesis, we selected\ntwo quantization methods: linear scale quantiza-\ntion and logarithmic scale quantization (details in\n 16\n0\n20\n40\n60\n80\nRoBERTa\nBERT\n1\n2\n3\n4\n5\n6\n7\n8\n9\nRoBERTa\nBERT\n#bits\nEM score\nRoBERTa-linear\nRoBERTa-linear-pruned\nRoBERTa-log\nRoBERTa-log-pruned\nBERT-linear\nBERT-linear-pruned\nBERT-log\nBERT-log-pruned\nRoBERTa-boolean\nBERT-boolean\n(a) EM scores of the models with differ-\nently quantized attention\n10\n6\n10\n3\n100\npruning threshold\n50\n60\n70\n80\n90\nAccuracy\noriginal\nRoBERTa-SST\n10\n6\n10\n3\n100\npruning threshold\n0\n20\n40\n60\n80\nEM score\noriginal\nRoBERTa-SQuAD\n10\n6\n10\n3\n100\npruning threshold\n102\n104\n106\n108\npseudo-perplexity\noriginal\nRoBERTa-MLM\n(b) performance with different pruning thresholds for 2-bit log quantization\nFigure 3: Performance of the quantized models with/without attention pruning, showing that the attention can be\neffectively quantized to as low as 3 bits with certain pruning thresholds. (a) Exact Match scores for the QA with\ndifferent quantization methods on \ufb01ne-tuned BERT and RoBERTa. \u201cBoolean\u201d quantization is provided as the ex-\ntreme case of quantization to a single bit. The pruning has only negligible effect on the linear scale quantization so\nthat \u201c*-linear\u201d and \u201c*-linear-pruned\u201d curves are highly overlapped. (b) Accuracy of the \ufb01ne-tuned RoBERTa mod-\nels with 2-bit quantized attention for QA, SA and MLM respectively. The attention is pruned before quantization\nby using different thresholds (shown on the x-axis). In all the \ufb01gures, the original model\u2019s performance scores are\nmarked with black dashed lines.\nAppendix E), quantized only the transformers\u2019 at-\ntention with various number of bits, and measured\nthe accuracy of the models. Then we repeated the\nexperiment but pruning \u03b1 < 10\u22123 (which creates\n\u223c80% sparsity with limited accuracy drop in our\nsparsity experiment) before quantizing the atten-\ntion.\nWe evaluate the models on different tasks to com-\npare how pruning the attention affects the accuracy\nwhen quantizing. Results in Figure 3a show that for\nboth BERT and RoBERTa models, log quantization\nis greatly improved after pruning, especially with\nthe 3-bit and 2-bit quantization. Notably, the 3-bit\nlog quantization with pruning only loses 0.8% and\n1.5% of the original accuracy for the RoBERTa\nand BERT, respectively. Contrarily, the pruning\nhas very limited effect on the linear quantization\nbecause the selected pruning threshold results only\nin a negligible change to the effective quantiza-\ntion range. (Details are provided in Appendix F.)\nWe also repeated the experiment on other tasks\nand found 2-bit log quantization with pruning only\nloses 0.7% accuracy on RoBERTa \ufb01ne-tuned for\nsentiment analysis. (Full results are provided in\nAppendix D.)\nWe further experimented with different pruning\nthresholds (Figure 3b) and observed that pruning\n\u03b1 < 10\u22122 gives the best performance; the thresh-\nold can undermine model accuracy if it is either too\nlarge (> 10\u22122) or too small (< 10\u22123).\nOur results prove that pruning the sparse atten-\ntion values helps recover model accuracy with log-\nscale quantization methods, without any retrain-\ning or \ufb01ne-tuning. With attention pruning, a trans-\nformer can retain a comparable amount of accuracy\neven with a simple, low-precision quantized atten-\ntion (in our case, a 3-bit log quantization).\nDiscussion.\nSparsifying the attention can help re-\nduce both the computation and memory cost of\nself-attention during inference. Our experiments\nabove demonstrate that it is possible to prune ap-\nproximately 80% of attention values while quan-\ntizing them to a 3-bit representation. Specialized\nhardware (FPGA and ASIC) can be designed to ef-\n\ufb01ciently operate on highly quantized datatypes and\nto \u201cskip\u201d the zeros to accelerate deep learning infer-\nence, such as Albericio et al. (2016) (which targets\nCNNs). Our results show that such an accelerator\ncould effectively reduce the arithmetic cost of com-\nputing attention matrices by 80% and reduce the\nmemory footprint of the attention matrices by up to\n96% (compounding the effect of sparse representa-\ntion and quantization). Although attention matrices\nare not occupying a huge amount of storage, these\nmemory savings can potentially greatly increase\nthe ef\ufb01ciency of a specialized hardware accelera-\ntor by reducing its on-chip SRAM usage and/or\nits memory bandwidth requirement. Further, the\ncomputational savings can help reduce the latency.\nLastly, it is important to note that the bene\ufb01ts of\nattention sparsity may extend much further than\njust computing attention values themselves; other\ncomputations in the transformer network can also\n bene\ufb01t from leveraging the high degree of sparsity\nwithout retraining/\ufb01ne-tuning, potentially yielding\nlarger bene\ufb01ts. Future work will investigate the\ncomputational bene\ufb01ts of utilizing attention spar-\nsity and the design of customized hardware accel-\nerators to ef\ufb01ciently do so.\n4\nRelated Work\nAttention distribution.\nMany have abstractly\nstudied the attention distribution from different\naspects (Clark et al., 2019; Pascual et al., 2021;\nRamsauer et al., 2020; Correia et al., 2019), but\nnone speci\ufb01cally have shown the histogram of the\n\u03b1i directly, nor did they investigate the sparse at-\ntention values quantitatively. Correia et al. (2019)\nindicated that not all of the sparsity in attention\nwas caused by the softmax, and it remained unclear\nwhether such sparsity affected accuracy (which is\ninspected in this paper).\nPruning.\nVoita et al. (2019); Sajjad et al. (2020);\nMichel et al. (2019); Kovaleva et al. (2019) pruned\none or more heads/layers resulting in comparable\nor higher model accuracy, either with or without\n\ufb01ne-tuning. These approaches assume that some\nheads/layers interpret the information redundantly,\nwhich is not always true (Brunner et al., 2020;\nRogers et al., 2020). In contrast, our work focuses\non a more general method of inducing attention\nsparsity without operating at layer/head granular-\nity.\nQuantization.\nBhandare et al. (2019); Shen et al.\n(2020); Prato et al. (2020) have shown bene\ufb01ts\nfrom selecting the quantization range, which mo-\ntivates us to prune the attention before quantiza-\ntion (Section 3). Kim et al. (2021); Zafrir et al.\n(2019); Prato et al. (2020) required re-training\nwhile ours does not. Zhang et al. (2020); Bai et al.\n(2020); Zadeh et al. (2020) focused on quantizing\nthe weights rather than the attention values, which\nis out of our scope.\nSparse transformers and attention visualiza-\ntion\nParmar et al. (2018); Child et al. (2019);\nHo et al. (2019); Beltagy et al. (2020); Ainslie et al.\n(2020); Li and Chan (2019); Tay et al. (2020) have\nproposed/summarized various kinds of ef\ufb01cient\ntransformers utilizing induced attention sparsity.\nHowever, none of them quantitatively analyzed the\nstatistical distribution and the tiny values of the at-\ntention. Vig (2019); Hoover et al. (2020) proposed\ninstance-level attention visualization tools. These\nare complementary to our quantitative visualization\nof the distributions of all attention values.\n5\nConclusion\nWe demonstrated that pruning near-zero values and\nlarge reductions in the number of bits needed for at-\ntention, even at application time without retraining\nor \ufb01ne-tuning, is possible with little loss of accu-\nracy. This suggests attention plays a very coarse\nrole in model accuracy at inference-time, yielding\nopportunities to run transformers more ef\ufb01ciently\nover applications. While quantization during train-\ning had previously shown promise (down to three\nbits, for most weights of the transformer), we ob-\nserved the same reduction potential on attention\nvalues at application-time, allowing their represen-\ntation to be reduced down to three bits (or even two\nfor sentiment) with little effort (e.g., without re-\ntraining or using a dynamic quantization range).\nThis shows it is feasible to implement ef\ufb01cient\ntransformers by leveraging heavily sparse and quan-\ntized attention values, suggesting the possibility\nof building specialized hardware (e.g., FPGA and\nASIC accelerators) to optimize the transformer\u2019s\nevaluation on-the-\ufb02y.\nAcknowledgments\nWe would like to express our appreciation to\nAdithya V. Ganesan who assisted with our experi-\nments.\nThis material is based upon work supported by\nthe National Science Foundation under Grant Nos.\n2007362 and 1918225. The experiments were con-\nducted with equipment purchased through NSF\nGrant No. OAC-1919752.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding Long and Structured Inputs\nin Transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268\u2013284, Online. Asso-\nciation for Computational Linguistics.\nJ. Albericio, P. Judd, T. Hetherington, T. Aamodt,\nN. E. Jerger, and A. Moshovos. 2016.\nCn-\nvlutin: Ineffectual-Neuron-Free Deep Neural Net-\nwork Computing. In 2016 ACM/IEEE 43rd Annual\nInternational Symposium on Computer Architecture\n(ISCA), pages 1\u201313. ISSN: 1063-6897.\n Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin,\nXin Jiang, Qun Liu, Michael Lyu, and Irwin King.\n2020.\nBinaryBERT: Pushing the Limit of BERT\nQuantization.\narXiv:2012.15701 [cs].\nArXiv:\n2012.15701.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer:\nThe Long-Document Transformer.\narXiv:2004.05150 [cs]. ArXiv: 2004.05150.\nAishwarya\nBhandare,\nVamsi\nSripathi,\nDeepthi\nKarkada, Vivek Menon, Sun Choi, Kushal Datta,\nand Vikram Saletore. 2019.\nEf\ufb01cient 8-Bit Quan-\ntization of Transformer Neural Machine Language\nTranslation Model. arXiv:1906.00532 [cs]. ArXiv:\n1906.00532.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\nhofer. 2020. On Identi\ufb01ability in Transformers. In\nInternational Conference on Learning Representa-\ntions.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020.\nThe Lottery Ticket Hypothesis for\nPre-trained BERT Networks. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n15834\u201315846. Curran Associates, Inc.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating Long Sequences with\nSparse Transformers. arXiv:1904.10509 [cs, stat].\nArXiv: 1904.10509 version: 1.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What Does BERT\nLook at? An Analysis of BERT\u2019s Attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276\u2013286, Florence, Italy. Association\nfor Computational Linguistics.\nGonc\u00b8alo M. Correia, Vlad Niculae, and Andr\u00b4e F. T.\nMartins. 2019. Adaptively Sparse Transformers. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2174\u2013\n2184, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn,\nand Tim Salimans. 2019. Axial Attention in Multi-\ndimensional Transformers. arXiv:1912.12180 [cs].\nArXiv: 1912.12180.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2020. exBERT: A Visual Analysis Tool\nto Explore Learned Representations in Transformer\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 187\u2013196, Online. As-\nsociation for Computational Linguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543\u20133556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W.\nMahoney, and Kurt Keutzer. 2021. I-BERT: Integer-\nonly BERT Quantization.\narXiv:2101.01321 [cs].\nArXiv: 2101.01321.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the Dark Secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365\u20134374, Hong Kong, China. Association for\nComputational Linguistics.\nLala Li and William Chan. 2019. Big bidirectional in-\nsertion representations for documents. In Proceed-\nings of the 3rd Workshop on Neural Generation and\nTranslation, pages 194\u2013198, Hong Kong. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach.\narXiv:1907.11692 [cs].\nArXiv:\n1907.11692.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre Sixteen Heads Really Better than One? In Ad-\nvances in Neural Information Processing Systems,\nvolume 32, pages 14014\u201314024. Curran Associates,\nInc.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018.\nImage Transformer.\nIn Proceed-\nings of the 35th International Conference on Ma-\nchine Learning, volume 80 of Proceedings of Ma-\nchine Learning Research, pages 4055\u20134064, Stock-\nholmsm\u00a8assan, Stockholm Sweden. PMLR.\nDamian Pascual, Gino Brunner, and Roger Watten-\nhofer. 2021. Telling BERT\u2019s full story: from Local\nAttention to Global Aggregation. arXiv:2004.05916\n[cs]. ArXiv: 2004.05916.\nGabriele Prato,\nElla Charlaix,\nand Mehdi Reza-\ngholizadeh. 2020. Fully Quantized Transformer for\nMachine Translation.\nIn Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020,\n pages 1\u201314, Online. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016.\nSQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2383\u20132392,\nAustin, Texas. Association for Computational Lin-\nguistics.\nHubert Ramsauer, Bernhard Sch\u00a8a\ufb02, Johannes Lehner,\nPhilipp Seidl, Michael Widrich, Thomas Adler,\nLukas Gruber, Markus Holzleitner, Milena Pavlovi\u00b4c,\nGeir Kjetil Sandve, Victor Greiff, David Kreil,\nMichael Kopp, G\u00a8unter Klambauer, Johannes Brand-\nstetter, and Sepp Hochreiter. 2020.\nHop\ufb01eld Net-\nworks is All You Need. arXiv:2008.02217 [cs, stat].\nArXiv: 2008.02217.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020.\nA Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the As-\nsociation for Computational Linguistics, 8(0):842\u2013\n866. Number: 0.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and\nPreslav Nakov. 2020. Poor Man\u2019s BERT: Smaller\nand Faster Transformer Models. arXiv:2004.03844\n[cs]. ArXiv: 2004.03844.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and\nKatrin Kirchhoff. 2020. Masked Language Model\nScoring. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2699\u20132712, Online. Association for Computa-\ntional Linguistics.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W. Mahoney, and\nKurt Keutzer. 2020. Q-BERT: Hessian Based Ultra\nLow Precision Quantization of BERT. Proceedings\nof the AAAI Conference on Arti\ufb01cial Intelligence,\n34(05):8815\u20138821. Number: 05.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank.\nIn Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631\u20131642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020.\nEf\ufb01cient Transformers: A Survey.\narXiv:2009.06732 [cs].\nArXiv: 2009.06732 ver-\nsion: 1.\nJesse Vig. 2019.\nA Multiscale Visualization of At-\ntention in the Transformer Model. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations,\npages 37\u201342, Florence, Italy. Association for Com-\nputational Linguistics.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing the\nStructure of Attention in a Transformer Language\nModel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63\u201376, Florence, Italy. As-\nsociation for Computational Linguistics.\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-Aware Neural Machine Trans-\nlation Learns Anaphora Resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264\u20131274, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing Multi-Head\nSelf-Attention: Specialized Heads Do the Heavy\nLifting, the Rest Can Be Pruned. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797\u20135808, Florence,\nItaly. Association for Computational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019.\nAttention\nis not not Explanation.\nIn Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 11\u201320, Hong Kong,\nChina. Association for Computational Linguistics.\nJohn Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Dur-\nrani, Fahim Dalvi, and James Glass. 2020.\nSimi-\nlarity Analysis of Contextual Word Representation\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4638\u20134655, Online. Association for Computa-\ntional Linguistics.\nA. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos.\n2020.\nGOBO: Quantizing Attention-Based NLP\nModels for Low Latency and Energy Ef\ufb01cient Infer-\nence. In 2020 53rd Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO), pages\n811\u2013824.\nO\ufb01r Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8Bit BERT.\narXiv:1910.06188 [cs]. ArXiv: 1910.06188.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020.\nTernary-\nBERT: Distillation-aware Ultra-low Bit BERT. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 509\u2013521, Online. Association for Computa-\ntional Linguistics.\n A\nConsistency of Inducing Sparsity in\nthe Attention\nBecause the softmax function normalizes its in-\nput into a probability distribution that sums to 1\nand larger values are projected to larger probabili-\nties, when highly focused tokens with close-to-one\nprobability appear in the attention, they must be\naccompanied by a large number of near-zero atten-\ntion values like in Figure 1b. Thus, the number of\nclose-to-one attention values not only represents\nhow many tokens are strongly attended, but also\nwhether \u03b1i has many near-zero attention values.\nTo quantitatively evaluate the proportion of these\ntiny attention values, we computed the number of\nthe largest values in each \u03b1i that sum to 0.5, visu-\nalizing their mean and standard deviation in Fig-\nure 4. On both pretrained RoBERTa and SQuAD-\n\ufb01ne-tuned RoBERTa, we observed that most of the\nheads require on average fewer than ten attention\nvalues to sum up to 0.5, meaning that most heads\nfocus strongly on fewer than ten tokens on average,\nleading to notable sparsity. We observe that seven\nof twelve heads in the \ufb01rst layers of both models\nhave a larger average number (> 10) of such major\ntokens. For deeper layers, the average number of\nmajor tokens decreases. Finally, in the last two lay-\ners, we again see an increasing trend in the average\nnumber of major tokens. This indicates that middle\nlayers commonly focus on only a small number of\ntokens, making these layers rich in sparsity. This\ncon\ufb01rms the \u201csparse deeper layers\u201d identi\ufb01ed by\nCorreia et al. (2019); Clark et al. (2019) and fur-\nther proves the existence of heavily focused tokens.\nIt implies the large potential of inducing sparsity\nin the transformers and motivates us to explore\nhow these sparse attention values contribute to the\nmodel accuracy. We also examined the BERT pre-\ntrained model and SQuAD-\ufb01ne-tuned model, and\nwe found behavior similar to RoBERTa. Figure 4\nshows the average of major tokens in the pretrained\nBERT and SQuAD-\ufb01ne-tuned BERT.\nB\nDispersion of Attention Histograms\nComparing the attention histograms in the lower\nlayers and the higher layers in RoBERTa (exam-\nples shown in Figure 5a and 5b respectively), we\nfound that the higher layers have more cumulative\nhistograms \u201cdispersed\u201d along the x-axis. Together\nwith the increasing variance of the number of ma-\njor tokens in the last two layers shown in Figure 4,\nsuch a distribution pattern evidently expresses the\ngreatly dissimilar sparsity among all the \u03b1i in the\nhead. As a quantitative analysis, we de\ufb01ne the\ndispersion of the \u03b1i distribution in a head as the\nstandard deviation of the index of the cumulative\nhistogram bin reaching 0.5. The dispersion ex-\npresses the dissimilarity of the \u03b1i histogram. Note\nthat this is different from the standard deviation\nshown in Figure 4, as the dispersion is measuring\nthe histograms of the attention, but not the attention\nvalues themselves.\nWe measure the dispersion at each head along\nthe layers for both pretrained and \ufb01ne-tuned\nRoBERTa models. Figure 5c illustrates the changes\nin dispersion along the layers in the RoBERTa\nmodels. In pretrained RoBERTa and its SQuAD-\n\ufb01ne-tuned version, the deep layers generally have\nhigher dispersion. The difference between these\ntwo models is mainly in layer 11, where the pre-\ntrained model has a dispersion drop. RoBERTa\n\ufb01ne-tuned for SST-2 does not show this trend. On\nthe BERT models, dispersion rarely increases along\nthe layers (shown in Figure 5d). The last layers\nhave been proved to be task-speci\ufb01c (Wu et al.,\n2020; Rogers et al., 2020), and their attention can\nlargely change after \ufb01ne-tuning (Kovaleva et al.,\n2019). This potentially explains why we observed\ndifferent dispersion behavior on different tasks, but\nneeds further investigation.\nC\nHeads with Outlier Attention\nDistribution\nOn some heads, a small portion of the tokens forms\nan attention histogram cluster separate from the\nmajority, clearly showing a dissimilarity between\nthese two types of distributions. For example, in\nFigure 1b, we observe a small number of tokens\nclustered on the right of the majority, between\n[10\u22124, 10\u22122]. Here we list all the heads with such\npattern:\n\u2022 Pretrained RoBERTa: Layer 1: head 8, head\n10, head 12; Layer 2: head 3, head 5, head\n10; Layer 3: head 2, head 10; Layer 4: head\n4, head 9; Layer 5: head 2, head 7, head 10;\nLayer 6: head 5, head 11, head 12; Layer 7:\nhead 3; Layer 8: head 7\n\u2022 Pretrained BERT: Layer 3: head 10; Layer 5:\nhead 5\nWe found that on these heads, the functional\nwords/tokens and punctuation exhibit distributions\nthat are signi\ufb01cantly different from other tokens.\nFor example, tokens such as <s>, </s>, and,\n layer 1\nlayer 2\nlayer 3\nlayer 4\nlayer 5\nlayer 6\nlayer 7\nlayer 8\nlayer 9\nlayer 10\nlayer 11\nlayer 12\n0\n20\n40\n60\n80\naverage #tokens \nto majority\nSQuAD RoBERTa\nPretrained RoBERTa\n(a) RoBERTa\nlayer 1\nlayer 2\nlayer 3\nlayer 4\nlayer 5\nlayer 6\nlayer 7\nlayer 8\nlayer 9\nlayer 10\nlayer 11\nlayer 12\n0\n20\n40\n60\n80\naverage #tokens \nto majority\nSQuAD BERT\nPretrained BERT\n(b) BERT\nFigure 4: Mean and standard deviation of the number of tokens\u2019 attentions needed to cover a majority (i.e. sum\nto 0.5) of attention densities in both pretrained and SQuAD-\ufb01ne-tuned RoBERTa/BERT models. Different layers\nare distinguished by different colors. In each layer the error bar represents the mean and std of head 1, head 2, ... ,\nhead 12 from the left to the right respectively.\n(a) Layer 1 head 1\n(b) Layer 12 head 1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12\nlayer\n2\n3\n4\n5\n6\n7\naverage diversity \nof all heads\nRoBERTa\nRoBERTa-SQuAD\nRoBERTa-SST2\n(c) Average dispersion of attention per layer in\nRoBERTa\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12\nlayer\n0\n1\n2\n3\n4\n5\naverage diversity \nof all heads\nBERT\nBERT-SQuAD\n(d) Average dispersion of attention per layer in\nBERT\nFigure 5: Attention distribution dispersion in different layers. Pretrained RoBERTa has more spread attention\ndistributions in layer 12 than in layer 1. In (c), the pretrained and SQuAD-\ufb01ne-tuned RoBERTa models exhibit\nincreasing dispersion in deeper layers, while RoBERTa \ufb01ne-tuned for SST-2 does not show such a trend.\n: and . are outliers in the pretrained RoBERTa\nmodel and [SEP] and [CLS] are outliers in the\npretrained BERT model. We also noticed these\ntokens\u2019 attention histograms could gather together\nlike the majority of the tokens do, to form either\na less sparse histogram cluster or more sparse his-\ntogram cluster, implying that on some heads, the\nfunctional words/tokens must be treated differently\n (a) Layer 2 head 3, <s>, le, and, : and </s>\nform a weak, less sparse cluster.\n(b) Layer 4 head 4, <s> and . form a weak, more\nsparse cluster\nFigure 6: A small portion of the tokens cluster outside\nof the majority of the attention\u2019s cumulative histogram\nin RoBERTa. Such tokens are noted in different colors\nwith their token strings (<s> and </s> are the \u201cstart\nof instance\u201d and \u201cend of instance\u201d tokens, respectively),\nwhile other tokens are in black as dashed lines.\nfrom the other tokens when exploring ef\ufb01ciency by\nutilizing sparsity. In Figure 6, we illustrate the at-\ntention histogram of such tokens. Our observation\ncon\ufb01rms that the special tokens and punctuation\ncan be heavily attended (Voita et al., 2018; Clark\net al., 2019; Kovaleva et al., 2019; Rogers et al.,\n2020). As a complement, we observed that it does\nnot necessarily mean that the special tokens\u2019 at-\ntention are always more sparse than other tokens\u2019\nattention.\nD\nQuantization with Pruned Attention\nfor SA and MLM\nWe provide the performance of different quantiza-\ntion methods with and without attention pruning\non the BERT and RoBERTa models tested on SA\nand MLM in Figure 7.\n16\n50\n60\n70\n80\n90\n2\n4\n6\n8\noriginal\n#bits\naccuracy\nRoBERTa-linear\nRoBERTa-linear-pruned\nRoBERTa-log\nRoBERTa-log-pruned\nRoBERTa-boolean\n(a) sentiment analysis\n16\n101\n102\n103\n104\n105\norigi\nBER\n2\n4\n6\n8\noriginal\nBERT\n#bits\npseudo-perplexity\nRoBERTa-linear\nRoBERTa-linear-pruned\nRoBERTa-log\nRoBERTa-log-pruned\nBERT-linear\nBERT-linear-pruned\nBERT-log\nBERT-log-pruned\n(b) masked language modeling\nFigure 7:\nPerformance of the quantized models\nwith and without pruning in advance for BERT and\nRoBERTa models on SA and MLM tasks.\nE\nQuantization Methods and Their\nEffectiveness\nQuantization methods.\nIn Section 3, we imple-\nmented two different quantization methods. Algo-\nrithms 1 and 2 list their pseudo code.\nQuantization and attention distribution\nBhan-\ndare et al. (2019) suggested analyzing the distri-\nbution to improve the quantization-effort-intensive\nfunctions like softmax (which generates the atten-\ntion values). Based on this, we assume that the\ntransformer model will perform better if its quan-\ntized attention values are distributed similarly to\nthe unquantized distribution. By measuring the\naverage Jensen-Shannon divergence between the\noriginal \u03b1i histogram and its quantized version, we\nfound that the logarithmic quantization has lower\ndivergence from the original attention distribution\ncompared to the linear quantization (see Table 1).\nWhile in our quantization experiment, the loga-\nrithmic quantization indeed achieves higher perfor-\n Algorithm 1: Linear quantization\ninput :att\u2190attention values;\nk\u2190number of bits used for quantization;\nt\u2190pruning threshold\noutput :res\u2190quantized attention values\nquantile size = (1 \u2212 t)/2k;\nset quantized value as middle point of quantile:\nquantile size/2;\nres=\ufb02oor(att / quantile size) * quantile size +\nquantized value + t;\nset attention values less than quantile size+t as zeros;\nAlgorithm 2: Log quantization\ninput :att\u2190attention values;\nk\u2190number of bits used for quantization;\nt\u2190pruning threshold\noutput :res\u2190quantized attention values\nwhen not pruning att, choosing a small value 10\u221210\nfor t;\nif pruning att then\nquantile size = (0 \u2212 log(t))/(2k \u2212 1);\nelse\nquantile size = (0 \u2212 log(t))/(2k)\nset quantized value as middle point of quantile:\nquantile size/2;\ncompute exponent of res: exp res=\ufb02oor((log(att) \u2212\nlog(t))/quantile size)*quantile size+quantized value+t;\nres=power(2, exp res);\nset values less than the \ufb01rst quantile boundry in the\nres as zeros;\nmance than the linear quantization on most num-\nbers of bits. This result indicates that selecting\nthe quantization method with less divergence from\nthe original attention distribution could improve\nthe performance. However, the lower divergence\nbetween the quantized and original attention dis-\ntribution does not necessarily relate to the model\nperformance once we introduce pruning. In Ta-\nble 1, even though the histogram\u2019s divergence of\nthe pruned log quantization is higher than the un-\npruned one, pruning still helps get better results.\nWe hypothesize that the pruning enlarged the dis-\nsimilarity between the attention histograms, but\nsuch a change did not affect the accuracy since it\nonly happened to the near-zero attention values.\nF\nLimited Accuracy Change on the\nLinear Quantization with/without\nPruning\nIn Figure 3a we observed similar performance of\nthe linear quantized attention models before and\nafter pruning. It is worth noting that the pruning\nthreshold we selected, \u03b1 < 10\u22123, is already a tiny\nvalue on the linear scale with respect to the range\nquantization method\npruned\nun-pruned\nlinear\n0.67\n0.67\nlog\n0.58\n0.55\nTable 1: Average Jensen-Shannon divergence between\nthe histogram of original \u03b1i and its 3-bit quantized val-\nues, evaluated on 100 samples from SQuAD Dev-1.1.\nLog quantization, which has lower divergence from the\noriginal attention distribution, retains more accuracy\nfrom the original model.\nof the attention values [0, 1]. As a result, pruning\nwill not signi\ufb01cantly narrow the quantization range,\nas it does for the log-scale quantization. Thus the\nlinear quantization has nearly the same effective\nquantized range with or without pruning, making it\nnearly impossible for the pruned linear quantized\nmodel to outperform the un-pruned one. This can\nbe veri\ufb01ed by the fact that the Jensen-Shannon\nDivergence of the linear quantized attention and\nthe original attention\u2019s histogram are the same with\nor without pruning in Table 1.\nG\nExperiment reproducibility\nAll evaluation is done on a server with the follow-\ning speci\ufb01cations:\n\u2022 CPU: Intel(R) Xeon(R) Silver 4216, 64 cores\n\u2022 GPU: Quadro RTX 8000\n\u2022 RAM: 377GB\nThe average runtime of the model inferences\nthrough the entire dataset is \u223c4 hours, for differ-\nent tasks. All datasets used in our experiment are\nbased on English. The SQuAD tests are evaluated\non 10570 sentences from the SQuAD Dev-v1.1\ndataset. The SST2 tests are evaluated on 872 in-\nstances from the GLUE validation dataset. The\nMasked Language Modeling tests are evaluated\non 480 paragraphs from the wikipedia training set,\neach having one random, unrepeated token masked\nfor 15\u201325 iterations.\n"}, "Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey": {"authors": ["Lovre Torbarina", "Tin Ferkovic", "Lukasz Roguski", "Velimir Mihelcic", "Bruno Sarlija", "Zeljko Kraljevic"], "title": "Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey", "url": "https://arxiv.org/pdf/2308.08234.pdf", "abstract": "The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it systematically analyses how transformer-based MTL in NLP fits into ML lifecycle phases. Furthermore, we motivate research on the connection between MTL and continual learning (CL), as this area remains unexplored. We believe it would be practical to have a model that can handle both MTL and CL, as this would make it easier to periodically re-train the model, update it due to distribution shifts, and add new capabilities to meet real-world requirements.", "arxiv_id": "2308.08234", "published_date": "2023-08-16", "year": 2023, "introduction": "INTRODUCTION In recent years, advancements in natural language processing (NLP) have revolutionized the way we deal with complex language problems. Consequently, those advances have been significantly impacting the global industry, driving growth in organizations that incorporate AI technologies as part of their core business. To illustrate, McKinsey\u2019s state of AI report for 2022 has reported an increase of 3.8 times since 2017 in AI capabilities that organizations have embedded within at least one function or business unit, where natural language understanding (NLU) took third place among reported capabilities, just behind computer vision (Chui et al., 2022). Furthermore, Fortune Business Insights projected growth of global NLP from USD 20.80 billion in 2021 to USD 161.81 billion by 2029.1 As a result, each NLP practitioner offering models through an API or using them internally, alone, or together with other AI capabilities, must have a machine learning (ML) system to efficiently manage these models. This involves having well-established processes from training and verifying those models to deploying them in production for end-users while continuously monitoring that those models remain up-to-date with the most recent knowledge they are being trained on. This trend of widespread adoption of ML models by various practitioners throughout industries, and the resulting need for ML systems to manage them efficiently, was tackled in a survey of ML systems conducted by Paleyes et al. (2022). The survey analyzed publications and blog posts reported by different practitioners, providing insights into phases of an ML lifecycle and challenges that commonly arise during those phases. The ML lifecycle refers to the phases and processes involved in designing, developing, and deploying an ML system. It encompasses the entire process, \u2217 Equal contribution. \u2020 Correspondence to: Lovre Torbarina <lovre.torbarina@doxray.com> 1Fortune Business Insights - NLP Market Size 1 arXiv:2308.08234v1  [cs.CL]  16 Aug 2023 Preprint. from defining the problem and collecting data to deploying models and monitoring their performance. Model learning and model deployment are two important phases in the ML lifecycle, among others (Ashmore et al., 2021; Paleyes et al., 2022). To support practitioners\u2019 needs, the model learning phase should be equipped to handle the training and updating of a large number of models, while the model deployment phase must provide an easy and efficient way to integrate and serve those models to run in production, that is, running as part of usual business operations. At the same time, it is a common practice in NLP production systems to utilize pre-trained language models based on transformers (Vaswani et al., 2017) by fine-tuning them for specific downstream tasks. While effective, the language models have a large number of parameters that require significant computational resources to fine-tune. Although finetuning pre-trained models can be more data-efficient than training a model from scratch, the expertise of annotators or domain experts may still be required to label a large number of examples, particularly if there is a significant difference between the downstream task and pre-training objectives (Wang et al., 2020). Therefore, it is a costly and time-consuming procedure, especially if there is a need to train and serve multiple models in production. To address the challenge of training multiple models, researchers have been exploring Multi-Task Learning (MTL) as a solution (Ruder, 2017). MTL trains a single model to learn multiple tasks simultaneously while sharing part of the model parameters between them (Caruana, 1997), making the process memory-efficient and, in some cases, computationally more efficient than training multiple models. Additionally, using a single model for multiple downstream tasks in a production system could simplify the integration of ML models with ML systems and reduce economic costs. This is due to the modular nature of MTL architectures that promote code and model sharing, reuse, easier collaboration, and maintenance. Moreover, the MTL model reduces idle time since the same model is used for various tasks. Therefore, MTL approaches offer a promising solution to mitigate some of the difficulties associated with managing multiple models in ML production systems. In this survey, we first provide an overview of transformer-based MTL approaches in NLP (see Section 3). Second, we highlight the opportunities for using MTL approaches across multiple stages of the ML lifecycle, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring (see Section 4). We focus solely on transformer-based architectures. To the best of our knowledge, this is the first survey that systematically discusses the benefits of using MTL approaches across multiple ML lifecycle phases (see Section 2). Additionally, we encourage further research on the connection between MTL and Continual Learning (CL). We argue that having a model capable of handling both MTL and CL is practical as it addresses the need for periodic re-training and continual updates in response to distribution shifts and the addition of new capabilities in production models. The rest of the paper is organized as follows. In Section 2, we briefly review related surveys and highlight the gaps addressed in our survey. In Section 3, we give an overview of transformer-based MTL approaches. In Section 4, we systematically analyze the benefits of using MTL through specific ML lifecycle phases. And finally, in Section 5, we give a conclusion to our work. 2 RELATED SURVEYS In this section, we give an overview of related work on MTL, ML systems, and CL, and point out what has not been discussed so far regarding the connection of MTL to both ML systems and CL. 2.1 MULTI-TASK LEARNING The idea of MTL was explored in many research studies. In this section, we provide an overview of related MTL surveys, address various aspects of MTL, and list them along with their corresponding surveys in Table 1. 2 In the rest of the section, we just mention related surveys and go over individual MTL aspects (shown in bold) in more detail.3 Many application domains were studied in previous work, ranging from surveys covering multiple domains (Ruder, 2017; Zhang & Yang, 2017; 2018; Thung & Wee, 2018; Vafaeikia et al., 2020; Crawshaw, 2020; Upadhyay et al., 2021; Abhadiomhen et al., 2022), to those dedicated to a specific domain, such as computer vision (Vandenhende et al., 2021) or natural language processing (Zhou, 2019; Worsham & Kalita, 2020; Chen et al., 2021; Samant et al., 2022; Zhang et al., 2023). Both traditional ML and deep learning computational models were studied. The traditional ML was discussed primarily in older studies, while deep learning was presented in all but one study. MTL architectures were widely discussed in previous work. Hard and soft parameter sharing (Ruder, 2017) was the most used architecture taxonomy, but recent surveys refined the taxonomies for more precise categorization (Crawshaw, 2020; Chen et al., 2021). Next, some MTL architectures were categorized as learning-to-share (Ruder et al., 2The broader version of the table is provided in Appendix Table 3. 3The broader version of the section is provided in Appendix A.1. 2 Preprint. Table 1: Discussed aspects per MTL survey. Aspects are indicated in bold. Year MTL Survey 2017 1 - (Ruder, 2017) 2 - (Zhang & Yang, 2017) 2018 3 - (Zhang & Yang, 2018) 4 - (Thung & Wee, 2018) 2019 5 - (Zhou, 2019) 2020 6 - (Vafaeikia et al., 2020) 7 - (Worsham & Kalita, 2020) 8 - (Crawshaw, 2020) 2021 9 - (Vandenhende et al., 2021) 10 - (Chen et al., 2021) 11 - (Upadhyay et al., 2021) 2022 12 - (Samant et al., 2022) 13 - (Abhadiomhen et al., 2022) 2023 14 - (Zhang et al., 2023) Aspect \\Survey 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Computational Model Traditional ML \u2713 \u2713 \u2713 \u2713 \u2713 Deep Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Architectures Learning to Share \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Universal Models \u2713 \u2713 \u2713 \u2713 \u2713 Optimization Loss Weighting \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Regularization \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Task Scheduling \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gradient Modulation \u2713 \u2713 \u2713 \u2713 \u2713 Knowledge Distillation \u2713 \u2713 \u2713 \u2713 Multi-Objective Optimization \u2713 \u2713 \u2713 Task Relationship Learning Task Grouping \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Relationships Transfer \u2713 \u2713 \u2713 \u2713 \u2713 Task Embeddings \u2713 \u2713 Connection to Learning Paradigm Reinforcement Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Transfer Learning \u2713 \u2713 \u2713 Meta-Learning \u2713 \u2713 \u2713 Online Learning \u2713 \u2713 \u2713 Continual Learning Application Domain Natural Language Processing \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Computer Vision \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 2017), which offers a more adaptive solution by learning how to share parameters between tasks, rather than having the sharing defined a priori. Additionally, some MTL architectures were categorized as universal models, handling multiple modalities, domains, and tasks using a single model (Kaiser et al., 2017; Pramanik et al., 2019). Optimization techniques for MTL architectures were also widely discussed, while loss weighting was the most common approach to mitigate MTL challenges. Techniques include loss weighting by uncertainty (Kendall et al., 2018), learning speed (Liu et al., 2019a; Zheng et al., 2019), or performance (Guo et al., 2018; Jean et al., 2019), among others. Next, and closely related to weighting task losses, is a task scheduling problem that involves choosing tasks to train on at each step. Many techniques were used, from simple ones that employ uniform or proportional task sampling, to the more complicated ones, such as annealed sampling (Stickland & Murray, 2019) or approaches based on active learning (Pilault et al., 2020). Finally, regularization approaches (Long et al., 2017; Lee et al., 2018; Pascal et al., 2021), gradient modulation (Lopez-Paz & Ranzato, 2017; Sinha et al., 2018), knowledge distillation (Clark et al., 2019b), and multi-objective optimization (Lin et al., 2019) were also applied to optimize MTL models. Task relationship learning in MTL focuses on learning explicit representations of tasks or relationships between them, and typically three categories of methods were used. First, task grouping aims to divide a set of tasks into groups to maximize knowledge sharing during joint training (Standley et al., 2020). Second, transfer relationship learning determines when transferring knowledge from one task to another will be beneficial for joint learning (Zamir et al., 2018). Finally, task embedding methods aim to learn task embedding space (Vu et al., 2020). Previous works made connections to other learning paradigms, including reinforcement learning, transfer learning, meta-learning, active and online learning. To the best of our knowledge, there had been no prior work systematically 3 Preprint. investigating connections between MTL and CL. We believe that a connection between MTL and CL represents a promising research direction, as we will motivate the need for this connection in Section 4.4. 2.2 ML LIFECYCLE AND ML SYSTEMS The unparalleled growth of advancements in ML methods in recent years, with applications in NLP, computer vision, and others, has increased the complexity of building ML systems that need to address the requirements of an ML lifecycle. Previous works reviewed the challenges of such systems, often defining ML lifecycle phases such as data management, model learning, and model deployment, to study systematically the workings of ML systems and identify challenges within and across phases (Vartak & Madden, 2018; Ashmore et al., 2021; Paleyes et al., 2022; Huyen, 2022). For example, Vartak & Madden (2018) defined ML lifecycle phases and analyzed model management challenges. Furthermore, Ashmore et al. (2021) discussed assurance of ML for each phase, while Paleyes et al. (2022) reviewed practitioners\u2019 challenges at each phase of an ML model deployment workflow in a broader scope than previous surveys. In the rest of this section, we provide a few examples of challenges that occur during typical phases of the ML lifecycle. There are many challenges that occur during different phases of the ML lifecycle. For example, data management is typically the early phase of the ML lifecycle with associated challenges such as data collection and preprocessing (Polyzotis et al., 2018; Sambasivan et al., 2021; Whang et al., 2023). Subsequently, model learning and verification phases take place, presenting challenges such as selecting (Ding et al., 2018) and training (Sun, 2020) a model, and determining the most effective method for verifying it (Bernardi et al., 2019; Schr\u00a8oder & Schulz, 2022), respectively. Then, a model deployment phase takes place with challenges such as model integration (Sculley et al., 2015; Renggli et al., 2019) into production. Finally, the monitoring phase with challenges such as continuous model performance monitoring (Schr\u00a8oder & Schulz, 2022) and updating a model over time (Ditzler et al., 2015; Abdelkader, 2020). Some challenges can impact several phases of the ML lifecycle, such as collaboration among diverse teams and roles, including software and data engineers, data scientists, and other stakeholders (Takeuchi & Yamamoto, 2020; Nahar et al., 2022; Pei et al., 2022; Yang et al., 2022). Furthermore, there are challenges of bias, fairness, and accountability in ethics (Mehrabi et al., 2021; Kim & Doshi-Velez, 2021), various regulations set by law (Marchant, 2011; Politou et al., 2018) and adversarial attacks in security (Ren et al., 2020; Rosenberg et al., 2021), among others. Previous works addressed MTL aspects to varying degrees in specific phases, either in a straightforward manner or indirectly. However, a systematic discussion of the potential benefits of using MTL approaches to alleviate challenges across different phases of the ML lifecycle has not been conducted. 2.3 CONTINUAL LEARNING CL incrementally learns a sequence of tasks, with a goal to progressively expand acquired knowledge and utilize it for subsequent learning (Chen & Liu, 2018). CL aims to overcome catastrophic forgetting (CF) and facilitate knowledge transfer (KT) across tasks, where CF is the degradation of performance on previous tasks when learning new ones, and KT is the ability to apply knowledge from past tasks to new tasks (Ke & Liu, 2022). Previous reviews on CL (Hsu et al., 2018; De Lange et al., 2021) categorized CL settings based on the marginal output and input distributions P(Y (t)) and P(X(t)) of a task t, with P(X(t)) \u0338= P(X(t+1)). First, class incremental learning is characterized by an expanding output space with observed class labels such that Y (t) \u2282 Y (t+1) and P(Y (t)) \u0338= P(Y (t+1)). Second, task incremental learning (TIL), requires a task label t to identify the separate output nodes Y (t) for the current task t, where Y (t) \u0338= Y (t+1). Lastly, incremental domain learning defines tasks with equal class labels and probability distributions, Y (t) = Y (t+1), and P(Y (t)) = P(Y (t+1)). CL approaches were also categorized into three main categories based on how task-specific information is stored and utilized during the incremental learning process. First, replay methods store samples in raw format or generate pseudo-samples with a generative model, replaying them while learning a new task to mitigate forgetting and prevent previous task interference. Second, regularization-based methods, on the other hand, avoid storing raw inputs and reduce memory requirements by introducing an extra regularization term in the loss function to consolidate prior knowledge while learning new data. Third, parameter isolation methods allocate different model parameters to each task, either by adding new task-specific branches or masking out previous task parts, to prevent forgetting and maintain task-specific knowledge. We refer readers to Ke & Liu (2022) for more refined CL taxonomy and details in NLP. In CL, MTL was usually used as an upper-bound baseline which can use all of the data from all of the tasks simultaneously (De Lange et al., 2021; Ke & Liu, 2022). Since CL and MTL work in different learning settings, few works tried to connect the two paradigms. Sun et al. (2020) presented a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learns pre-trained models on these constructed tasks via contin4 Preprint. ual multi-task learning. Subsequently, ERNIE 2.0 was tested against CL and MTL pre-training approaches to evaluate the impact on abstractive text summarization task but performed similarly to other approaches (Kirstein et al., 2022). In Section 4.4, we motivate further research on combining CL and MTL approaches, as pre-training approaches are not sufficient for handling distribution shifts and adjusting models for new business requirements in real-world scenarios. 3 MULTI-TASK LEARNING APPROACHES 3.1 TAXONOMY There were various MTL taxonomies covered in the surveys presented in Section 2.1. Ruder (2017) distinguishes between a hard and soft parameter sharing, which proved to be an influential taxonomy, since it was used in later works as well. Zhang & Yang (2018) defines three categories of multi-task supervised learning \u2013 feature-, parameter-, and instance-based. Vandenhende et al. (2021) distinguishes between encoder-focused and decoder-focused architectures. Chen et al. (2021) discusses parallel, hierarchical, modular, and generative adversarial architectures. We categorized transformer-based MTL approaches into 3 main categories based on differences in architectures: (1) Fully-Shared Encoder, (2) Adapters, and (3) Hypernetworks (Figure 1).4 Figure 1: Simplified overview of MTL architectures. Sub-figure a) represents a fully-shared encoder, b) an adapter, and c) a hypernetwork. Blue components are trained jointly by all the tasks, green ones are task-specific, and grey ones are kept frozen. A dotted adapter component suggests possible adapter insertion positions. 3.2 MTL APPROACHES OVERVIEW 3.2.1 FULLY-SHARED ENCODER A simple and intuitive approach to MTL is to have a clear division between shared and task-specific parameters. In such an approach, there is a shared transformer-based encoder in lower layers, while the top layers consist of different, task-specific layers (heads). One such approach, MT-DNN (Liu et al., 2019b), batches all the GLUE tasks (Wang et al., 2018) together and updates the model accordingly. The shared encoder is updated for all the instances, while the task-specific heads are updated only for the instances of a task that they are specific for. There are several downsides to this MT-DNN approach. First, task interference is not taken into account and the authors simply hope that the tasks would interact well, although some of them are in different domains. Next, proportional random sampling is used, which could lead to underfitting on low-resource datasets. Finally, the loss is calculated in three different ways (for classification, regression, and ranking), and as a result, it has different scales. Nevertheless, all the loss functions are weighted equally. Notwithstanding these observations, their model outperforms fine-tuning a different BERT (Devlin et al., 2019) model on most of the tasks. Additionally, they tried fine-tuning this multi-task model further on each task separately after training jointly on all the tasks, producing N models for N tasks. That again gave an improvement and a state-of-the-art performance at the time. However, an obvious downside is having a different model for each task. Another shared-encoder approach is pre-finetuning. Muppet (Aghajanyan et al., 2021) shared an encoder in an MTL on 46 diverse datasets. Heterogeneous batches have proved beneficial in handling noisy gradients from different tasks. Furthermore, to have stable training, the data-point loss was divided by log(n), where n denotes the cardinality of the label set for the associated task. They maintained a natural distribution of datasets as other approaches led 4In Appendix B.1, we give a brief overview of prompt engineering approaches as a fourth category. However, we do not include it in the main paper due to its need for a large number of parameters to perform well, making it inaccessible to most practitioners. 5 Preprint. to degraded performance. The authors found a threshold of around 15 tasks, below which downstream fine-tuning performance is degraded, and above which the performance improves linearly in the number of pre-finetuning tasks. A similar approach with added decoder, EXT5 (Aribandi et al., 2021), extended the mixture to 107 supervised tasks, formatted them for encoder-decoder architectures, and performed pre-finetuning along with unsupervised T5\u2019s C4 span denoising (Raffel et al., 2020). Their mixture of tasks also included NLP applications such as reading comprehension, closed-book question answering, commonsense reasoning, dialogue, and summarization, among others. This showed that encoder-decoder models like T5 are capable of solving a wider range of NLP applications than encoder ones. However, task-specific models still achieve a better performance than general ones (Chung et al., 2022). 3.2.2 ADAPTERS Before being used in NLP, adapter residual modules were first introduced for the visual domain (Rebuffi et al., 2017). Adapters are small, task-specific modules that are typically inserted within network layers, but can also be injected in parallel to them. In this survey, the network is always a transformer-based architecture. Compared to transformers\u2019 sizes, they add a negligible number of parameters per task. The parameters of the original network remain frozen unless stated otherwise, resulting in a high degree of parameter sharing and a small number of trainable parameters. Consequently, adapters for new tasks can be easily added without retraining the transformer or other adapters. They learn task-specific layer-wise representation, are small, scalable, and shareable, have modular representations and a non-interfering composition of information (Pfeiffer et al., 2020b). Since the respective adapters were trained separately, the necessity of sampling heuristics due to skewed data set sizes no longer arises (Pfeiffer et al., 2020b). AdapterHub. AdapterHub (Pfeiffer et al., 2020b) is a framework that allows dynamic usage of pre-trained adapters for different tasks and languages.5 The framework is built on top of the HuggingFace Transformers library and enables quick and easy adaptation of state-of-the-art pre-trained models. It allows for efficient parameter sharing between tasks by training many task- and language-specific adapters, which can be exchanged and combined post-hoc. One can choose to stack the adapters on top of each other, combine them with attention (Pfeiffer et al., 2020a), or replace them dynamically. Downloading, sharing, and training adapters require minimal changes in the training scripts. Bottleneck adapters. In AdapterHub\u2019s documentation, three different bottleneck adapter approaches were mentioned. Adapters can be inserted after both Multi-Head Attention (MHA) and Feed-Forward (FF) block (Houlsby et al., 2019), only after the FF block (Pfeiffer et al., 2020c), or in parallel to the Transformer layers (He et al., 2021). Bottleneck adapters consist of a down-projection, non-linearity (typically ReLU), and an up-projection back to the original size. Residual connection is used, and layer normalization is applied afterward. Language adapters. In the MAD-X framework (Pfeiffer et al., 2020c), the authors train (1) language adapters via masked language modeling (MLM) on unlabelled target language data, and (2) task adapters by optimizing a target task on labeled data in a source language with the most training data. Then, adapters are stacked, allowing for a zero-shot cross-lingual transfer by substituting the target language adapter at inference. Invertible adapters are introduced to tackle the mismatch between the pre-trained model\u2019s multilingual vocabulary and target language vocabulary. Consequently, language adapters could be useful when one had already trained a task adapter for a specific task and now needs to perform inference for the same task, but on new data from a different language. Other. Projected Attention Layer (PAL) (Stickland & Murray, 2019) is a low-dimensional multi-head attention layer added in parallel to the transformer layers. Multi-head attention is applied on a down-projected input, after which an up-projection to an original dimension is applied. These down- and up-projection matrices are shared among layers, but not among tasks. The authors fine-tune a pre-trained encoder along the PALs. This has downsides: (1) forgetting of pre-trained knowledge is possible, (2) access to all the tasks at training time is required, and (3) adding new tasks requires complete joint retraining. Thus, this approach misses many of the characteristics of an adapter. AdapterFusion (Pfeiffer et al., 2020a) introduces a knowledge composition phase, in which the previously trained adapters are combined. This approach uses multiple adapters to maximize knowledge transfer between tasks without suffering from the MTL drawbacks, such as catastrophic forgetting (Serra et al., 2018) or task interference (Wu et al., 2020). It introduces a new set of weights that learn to combine the adapters as a dynamic function of the target task data by using attention. This shows its greatest downside \u2013 AdapterFusion is trained for one task only. Hu et al. (2021) argue that the original adapter bottleneck design (Houlsby et al., 2019) introduces inference latency because the adapters are processed sequentially, whereas large language models (LLMs) rely on hardware parallelism. Their approach, LoRA (Low Rank Approximation) modifies attention weights of query and value projection matrices by introducing trainable low-rank decomposition matrices in parallel to the original computation. This reduces inference latency, as the decomposition matrices can be merged with the pre-trained weights for faster inference. 5https://adapterhub.ml 6 Preprint. Figure 2: ML lifecycle phases. Image is taken from Huyen (2022). Table 2: ML Lifecycle phases and corresponding challenges. ML Lifecycle Phase Challenges Project Scoping Initial Requirements Data Engineering Labeling of Large Volumes of Data Cost of Annotators and Experts Lack of High-Variance Data Model Development Model Complexity Resource-Constrained Environments Computational Cost Environmental Impact Deployment Ease of integration Monitoring Distribution Shift Business Analysis New Requirements 3.2.3 HYPERNETWORKS A hypernetwork is a network that generates weights of another network (Ha et al., 2016). This approach can alleviate a downside of adapters, which is a lack of knowledge sharing. Hypernetwork allows sharing of knowledge across tasks while adapting to individual tasks through task-specific parameter generation. CA-MTL (Pilault et al., 2020) modularizes a pre-trained network by either adding task-conditioned layers or changing the pre-trained weights using the task embedding. Their task-conditioned transformer-based network has four components: (1) conditional attention, (2) conditional alignment, (3) conditional layer normalization, and (4) conditional bottleneck. In (1), they use block-diagonal conditional attention which allows the attention to account for task-specific biases. Component (2) aligns the data of diverse tasks. In (3), they adjust layer normalization statistics for specific tasks. Finally, (4) facilitates weight sharing and boosts task-specific information flow from lower layers. In addition, they use multi-task uncertainty sampling. This favors tasks with the highest uncertainty by sampling a task whenever its entropy increases, helping to avoid catastrophic forgetting. When introducing a new task, they claim that only a new linear decoder head and a new task embedding vector need to be added to re-modulate existing weights. HyperFormer++ (Mahabadi et al., 2021) uses hypernetworks to generate the weights of adapter and layer normalization parameters. These hypernetworks condition on a task embedding, adapter position (after MHA or FF sub-layer), and layer id within the T5 model (Raffel et al., 2020). During training, they sample the tasks using temperature-based sampling. They state that for each new task, their model only requires learning an additional task embedding. HyperGrid (Tay et al., 2020) leverages a grid-wise decomposable hyper projection structure which helps specialize regions in weight matrices for different tasks. To construct the proposed hypernetwork, their method learns the interactions and composition between a global, task-agnostic state and a local, task-specific state. They equip position-wise FF sub-layers of a Transformer with HyperGrid. They initialize a T5 model from a pre-trained checkpoint and add additional parameters that are fine-tuned along the rest of the network. The authors of the paper did not mention anything specific regarding the ability to add a new task without re-training, as it appears to be non-trivial. 4 MTL FROM ML LIFECYCLE POINT OF VIEW In this section, we discuss the challenges and opportunities of incorporating into ML production systems MTL approaches instead of using multiple single-task counterparts. Motivated by previous reviews on ML lifecycle (see Section 2.2), we define ML lifecycle phases to discuss challenges and opportunities in a systematic manner. Following Huyen (2022), we define six ML lifecycle phases: (1) Project Scoping, (2) Data Engineering, (3) Model Development, (4) Deployment, (5) Monitoring, and (6) Bussiness Analysis (Figure 2). Next, we mostly focus on challenges that were discussed in a broader scope in Paleyes et al. (2022), while we argue how MTL can alleviate them. Challenges per each phase of the ML lifecycle are listed in Table 2. When discussing the challenges and opportunities of using MTL approaches, we compare them to corresponding single-task model solutions. In the rest of the section, we first discuss data engineering and model development phases in isolation. Then, we discuss an ML model updating problem by indicating how certain aspects of the problem pose different challenges in different phases of the ML lifecycle. 7 Preprint. 4.1 DATA ENGINEERING The first phase we discuss is data engineering. This phase focuses on preparing data that is needed to train a machine learning model, while we have a particular interest in challenges related to the lack of labeled data (see Table 2). Challenges. The need for data augmentation can arise from various factors, with one of the most problematic being the lack of labels in the data, especially in real-world applications where labeled data may be scarce. It is a common practice in NLP production systems to utilize pre-trained transformer-based language models by fine-tuning them for specific downstream tasks. However, if there is a significant gap between a downstream task and the pre-training objectives, a larger amount of labeled data may still be required to achieve the target performance (Wang et al., 2020). Obtaining this data involves costly and time-consuming involvement of annotators and domain experts. Additionally, the absence of high-variance data results in a model that is unable to generalize well, such as adapting language models to low-resource languages (Clark et al., 2019a). Opportunities. The challenges posed by the lack of labeled data can be alleviated using MTL approaches. For example, if a set of single-task models is being used in a production system, training an MTL model instead can help alleviate data sparsity by jointly learning to solve related tasks (Section 3.2.1). The benefits of MTL have been previously discussed in Caruana (1997); Ruder (2017), including its ability to increase data-efficiency. First, different tasks transfer different knowledge aspects to each other, enhancing the representation\u2019s ability to express the input text, which can be beneficial for tasks with low-resource datasets (Section 3.2.1). However, some MTL approaches may underperform in resource-constrained environments due to inadequate optimization choices (Section 3.2.1). Additionally, the presence of different noise patterns in each task acts as an implicit data augmentation method, effectively increasing the sample size used for training, and leading to a robust model with more general representations (Ruder, 2017). Finally, using pre-finetuning (Section 3.2.1) could reduce convergence time, saving computational resources. 4.2 MODEL DEVELOPMENT In the model development phase, we focus on two groups of challenges. The first group refers to the model selection problem, including issues related to model complexity and resource constraints. The second group of challenges is related to problems in model training, such as the computational cost of the training procedure and its impact on the environment. We refer readers to (Gupta & Agrawal, 2022) for an overview of methods for efficient models in text. Model Selection Challenges. When selecting a model to handle tasks that end-users are interested in, practitioners often face a dilemma regarding the trade-off between model complexity and performance. Typically, complex models have better performance, but they come with the risk of over-complicating the design in the first place, leading to a longer development time and deployment failure (Haldar et al., 2019). Furthermore, they may not be practical to use in resource-constrained environments where they require high computational and memory resources. Model Selection Opportunities. MTL architectures, presented in Section 3.2, have properties that can alleviate challenges related to the trade-off between model complexity and performance. For example, consider replacing N single-task models with a single MTL shared encoder model. The MTL model will have close to N times smaller memory footprint, as the number of task-specific parameters is negligible compared to the number of shared parameters. This reduction results in a better fit to memory-constrained environments while only having slightly worse performance than the single-task counterparts. Similarly, saving N adapters or a single hypernetwork is much more memory-efficient than saving N single-task models. Model Training Challenges. The training of machine learning models presents several challenges that must be addressed by practitioners. One of the major challenges is the high economic cost associated with training, which is due to the computational resources required. In the field of natural language processing, the cost of model training continues to rise, even as the cost of individual floating-point operations decreases, due to factors such as the growth in training dataset size, number of model parameters, and number of operations involved in the training process (Sharir et al., 2020). The training process also has a significant impact on the environment, leading to increased energy consumption and greenhouse gas emissions (Strubell et al., 2020). These challenges emphasize the need to address the economic and environmental implications of training machine learning models. Model Training Opportunities. Computational resource challenges could be alleviated in some aspects by using MTL approaches to reduce the cost of model training. First, in certain cases, smaller dataset sizes can be used due to pre-finetuning or knowledge transfer between related tasks during joint training of the MTL model, leading to more data-efficient training. Second, the joint model is more parameter-efficient, resulting in a significant reduction in the number of parameters required for multiple single-task models. 8 Preprint. Selection, Training, and Inference Trade-Off. The number of floating-point operations is not reduced in some cases, and depends on the choice of MTL architecture and the nature of the tasks. Different task-specific heads require different inputs if tasks belong to different domains or have different input encodings. However, if tasks belong to the same domain and have the same input encoding, part of the computation can be shared among the task-specific heads. For example, in a fully-shared encoder (Section 3.2.1), the computation of the full encoder can be shared, while the computation in the task-specific heads is negligible. Similarly goes for adapters (Section 3.2.2) \u2013 the large majority of the computation is shared, and only the adapters are then dynamically plugged for performing different tasks on the same input. Namely, the benefit of using adapters is a small number of trainable parameters, thanks to a frozen encoder, which results in a faster gradient back-propagation. A forward pass, on the other hand, takes more time compared to an encoder with no adapters. Thus, when tasks don\u2019t share the datasets, using adapters results in a longer inference time compared to single-task counterparts. AdapterFusion inference is even slower, as an input must pass through all the available adapters. LoRA solves the inference latency problem by using parameter composition. 4.3 MODEL DEPLOYMENT In the model deployment phase, our focus is on simplifying the integration of trained ML models with existing ML systems that are running in production, with a particular emphasis on straightforward implementation, seamless collaboration, and ease of maintenance. Challenges. The first challenge in model deployment is preparing the developed model for use in a production environment. The initial versions of models are often developed by stakeholders (e.g., ML researchers) who are different from those responsible for deploying them into production (e.g., ML and DevOps engineers). This means that the model code needs to be adapted to meet the requirements of the operational production environment. Those requirements are typically more strict and different from those available during the development phase, so it is important to address operational aspects such as scalability, security, and reliability. Hence, each additional model adds complexity to the process, both for the stakeholders involved and for the ML system infrastructure and computational resources in place. Another challenge during model integration is incorporating the model into real-data processing pipelines in production, whether it is for batch offline processes or handling real-time user requests. During model training, the researcher often uses preprocessed and clean datasets. However, when integrating the model into production, it will be integrated into existing data pipelines. The more complex the model\u2019s input data requirements, or the more models that are used, the more complex the data processing pipelines will need to be. Adapting the models and existing data pipelines often requires collaboration between different stakeholders or teams responsible for different parts of the ML system and/or ML lifecycle phase. All of these factors directly impact the increasing costs of maintenance, operations, support, and infrastructure. Opportunities. MTL approach was presented by the team from Pinterest, where the authors trained a universal set of image embeddings for three different models, which simplified their deployment pipelines and improved performance on individual tasks (Zhai et al., 2019). The key benefit is that the use of MTL can reduce the number of parameters needed to support different models required to solve a problem. This leads to smaller task-specific models and less code to be adapted to the production environment, reducing the overhead of potential changes in existing data pipelines. Reusing data, code, and models can save time and simplify the model deployment process. Next, the modular design of MTL architectures makes it easier to work with by enabling code reuse. For example, the fully-shared encoder architecture (described in Section 3.2.1) reuses the encoder between multiple task-specific heads. Moreover, freezing the shared encoder would allow for working on separate tasks independently and simultaneously, making cross-team collaboration easier and more efficient. This idea was used in the HydraNet MTL model by the Tesla AI team (Karpathy, 2021). However, freezing an encoder and updating only the task-specific heads may decrease performance. Additionally, the concept behind the AdapterHub (described in Section 3.2.2) was designed to allow users to choose from a set of adapter modules, combine them in their preferred way, and insert or replace them dynamically into state-of-the-art pre-trained models. To conclude, the modular nature of MTL architectures has a positive impact on making the incorporation of these architectures into ML software easier, making it simpler to develop, collaborate, configure, and integrate into deployment processes. 4.4 MODEL UPDATING THROUGH MULTIPLE ML LIFECYCLE PHASES Often it is necessary to update the ML model regularly after it has been deployed and is running in production, in order to keep it aligned with the most current changes in data and environment. The need for updating models is one of the most important requirements of ML production systems (Pacheco et al., 2018; Abdelkader, 2020; Lakshmanan et al., 2020; Paleyes et al., 2022; Huyen, 2022; Wu & Xie, 2022; Nahar et al., 2022). In this section, we discuss the challenges of model updating and how MTL approaches can alleviate these challenges. Challenges of different 9 Preprint. phases of the ML lifecycle can trigger the need for model updates. First, we identify these occurrences and discuss the actions they trigger. Then, we discuss how MTL approaches can alleviate these challenges and point out the current limitations of these approaches. Model Updating Challenges. A distribution shift is one of the common reasons for the need to update models. Distribution shift refers to changes observed in the joint distribution of the input and output variables of an ML model (Ditzler et al., 2015). Two problems must be solved to address the distribution shift effectively. First, in the monitoring phase, a mechanism must be in place to detect changes in distribution or a drop in key performance indicators, which will signal the need for an ML model update. Second, in the model development phase, there must be a way to continuously learn and update the model in response to signals from the monitoring phase. New business requirements often arise, requiring the model to have new capabilities in addition to the existing ones. For example, a named entity recognition model trained for 10 entity labels in news articles may require 5 new labels after 3 months of use. New requirements are introduced during the business analysis phase, which also triggers the need for an ML model update. Unlike the initial requirements in the project scoping phase, adding new requirements should be possible without re-training the full model. Periodic or scheduled re-training and continual learning are the most common approaches for adapting models to new data and requirements. Periodic re-training refers to the process of re-training a model at a predetermined interval, regardless of whether there have been any changes to the data or the environment. The frequency of updating is determined beforehand and is typically based on the amount of data and the desired level of model performance. Striking a balance between updating the model regularly to maintain good performance and avoiding over-updating the model to minimize computational costs is crucial. It is important to note that different models may require different retraining schedules, making it a situation-dependent problem. Finding the optimal re-training schedule requires careful consideration of the specific requirements and circumstances of each ML system. Continual learning, on the other hand, refers to the ability of an ML model to adapt to changes in the data and environment over time. This approach involves continuously monitoring the performance of the model and updating it as needed to ensure it remains accurate and up-to-date. The frequency of model updates in continual learning is determined dynamically based on the changes observed in the data and environment. Model Updating Opportunities. Following Huyen (2022), we differentiate two types of model updates: data iteration and model iteration. Data iteration refers to updating the model with new data while keeping the model architecture and features the same, while model iteration refers to adding new features to an existing model architecture or changing the model architecture itself. To discuss the challenges and opportunities of MTL approaches for model updating, we consider two scenarios. First, periodic re-training is done every 6 or 12 months, with the goal of training each model to perform optimally using all available data. Due to the high economic cost, we assume this cannot be done more frequently. Second, between periodic retrains, there may be situations where data iteration is necessary due to a distribution shift, or model iteration is required to extend model capabilities in response to a new business requirement. We believe that incorporating MTL approaches into the update scenario would be practical, in addition to the benefits discussed in Sections 4.1-4.3. MTL is particularly well-suited for periodic re-training scenarios where the objective is to obtain a single best-performing model on all tasks using all available data, instead of training individual models for each task. However, the challenge lies in how MTL approaches can manage the second scenario where the MTL model must learn new tasks or domains sequentially over time. As a result, the same model must be able to learn in both MTL and CL settings as needed, and we refer to this setting as Continual MTL (CMTL). Continual MTL. MTL and CL models both learn multiple tasks, however, MTL learns them simultaneously, while CL learns them incrementally. As mentioned in Section 2.3, the authors of Sun et al. (2020) combined CL and MTL to further improve pre-training. Although the approach enhanced performance on downstream tasks, it does not address the real-world scenario in which an MTL model should be updated to support new downstream tasks or handle distribution shifts. Moreover, the total number of sequential tasks must be known a priori for the algorithm to determine an efficient training schedule. We believe that the similarities between MTL and CL architectures could enable the construction of a CMTL model. For example, most MTL architectures in Section 3.2 are transformer-based MTL architectures with task-specific heads, which resemble the parameter-isolation architectures for the TIL setting in CL (Section 2.3). In TIL, the task identifier is available during both training and testing and is used to identify task-specific parameters in multi-headed architectures, similar to MTL architectures (Ke & Liu, 2022). This similarity can be seen in the use of adapter architectures (Section 3.2.2) in both MTL (Stickland & Murray, 2019; Pfeiffer et al., 2020c; He et al., 2021) and CL (Ke et al., 2021a;b), as well as in the use of hypernetworks (Section 3.2.3) in both MTL (Pilault et al., 2020; Mahabadi et al., 2021) and CL (Von Oswald et al., 2019; Jin et al., 2021). 10 Preprint. Figure 3: (A). Adapter-BERT (Houlsby et al., 2019) uses adapters in a transformer layer (Vaswani et al., 2017). Adapters are 2-layer networks with skip-connections, added twice per layer. Only adapters (yellow) and layer norm (green) are trainable while other modules (grey) are frozen. (B). B-CL replaces adapters with CLA, containing a knowledge-sharing module (KSM) and taskspecific module (TSM), both with skip-connections. Image and modified caption are taken from (Ke et al., 2021b). For example, if we consider the BERT layer in Figure 3, we can observe that the adapter layers for the MTL approach in Figure 3(A) are in the same positions as the CL adapters in Figure 3(B) (Ke et al., 2021b). To switch from MTL to CL we only need to change adapters. We believe that this similarity could be a promising direction for further research on the development of a CMTL model, but proper evaluation will not be possible without a good benchmark. Finally, we believe that a benchmark that accurately represents the challenges of real-world systems could be beneficial for both researchers and practitioners to evaluate CMTL models effectively. To accomplish this, we propose combining and temporarily ordering tasks to simulate periodic re-training and CL scenarios. The benchmark can be further refined to reflect varying frequencies of periodic re-training, as well as the frequency of incoming tasks and distribution shifts between re-training periods. Variation in these scenarios could better represent real-world situations, where a simpler CMTL model might be sufficient in some cases, while more advanced CMTL models would be needed to handle environmental changes in other situations. 5 CONCLUSION In this paper, we reviewed transformer-based MTL approaches in NLP and explored the challenges and opportunities of those approaches in the context of the ML lifecycle. We discussed how MTL can be a possible solution addressing some of the key challenges in data engineering, model development, deployment, and monitoring phases of the ML lifecycle, as compared to using multiple single-task models. We also discussed the opportunities for applying MTL to alleviate challenges regarding model updating due to distribution shifts or evolving real-world requirements, where the ability to learn multiple tasks simultaneously can be leveraged to periodically update the model in response to changes in data and environment. However, we also acknowledged the limitations of current MTL approaches to handle updates sequentially, where CL is required. To address this, we proposed the concept of CMTL, which aims to combine the benefits of MTL and CL in a single model. We motivated creating a benchmark for the proper evaluation of CMTL models, a benchmark that better represents the challenges of production systems that can guide the development of those models. In conclusion, MTL approaches offer many opportunities for improving the efficiency and performance of ML systems in different phases of the typical ML lifecycle. We believe that MTL will become an important part of practitioners\u2019 toolbox seeking to address the challenges in their ML systems as highlighted in this survey. REFERENCES Hala Abdelkader. Towards robust production machine learning systems: Managing dataset shift. In 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 1164\u20131166. IEEE, 2020. 11 Preprint. Stanley Ebhohimhen Abhadiomhen, Royransom Chimela Nzeh, Ernest Domanaanmwi Ganaa, Honour Chika Nwagwu, George Emeka Okereke, and Sidheswar Routray. Supervised shallow multi-task learning: analysis of methods. Neural Processing Letters, 54(3):2491\u20132508, 2022. Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5799\u20135811, 2021. Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations, 2021. Rob Ashmore, Radu Calinescu, and Colin Paterson. Assuring the machine learning lifecycle: Desiderata, methods, and challenges. ACM Computing Surveys (CSUR), 54(5):1\u201339, 2021. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked language models for unified language model pretraining. In Hal Daum\u00b4e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 642\u2013652. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/bao20a.html. Lucas Bernardi, Themistoklis Mavridis, and Pablo Estevez. 150 successful machine learning models: 6 lessons learned at booking. com. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1743\u20131751, 2019. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165. Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997. Shijie Chen, Yu Zhang, and Qiang Yang. Multi-task learning in natural language processing: An overview. arXiv preprint arXiv:2109.09138, 2021. Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 12(3):1\u2013207, 2018. Michael Chui, Bryce Hall, Helen Mayhew, and Alex Singla. The state of ai in 2022\u2013and a half decade in review, Dec 2022. URL https://www.mckinsey.com/capabilities/quantumblack/our-insights/ the-state-of-ai-in-2022-and-a-half-decade-in-review. Accessed: 2023-01-15. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4069\u20134082, Hong Kong, China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1418. URL https://aclanthology.org/D19-1418. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. Bam! born-again multi-task networks for natural language understanding. CoRR, abs/1907.04829, 2019b. URL http://arxiv. org/abs/1907.04829. Michael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796, 2020. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u02c7s Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021. 12 Preprint. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/ N19-1423. URL https://aclanthology.org/N19-1423. Jie Ding, Vahid Tarokh, and Yuhong Yang. Model selection techniques: An overview. IEEE Signal Processing Magazine, 35(6):16\u201334, 2018. Gregory Ditzler, Manuel Roveri, Cesare Alippi, and Robi Polikar. Learning in nonstationary environments: A survey. IEEE Computational Intelligence Magazine, 10(4):12\u201325, 2015. Ana Gonz\u00b4alez-Gardu\u02dcno and Anders S\u00f8gaard. Learning to predict readability using eye-movement data from natives and learners. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. doi: 10.1609/aaai. v32i1.11978. URL https://ojs.aaai.org/index.php/AAAI/article/view/11978. Han Guo, Ramakanth Pasunuru, and Mohit Bansal. Autosem: Automatic task selection and mixing in multi-task learning. CoRR, abs/1904.04153, 2019. URL http://arxiv.org/abs/1904.04153. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask learning. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Manish Gupta and Puneet Agrawal. Compression of deep learning models for text: A survey. ACM Transactions on Knowledge Discovery from Data (TKDD), 16(4):1\u201355, 2022. David Ha, Andrew Dai, and Quoc V. Le. Hypernetworks, 2016. URL https://arxiv.org/abs/1609.09106. Malay Haldar, Mustafa Abdool, Prashant Ramanathan, Tao Xu, Shulin Yang, Huizhong Duan, Qing Zhang, Nick Barrow-Williams, Bradley C Turnbull, Brendan M Collins, et al. Applying deep learning to airbnb search. In proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & Data Mining, pp. 1927\u2013 1935, 2019. Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. PTR: prompt tuning with rules for text classification. CoRR, abs/2105.11259, 2021. URL https://arxiv.org/abs/2105.11259. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021. Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, Yaguang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. HyperPrompt: Prompt-based task-conditioning of transformers. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8678\u20138690. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/he22f.html. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790\u20132799. PMLR, 2019. Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. Chip Huyen. Designing Machine Learning Systems. \u201d O\u2019Reilly Media, Inc.\u201d, 2022. S\u00b4ebastien Jean, Orhan Firat, and Melvin Johnson. Adaptive scheduling for multi-task learning. CoRR, abs/1909.06434, 2019. URL http://arxiv.org/abs/1909.06434. Xisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. Learn continually, generalize rapidly: lifelong knowledge accumulation for few-shot learning. arXiv preprint arXiv:2104.08808, 2021. Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit. One model to learn them all, 2017. URL https://arxiv.org/abs/1706.05137. 13 Preprint. Andrej Karpathy. HydraNets - Tesla AI Day 2021, 8 2021. URL https://www.youtube.com/watch?t= 4284&v=j0z4FweCy4M&feature=youtu.be. Accessed: 2023-02-15. Zixuan Ke and Bing Liu. Continual learning of natural language processing tasks: A survey. arXiv preprint arXiv:2211.12701, 2022. Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu. Achieving forgetting prevention and knowledge transfer in continual learning. Advances in Neural Information Processing Systems, 34:22443\u201322456, 2021a. Zixuan Ke, Hu Xu, and Bing Liu. Adapting bert for continual learning of a sequence of aspect sentiment classification tasks. arXiv preprint arXiv:2112.03271, 2021b. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Been Kim and Finale Doshi-Velez. Machine learning techniques for accountability. AI Magazine, 42(1):47\u201352, 2021. Frederic Kirstein, Jan Philip Wahle, Terry Ruas, and Bela Gipp. Analyzing multi-task learning for abstractive text summarization. arXiv preprint arXiv:2210.14606, 2022. Valliappa Lakshmanan, Sara Robinson, and Michael Munn. Machine learning design patterns. O\u2019Reilly Media, 2020. Hae Beom Lee, Eunho Yang, and Sung Ju Hwang. Deep asymmetric multi-task feature learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2956\u20132964. PMLR, 10\u201315 Jul 2018. URL https: //proceedings.mlr.press/v80/lee18d.html. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. CoRR, abs/2104.08691, 2021. URL https://arxiv.org/abs/2104.08691. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/685bfde03eb646c27ed565881917c71c-Paper.pdf. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. CoRR, abs/2107.13586, 2021a. URL https://arxiv.org/abs/2107.13586. Shengchao Liu, Yingyu Liang, and Anthony Gitter. Loss-balanced task weighting to reduce negative transfer in multitask learning. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):9977\u20139978, Jul. 2019a. doi: 10. 1609/aaai.v33i01.33019977. URL https://ojs.aaai.org/index.php/AAAI/article/view/5125. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT understands, too. CoRR, abs/2103.10385, 2021b. URL https://arxiv.org/abs/2103.10385. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding, 2019b. URL https://arxiv.org/abs/1901.11504. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019c. URL http://arxiv.org/abs/1907.11692. 14 Preprint. Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, and Philip S Yu. Learning multiple tasks with multilinear relationship networks. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf. David Lopez-Paz and Marc' Aurelio Ranzato. Gradient episodic memory for continual learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/ paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf. Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. URL https://arxiv.org/abs/2106.04489. Gary E Marchant. The growing gap between emerging technologies and the law. Springer, 2011. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1\u201335, 2021. Nadia Nahar, Shurui Zhou, Grace Lewis, and Christian K\u00a8astner. Collaboration challenges in building ml-enabled systems: Communication, documentation, engineering, and process. In Proceedings of the 44th International Conference on Software Engineering, pp. 413\u2013425, 2022. Fannia Pacheco, Ernesto Exposito, Mathieu Gineste, Cedric Baudoin, and Jose Aguilar. Towards the deployment of machine learning solutions in network traffic classification: A systematic survey. IEEE Communications Surveys & Tutorials, 21(2):1988\u20132014, 2018. Andrei Paleyes, Raoul-Gabriel Urma, and Neil D Lawrence. Challenges in deploying machine learning: a survey of case studies. ACM Computing Surveys, 55(6):1\u201329, 2022. Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. In-BoXBART: Get instructions into biomedical multi-task learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 112\u2013128, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.10. URL https://aclanthology.org/2022.findings-naacl.10. Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A. Zuluaga. Maximum roaming multi-task learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10):9331\u20139341, May 2021. doi: 10. 1609/aaai.v35i10.17125. URL https://ojs.aaai.org/index.php/AAAI/article/view/17125. Zhongyi Pei, Lin Liu, Chen Wang, and Jianmin Wang. Requirements engineering for machine learning: A review and reflection. In 2022 IEEE 30th International Requirements Engineering Conference Workshops (REW), pp. 166\u2013175. IEEE, 2022. Fabio Petroni, Tim Rockt\u00a8aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 2463\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250. Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00a8uckl\u00b4e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Nondestructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020a. Jonas Pfeiffer, Andreas R\u00a8uckl\u00b4e, Clifton Poth, Aishwarya Kamath, Ivan Vuli\u00b4c, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. arXiv preprint arXiv:2007.07779, 2020b. Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. MAD-X: an adapter-based framework for multi-task cross-lingual transfer. CoRR, abs/2005.00052, 2020c. URL https://arxiv.org/abs/2005.00052. Jonathan Pilault, Amine Elhattami, and Christopher Pal. Conditionally adaptive multi-task learning: Improving transfer learning in nlp using fewer parameters & less data, 2020. URL https://arxiv.org/abs/2009.09139. 15 Preprint. Eugenia Politou, Efthimios Alepis, and Constantinos Patsakis. Forgetting personal data and revoking consent under the gdpr: Challenges and proposed solutions. Journal of cybersecurity, 4(1):tyy001, 2018. Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich. Data lifecycle challenges in production machine learning: a survey. ACM SIGMOD Record, 47(2):17\u201328, 2018. Subhojeet Pramanik, Priyanka Agrawal, and Aman Hussain. Omninet: A unified architecture for multi-modal multitask learning. CoRR, abs/1907.07804, 2019. URL http://arxiv.org/abs/1907.07804. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017. Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. Adversarial attacks and defenses in deep learning. Engineering, 6 (3):346\u2013360, 2020. Cedric Renggli, Bojan Karla\u02c7s, Bolin Ding, Feng Liu, Kevin Schawinski, Wentao Wu, and Ce Zhang. Continuous integration of machine learning models with ease. ml/ci: Towards a rigorous yet practical treatment. Proceedings of Machine Learning and Systems, 1:322\u2013333, 2019. Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Adversarial machine learning attacks and defense methods in the cyber security domain. ACM Computing Surveys (CSUR), 54(5):1\u201336, 2021. Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017. Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Sluice networks: Learning what to share between loosely related tasks. arXiv preprint arXiv:1705.08142, 2, 2017. Rahul Manohar Samant, Mrinal Bachute, Shilpa Gite, and Ketan Kotecha. Framework for deep learning-based language models using multi-task learning in natural language understanding: A systematic literature review and future directions. IEEE Access, 2022. Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. \u201ceveryone wants to do the model work, not the data work\u201d: Data cascades in high-stakes ai. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1\u201315, 2021. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2021. URL https://arxiv.org/abs/2110.08207. Tim Schr\u00a8oder and Michael Schulz. Monitoring machine learning models: a categorization of challenges and methods. Data Science and Management, 5(3):105\u2013116, 2022. David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. Advances in neural information processing systems, 28, 2015. Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4548\u20134557. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/serra18a.html. Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training nlp models: A concise overview. arXiv preprint arXiv:2004.08900, 2020. 16 Preprint. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222\u20134235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https: //aclanthology.org/2020.emnlp-main.346. Ayan Sinha, Zhao Chen, Vijay Badrinarayanan, and Andrew Rabinovich. Gradient adversarial training of neural networks. CoRR, abs/1806.08028, 2018. URL http://arxiv.org/abs/1806.08028. Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? In Hal Daum\u00b4e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9120\u20139132. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/standley20a. html. Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efficient adaptation in multi-task learning. In International Conference on Machine Learning, pp. 5986\u20135995. PMLR, 2019. Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 13693\u201313696, 2020. Ruo-Yu Sun. Optimization for deep learning: An overview. Journal of the Operations Research Society of China, 8 (2):249\u2013294, 2020. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: A continual pretraining framework for language understanding. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 8968\u20138975, 2020. Hironori Takeuchi and Shuichiro Yamamoto. Business analysis method for constructing business\u2013ai alignment model. Procedia Computer Science, 176:1312\u20131321, 2020. Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid: Efficient multi-task transformers with grid-wise decomposable hyper projections. CoRR, abs/2007.05891, 2020. URL https://arxiv.org/abs/ 2007.05891. Kim-Han Thung and Chong-Yaw Wee. A brief review on multi-task learning. Multimedia Tools and Applications, 77 (22):29705\u201329725, 2018. Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, and Marcus Liwicki. Sharing to learn and learning to share-fitting together meta-learning, multi-task learning, and transfer learning: A meta review. arXiv preprint arXiv:2111.12146, 2021. Partoo Vafaeikia, Khashayar Namdar, and Farzad Khalvati. A brief review of deep multi-task learning and auxiliary task learning. arXiv preprint arXiv:2007.01126, 2020. Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. IEEE transactions on pattern analysis and machine intelligence, 2021. Manasi Vartak and Samuel Madden. Modeldb: Opportunities and challenges in managing machine learning models. IEEE Data Eng. Bull., 41(4):16\u201325, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Johannes Von Oswald, Christian Henning, Jo\u02dcao Sacramento, and Benjamin F Grewe. Continual learning with hypernetworks. arXiv preprint arXiv:1906.00695, 2019. Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability across NLP tasks. CoRR, abs/2005.00770, 2020. URL https://arxiv.org/abs/2005.00770. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. 17 Preprint. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019. Liwen Wang, Rumei Li, Yang Yan, Yuanmeng Yan, Sirui Wang, Wei Wu, and Weiran Xu. Instructionner: A multitask instruction-based generative framework for few-shot ner, 2022. URL https://arxiv.org/abs/2203. 03903. Sinong Wang, Madian Khabsa, and Hao Ma. To pretrain or not to pretrain: Examining the benefits of pretraining on resource rich tasks. arXiv preprint arXiv:2006.08671, 2020. Steven Euijong Whang, Yuji Roh, Hwanjun Song, and Jae-Gil Lee. Data collection and quality challenges in deep learning: A data-centric ai perspective. The VLDB Journal, pp. 1\u201323, 2023. Joseph Worsham and Jugal Kalita. Multi-task learning for natural language processing in the 2020s: where are we going? Pattern Recognition Letters, 136:120\u2013126, 2020. Nan Wu and Yuan Xie. A survey of machine learning for computer architecture and systems. ACM Computing Surveys (CSUR), 55(3):1\u201339, 2022. Sen Wu, Hongyang R. Zhang, and Christopher R\u00b4e. Understanding and improving information transfer in multi-task learning, 2020. URL https://arxiv.org/abs/2005.00944. Chenyang Yang, Rachel Brower-Sinning, Grace A Lewis, Christian K\u00a8astner, and Tongshuang Wu. Capabilities for better ml engineering. arXiv preprint arXiv:2211.06409, 2022. Yongxin Yang and Timothy M. Hospedales. Trace norm regularised deep multi-task learning. CoRR, abs/1606.04038, 2016. URL http://arxiv.org/abs/1606.04038. Amir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg. Learning a unified embedding for visual search at pinterest. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2412\u20132420, 2019. Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017. Yu Zhang and Qiang Yang. An overview of multi-task learning. National Science Review, 5(1):30\u201343, 2018. Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, and Meng Jiang. A survey of multi-task learning in natural language processing: Regarding task relatedness and training methods. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 943\u2013956, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.66. Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao Yu, Feiyue Huang, and Rongrong Ji. Pyramidal person re-identification via multi-loss dynamic training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Wenxuan Zhou. An overview of models and methods for multi-task learning, Oct 2019. URL https: //shanzhenren.github.io/csci-699-replnlp-2019fall/lectures/W6-L1-Multi_Task_ Learning.pdf. A ADDITIONAL DETAILS FOR RELATED SURVEYS A.1 MULTI-TASK LEARNING SURVEYS The idea of MTL has been explored in many research studies. In this section, we provide an overview of related work in MTL surveys and address different MTL aspects that were discussed. In Table 3, we list certain aspects and indicate the survey in which they were discussed. In the rest of the section, we just mention related surveys and go over individual MTL aspects (shown in bold) in more detail. 18 Preprint. Table 3: Discussed aspects per MTL survey. Aspects are indicated in bold. Year MTL Survey 2017 1 - (Ruder, 2017) 2 - (Zhang & Yang, 2017) 2018 3 - (Zhang & Yang, 2018) 4 - (Thung & Wee, 2018) 2019 5 - (Zhou, 2019) 2020 6 - (Vafaeikia et al., 2020) 7 - (Worsham & Kalita, 2020) 8 - (Crawshaw, 2020) 2021 9 - (Vandenhende et al., 2021) 10 - (Chen et al., 2021) 11 - (Upadhyay et al., 2021) 2022 12 - (Samant et al., 2022) 13 - (Abhadiomhen et al., 2022) 2023 14 - (Zhang et al., 2023) Aspect \\Survey 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Computational Model Traditional ML \u2713 \u2713 \u2713 \u2713 \u2713 Deep Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Learning Type Joint Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Auxiliary Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Architectures Taxonomy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Learning to Share \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Universal Models \u2713 \u2713 \u2713 \u2713 \u2713 Optimization Loss Weighting \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Regularization \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Task Scheduling \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gradient Modulation \u2713 \u2713 \u2713 \u2713 \u2713 Knowledge Distillation \u2713 \u2713 \u2713 \u2713 Multi-Objective Optimization \u2713 \u2713 \u2713 Task Relationship Learning Task Grouping \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Relationships Transfer \u2713 \u2713 \u2713 \u2713 \u2713 Task Embeddings \u2713 \u2713 Supervision Level Supervised Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Semi-supervised Learning \u2713 \u2713 \u2713 \u2713 \u2713 Self-supervised Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Connection to Learning Paradigm Reinforcement Learning \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Transfer Learning \u2713 \u2713 \u2713 Domain Adaptation \u2713 \u2713 \u2713 Meta-Learning \u2713 \u2713 \u2713 Active Learning \u2713 \u2713 Online Learning \u2713 \u2713 \u2713 Continual Learning Benchmarks Benchmark Overview \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Model Comparison \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Application Domain Natural Language Processing \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Computer Vision \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Healthcare \u2713 \u2713 \u2713 Bioinformatics \u2713 \u2713 \u2713 \u2713 \u2713 Other \u2713 \u2713 \u2713 \u2713 \u2713 Many application domains were studied in previous work, ranging from surveys covering multiple domains (Ruder, 2017; Zhang & Yang, 2017; 2018; Thung & Wee, 2018; Vafaeikia et al., 2020; Crawshaw, 2020; Upadhyay et al., 2021; Abhadiomhen et al., 2022), to those dedicated to a specific domain, such as computer vision (Vandenhende et al., 2021) or natural language processing (Zhou, 2019; Worsham & Kalita, 2020; Chen et al., 2021; Samant et al., 2022; Zhang et al., 2023). Both traditional ML and deep learning computational models were studied. The traditional ML was discussed primarily in older studies, while deep learning was represented in all but one study. Furthermore, 19 Preprint. most prior works provided an overview of the benchmarks for the specific domain (McCann et al., 2018; Wang et al., 2019), and compare models on them. Learning types. Two learning types were mainly discussed. First, joint learning was used in an MTL setting where all the tasks are equally important (Kendall et al., 2018; Liu et al., 2019b). Here, the goal is to achieve an on-par performance compared to their single-task learning (STL) counterparts. Second, auxiliary learning was used when there was a single main task or a set of them, while auxiliary tasks are only used to improve the performance of the main tasks (Gonz\u00b4alez-Gardu\u02dcno & S\u00f8gaard, 2018; Wang et al., 2022). Although the difference between the two learning types can be made, sometimes auxiliary learning was not differentiated from joint learning. Architectures. MTL model architectures were among the most discussed aspects of MTL and one of the first aspects tackled in previous work. Distinguishing between hard and soft parameter sharing (Ruder, 2017) is the most used architecture taxonomy. In recent surveys, the method of sharing parameters between tasks has improved, leading to the refinement of taxonomies to categorize architectures more precisely (Crawshaw, 2020; Chen et al., 2021). Next, some MTL architectures were categorized as learning-to-share approaches (Ruder et al., 2017). Those works argue that it is better to learn parameter sharing in architectures for MTL rather than hand-design where sharing happens, as it offers a more adaptive solution to accommodate task similarities at different parts of the network. Additionally, some MTL architectures were categorized as universal models which can handle multiple modalities, domains, and tasks with a single model (Kaiser et al., 2017; Pramanik et al., 2019). Optimization. Optimization techniques for MTL architectures were also discussed in detail. To start with, the most common approach to mitigating MTL challenges is loss weighting. Computing weights of task-specific losses is crucial, as it helps optimize the loss function and considers the relative importance of each task. There are various approaches to computing the loss weights dynamically, including weighting by uncertainty (Kendall et al., 2018), learning speed (Liu et al., 2019a; Zheng et al., 2019), or performance (Guo et al., 2018; Jean et al., 2019), among others. Next, and closely related to weighting task losses, is a task scheduling problem that involves choosing tasks to train on at each step. Many techniques were used, from simple ones that employ uniform or proportional task sampling, to the more complicated ones, such as annealed sampling (Stickland & Murray, 2019) or approaches based on active learning (Pilault et al., 2020). Regularization approaches were also analyzed. Methods include (1) minimizing the L2 norm between the parameters of soft-parameter sharing models (Yang & Hospedales, 2016), (2) placing prior distributions on the network parameters (Long et al., 2017), (3) introducing an auto-encoder term to the objective function (Lee et al., 2018), (4) MTL variant of dropout (Pascal et al., 2021), and others. To continue, gradient modulation techniques were employed to mitigate the problem of negative transfer by manipulating gradients of contradictory tasks, either through adversarial training (Sinha et al., 2018) or by replacing the gradient by its modified version (Lopez-Paz & Ranzato, 2017). Another approach for optimizing MTL models is by applying knowledge distillation (Clark et al., 2019b). Finally, multi-objective optimization has been applied to the MTL setting to obtain a set of Pareto optimal solutions on the Pareto frontier, providing greater flexibility in balancing trade-offs between tasks (Lin et al., 2019). Task relationship learning. The approach in MTL that focuses on learning the explicit representation of tasks or relationships between them is task relationship learning. This approach consists of three main categories of methods. First, task grouping aims to divide a set of tasks into groups in order to maximize knowledge sharing during joint training (Standley et al., 2020). Second, transfer relationship learning involves methods that determine when transferring knowledge from one task to another will be beneficial for joint learning (Zamir et al., 2018; Guo et al., 2019). Finally, task embedding methods aim to learn an embedding space for the tasks themselves (Vu et al., 2020). As for the supervision level, most studies focused on supervised approaches. Some studies analyzed semi-supervised approaches that incorporate self-supervised objectives, such as MLM. However, self-supervised approaches are mainly discussed in the context of pre-training. Most studies had made connections to other learning paradigms, including reinforcement learning, transfer learning with a special emphasis on domain adaptation, meta-learning, and active and online learning. However, only Ruder (2017); Zhang & Yang (2017; 2018) explored the relationship between MTL and online learning in the context of traditional ML. To the best of our knowledge, there had been no prior work systematically investigating connections between MTL and CL. We believe that a connection between MTL and CL represents a promising research direction, as we will motivate the importance of this connection in Section 4. 20 ", "conclusion": "CONCLUSION In this paper, we reviewed transformer-based MTL approaches in NLP and explored the challenges and opportunities of those approaches in the context of the ML lifecycle. We discussed how MTL can be a possible solution addressing some of the key challenges in data engineering, model development, deployment, and monitoring phases of the ML lifecycle, as compared to using multiple single-task models. We also discussed the opportunities for applying MTL to alleviate challenges regarding model updating due to distribution shifts or evolving real-world requirements, where the ability to learn multiple tasks simultaneously can be leveraged to periodically update the model in response to changes in data and environment. However, we also acknowledged the limitations of current MTL approaches to handle updates sequentially, where CL is required. To address this, we proposed the concept of CMTL, which aims to combine the benefits of MTL and CL in a single model. We motivated creating a benchmark for the proper evaluation of CMTL models, a benchmark that better represents the challenges of production systems that can guide the development of those models. In conclusion, MTL approaches offer many opportunities for improving the efficiency and performance of ML systems in different phases of the typical ML lifecycle. We believe that MTL will become an important part of practitioners\u2019 toolbox seeking to address the challenges in their ML systems as highlighted in this survey. ", "full_text": "Preprint.\nCHALLENGES AND OPPORTUNITIES OF USING\nTRANSFORMER-BASED MULTI-TASK LEARNING IN NLP\nTHROUGH ML LIFECYCLE: A SURVEY\nLovre Torbarina\u2217,\u2020, \u03c9\nTin Ferkovic\u2217, \u03c9\nLukasz Roguski \u03c9\nVelimir Mihelcic \u03c9\nBruno Sarlija \u03c9\nZeljko Kraljevic \u03c9\n\u03c9 doXray B.V., Neede, Netherlands\nname.lastname@doxray.com\nABSTRACT\nThe increasing adoption of natural language processing (NLP) models across industries has led to\npractitioners\u2019 need for machine learning systems to handle these models efficiently, from training\nto serving them in production. However, training, deploying, and updating multiple models can be\ncomplex, costly, and time-consuming, mainly when using transformer-based pre-trained language\nmodels. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency\nand performance through joint training, rather than training separate models. Motivated by this,\nwe first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the\nchallenges and opportunities of using MTL approaches throughout typical ML lifecycle phases,\nspecifically focusing on the challenges related to data engineering, model development, deployment,\nand monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best\nof our knowledge, is novel in that it systematically analyses how transformer-based MTL in NLP fits\ninto ML lifecycle phases. Furthermore, we motivate research on the connection between MTL and\ncontinual learning (CL), as this area remains unexplored. We believe it would be practical to have\na model that can handle both MTL and CL, as this would make it easier to periodically re-train the\nmodel, update it due to distribution shifts, and add new capabilities to meet real-world requirements.\n1\nINTRODUCTION\nIn recent years, advancements in natural language processing (NLP) have revolutionized the way we deal with complex\nlanguage problems. Consequently, those advances have been significantly impacting the global industry, driving\ngrowth in organizations that incorporate AI technologies as part of their core business. To illustrate, McKinsey\u2019s\nstate of AI report for 2022 has reported an increase of 3.8 times since 2017 in AI capabilities that organizations have\nembedded within at least one function or business unit, where natural language understanding (NLU) took third place\namong reported capabilities, just behind computer vision (Chui et al., 2022). Furthermore, Fortune Business Insights\nprojected growth of global NLP from USD 20.80 billion in 2021 to USD 161.81 billion by 2029.1 As a result, each\nNLP practitioner offering models through an API or using them internally, alone, or together with other AI capabilities,\nmust have a machine learning (ML) system to efficiently manage these models. This involves having well-established\nprocesses from training and verifying those models to deploying them in production for end-users while continuously\nmonitoring that those models remain up-to-date with the most recent knowledge they are being trained on.\nThis trend of widespread adoption of ML models by various practitioners throughout industries, and the resulting need\nfor ML systems to manage them efficiently, was tackled in a survey of ML systems conducted by Paleyes et al. (2022).\nThe survey analyzed publications and blog posts reported by different practitioners, providing insights into phases\nof an ML lifecycle and challenges that commonly arise during those phases. The ML lifecycle refers to the phases\nand processes involved in designing, developing, and deploying an ML system. It encompasses the entire process,\n\u2217 Equal contribution.\n\u2020 Correspondence to: Lovre Torbarina <lovre.torbarina@doxray.com>\n1Fortune Business Insights - NLP Market Size\n1\narXiv:2308.08234v1  [cs.CL]  16 Aug 2023\n Preprint.\nfrom defining the problem and collecting data to deploying models and monitoring their performance. Model learning\nand model deployment are two important phases in the ML lifecycle, among others (Ashmore et al., 2021; Paleyes\net al., 2022). To support practitioners\u2019 needs, the model learning phase should be equipped to handle the training and\nupdating of a large number of models, while the model deployment phase must provide an easy and efficient way to\nintegrate and serve those models to run in production, that is, running as part of usual business operations.\nAt the same time, it is a common practice in NLP production systems to utilize pre-trained language models based on\ntransformers (Vaswani et al., 2017) by fine-tuning them for specific downstream tasks. While effective, the language\nmodels have a large number of parameters that require significant computational resources to fine-tune. Although fine-\ntuning pre-trained models can be more data-efficient than training a model from scratch, the expertise of annotators\nor domain experts may still be required to label a large number of examples, particularly if there is a significant\ndifference between the downstream task and pre-training objectives (Wang et al., 2020). Therefore, it is a costly and\ntime-consuming procedure, especially if there is a need to train and serve multiple models in production. To address\nthe challenge of training multiple models, researchers have been exploring Multi-Task Learning (MTL) as a solution\n(Ruder, 2017). MTL trains a single model to learn multiple tasks simultaneously while sharing part of the model\nparameters between them (Caruana, 1997), making the process memory-efficient and, in some cases, computationally\nmore efficient than training multiple models. Additionally, using a single model for multiple downstream tasks in a\nproduction system could simplify the integration of ML models with ML systems and reduce economic costs. This is\ndue to the modular nature of MTL architectures that promote code and model sharing, reuse, easier collaboration, and\nmaintenance. Moreover, the MTL model reduces idle time since the same model is used for various tasks. Therefore,\nMTL approaches offer a promising solution to mitigate some of the difficulties associated with managing multiple\nmodels in ML production systems.\nIn this survey, we first provide an overview of transformer-based MTL approaches in NLP (see Section 3). Second, we\nhighlight the opportunities for using MTL approaches across multiple stages of the ML lifecycle, specifically focusing\non the challenges related to data engineering, model development, deployment, and monitoring (see Section 4). We\nfocus solely on transformer-based architectures. To the best of our knowledge, this is the first survey that systematically\ndiscusses the benefits of using MTL approaches across multiple ML lifecycle phases (see Section 2). Additionally,\nwe encourage further research on the connection between MTL and Continual Learning (CL). We argue that having a\nmodel capable of handling both MTL and CL is practical as it addresses the need for periodic re-training and continual\nupdates in response to distribution shifts and the addition of new capabilities in production models.\nThe rest of the paper is organized as follows. In Section 2, we briefly review related surveys and highlight the gaps\naddressed in our survey. In Section 3, we give an overview of transformer-based MTL approaches. In Section 4, we\nsystematically analyze the benefits of using MTL through specific ML lifecycle phases. And finally, in Section 5, we\ngive a conclusion to our work.\n2\nRELATED SURVEYS\nIn this section, we give an overview of related work on MTL, ML systems, and CL, and point out what has not been\ndiscussed so far regarding the connection of MTL to both ML systems and CL.\n2.1\nMULTI-TASK LEARNING\nThe idea of MTL was explored in many research studies. In this section, we provide an overview of related MTL\nsurveys, address various aspects of MTL, and list them along with their corresponding surveys in Table 1. 2 In the rest\nof the section, we just mention related surveys and go over individual MTL aspects (shown in bold) in more detail.3\nMany application domains were studied in previous work, ranging from surveys covering multiple domains (Ruder,\n2017; Zhang & Yang, 2017; 2018; Thung & Wee, 2018; Vafaeikia et al., 2020; Crawshaw, 2020; Upadhyay et al.,\n2021; Abhadiomhen et al., 2022), to those dedicated to a specific domain, such as computer vision (Vandenhende\net al., 2021) or natural language processing (Zhou, 2019; Worsham & Kalita, 2020; Chen et al., 2021; Samant et al.,\n2022; Zhang et al., 2023). Both traditional ML and deep learning computational models were studied. The traditional\nML was discussed primarily in older studies, while deep learning was presented in all but one study.\nMTL architectures were widely discussed in previous work. Hard and soft parameter sharing (Ruder, 2017) was the\nmost used architecture taxonomy, but recent surveys refined the taxonomies for more precise categorization (Craw-\nshaw, 2020; Chen et al., 2021). Next, some MTL architectures were categorized as learning-to-share (Ruder et al.,\n2The broader version of the table is provided in Appendix Table 3.\n3The broader version of the section is provided in Appendix A.1.\n2\n Preprint.\nTable 1: Discussed aspects per MTL survey. Aspects are indicated in bold.\nYear\nMTL Survey\n2017\n1 - (Ruder, 2017) 2 - (Zhang & Yang, 2017)\n2018\n3 - (Zhang & Yang, 2018) 4 - (Thung & Wee, 2018)\n2019\n5 - (Zhou, 2019)\n2020\n6 - (Vafaeikia et al., 2020) 7 - (Worsham & Kalita, 2020) 8 - (Crawshaw, 2020)\n2021\n9 - (Vandenhende et al., 2021) 10 - (Chen et al., 2021) 11 - (Upadhyay et al., 2021)\n2022\n12 - (Samant et al., 2022) 13 - (Abhadiomhen et al., 2022)\n2023\n14 - (Zhang et al., 2023)\nAspect \\Survey\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nComputational Model\nTraditional ML\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nDeep Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nArchitectures\nLearning to Share\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nUniversal Models\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nOptimization\nLoss Weighting\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nRegularization\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTask Scheduling\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nGradient Modulation\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nKnowledge Distillation\n\u2713\n\u2713\n\u2713\n\u2713\nMulti-Objective Optimization\n\u2713\n\u2713\n\u2713\nTask Relationship Learning\nTask Grouping\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nRelationships Transfer\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTask Embeddings\n\u2713\n\u2713\nConnection to Learning Paradigm\nReinforcement Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTransfer Learning\n\u2713\n\u2713\n\u2713\nMeta-Learning\n\u2713\n\u2713\n\u2713\nOnline Learning\n\u2713\n\u2713\n\u2713\nContinual Learning\nApplication Domain\nNatural Language Processing\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nComputer Vision\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n2017), which offers a more adaptive solution by learning how to share parameters between tasks, rather than having\nthe sharing defined a priori. Additionally, some MTL architectures were categorized as universal models, handling\nmultiple modalities, domains, and tasks using a single model (Kaiser et al., 2017; Pramanik et al., 2019).\nOptimization techniques for MTL architectures were also widely discussed, while loss weighting was the most com-\nmon approach to mitigate MTL challenges. Techniques include loss weighting by uncertainty (Kendall et al., 2018),\nlearning speed (Liu et al., 2019a; Zheng et al., 2019), or performance (Guo et al., 2018; Jean et al., 2019), among\nothers. Next, and closely related to weighting task losses, is a task scheduling problem that involves choosing tasks\nto train on at each step. Many techniques were used, from simple ones that employ uniform or proportional task\nsampling, to the more complicated ones, such as annealed sampling (Stickland & Murray, 2019) or approaches based\non active learning (Pilault et al., 2020). Finally, regularization approaches (Long et al., 2017; Lee et al., 2018; Pascal\net al., 2021), gradient modulation (Lopez-Paz & Ranzato, 2017; Sinha et al., 2018), knowledge distillation (Clark\net al., 2019b), and multi-objective optimization (Lin et al., 2019) were also applied to optimize MTL models.\nTask relationship learning in MTL focuses on learning explicit representations of tasks or relationships between\nthem, and typically three categories of methods were used. First, task grouping aims to divide a set of tasks into\ngroups to maximize knowledge sharing during joint training (Standley et al., 2020). Second, transfer relationship\nlearning determines when transferring knowledge from one task to another will be beneficial for joint learning (Zamir\net al., 2018). Finally, task embedding methods aim to learn task embedding space (Vu et al., 2020).\nPrevious works made connections to other learning paradigms, including reinforcement learning, transfer learning,\nmeta-learning, active and online learning. To the best of our knowledge, there had been no prior work systematically\n3\n Preprint.\ninvestigating connections between MTL and CL. We believe that a connection between MTL and CL represents a\npromising research direction, as we will motivate the need for this connection in Section 4.4.\n2.2\nML LIFECYCLE AND ML SYSTEMS\nThe unparalleled growth of advancements in ML methods in recent years, with applications in NLP, computer vision,\nand others, has increased the complexity of building ML systems that need to address the requirements of an ML\nlifecycle. Previous works reviewed the challenges of such systems, often defining ML lifecycle phases such as data\nmanagement, model learning, and model deployment, to study systematically the workings of ML systems and identify\nchallenges within and across phases (Vartak & Madden, 2018; Ashmore et al., 2021; Paleyes et al., 2022; Huyen, 2022).\nFor example, Vartak & Madden (2018) defined ML lifecycle phases and analyzed model management challenges.\nFurthermore, Ashmore et al. (2021) discussed assurance of ML for each phase, while Paleyes et al. (2022) reviewed\npractitioners\u2019 challenges at each phase of an ML model deployment workflow in a broader scope than previous surveys.\nIn the rest of this section, we provide a few examples of challenges that occur during typical phases of the ML lifecycle.\nThere are many challenges that occur during different phases of the ML lifecycle. For example, data management\nis typically the early phase of the ML lifecycle with associated challenges such as data collection and preprocessing\n(Polyzotis et al., 2018; Sambasivan et al., 2021; Whang et al., 2023). Subsequently, model learning and verification\nphases take place, presenting challenges such as selecting (Ding et al., 2018) and training (Sun, 2020) a model, and\ndetermining the most effective method for verifying it (Bernardi et al., 2019; Schr\u00a8oder & Schulz, 2022), respectively.\nThen, a model deployment phase takes place with challenges such as model integration (Sculley et al., 2015; Renggli\net al., 2019) into production. Finally, the monitoring phase with challenges such as continuous model performance\nmonitoring (Schr\u00a8oder & Schulz, 2022) and updating a model over time (Ditzler et al., 2015; Abdelkader, 2020).\nSome challenges can impact several phases of the ML lifecycle, such as collaboration among diverse teams and roles,\nincluding software and data engineers, data scientists, and other stakeholders (Takeuchi & Yamamoto, 2020; Nahar\net al., 2022; Pei et al., 2022; Yang et al., 2022). Furthermore, there are challenges of bias, fairness, and accountability\nin ethics (Mehrabi et al., 2021; Kim & Doshi-Velez, 2021), various regulations set by law (Marchant, 2011; Politou\net al., 2018) and adversarial attacks in security (Ren et al., 2020; Rosenberg et al., 2021), among others.\nPrevious works addressed MTL aspects to varying degrees in specific phases, either in a straightforward manner or\nindirectly. However, a systematic discussion of the potential benefits of using MTL approaches to alleviate challenges\nacross different phases of the ML lifecycle has not been conducted.\n2.3\nCONTINUAL LEARNING\nCL incrementally learns a sequence of tasks, with a goal to progressively expand acquired knowledge and utilize it for\nsubsequent learning (Chen & Liu, 2018). CL aims to overcome catastrophic forgetting (CF) and facilitate knowledge\ntransfer (KT) across tasks, where CF is the degradation of performance on previous tasks when learning new ones,\nand KT is the ability to apply knowledge from past tasks to new tasks (Ke & Liu, 2022). Previous reviews on CL\n(Hsu et al., 2018; De Lange et al., 2021) categorized CL settings based on the marginal output and input distributions\nP(Y (t)) and P(X(t)) of a task t, with P(X(t)) \u0338= P(X(t+1)). First, class incremental learning is characterized by\nan expanding output space with observed class labels such that Y (t) \u2282 Y (t+1) and P(Y (t)) \u0338= P(Y (t+1)). Second,\ntask incremental learning (TIL), requires a task label t to identify the separate output nodes Y (t) for the current task\nt, where Y (t) \u0338= Y (t+1). Lastly, incremental domain learning defines tasks with equal class labels and probability\ndistributions, Y (t) = Y (t+1), and P(Y (t)) = P(Y (t+1)).\nCL approaches were also categorized into three main categories based on how task-specific information is stored\nand utilized during the incremental learning process. First, replay methods store samples in raw format or generate\npseudo-samples with a generative model, replaying them while learning a new task to mitigate forgetting and prevent\nprevious task interference. Second, regularization-based methods, on the other hand, avoid storing raw inputs and\nreduce memory requirements by introducing an extra regularization term in the loss function to consolidate prior\nknowledge while learning new data. Third, parameter isolation methods allocate different model parameters to each\ntask, either by adding new task-specific branches or masking out previous task parts, to prevent forgetting and maintain\ntask-specific knowledge. We refer readers to Ke & Liu (2022) for more refined CL taxonomy and details in NLP.\nIn CL, MTL was usually used as an upper-bound baseline which can use all of the data from all of the tasks simulta-\nneously (De Lange et al., 2021; Ke & Liu, 2022). Since CL and MTL work in different learning settings, few works\ntried to connect the two paradigms. Sun et al. (2020) presented a continual pre-training framework named ERNIE 2.0\nwhich incrementally builds pre-training tasks and then learns pre-trained models on these constructed tasks via contin-\n4\n Preprint.\nual multi-task learning. Subsequently, ERNIE 2.0 was tested against CL and MTL pre-training approaches to evaluate\nthe impact on abstractive text summarization task but performed similarly to other approaches (Kirstein et al., 2022).\nIn Section 4.4, we motivate further research on combining CL and MTL approaches, as pre-training approaches are not\nsufficient for handling distribution shifts and adjusting models for new business requirements in real-world scenarios.\n3\nMULTI-TASK LEARNING APPROACHES\n3.1\nTAXONOMY\nThere were various MTL taxonomies covered in the surveys presented in Section 2.1. Ruder (2017) distinguishes\nbetween a hard and soft parameter sharing, which proved to be an influential taxonomy, since it was used in later works\nas well. Zhang & Yang (2018) defines three categories of multi-task supervised learning \u2013 feature-, parameter-, and\ninstance-based. Vandenhende et al. (2021) distinguishes between encoder-focused and decoder-focused architectures.\nChen et al. (2021) discusses parallel, hierarchical, modular, and generative adversarial architectures. We categorized\ntransformer-based MTL approaches into 3 main categories based on differences in architectures: (1) Fully-Shared\nEncoder, (2) Adapters, and (3) Hypernetworks (Figure 1).4\nFigure 1: Simplified overview of MTL architectures. Sub-figure a) represents a fully-shared encoder, b) an adapter, and c) a\nhypernetwork. Blue components are trained jointly by all the tasks, green ones are task-specific, and grey ones are kept frozen. A\ndotted adapter component suggests possible adapter insertion positions.\n3.2\nMTL APPROACHES OVERVIEW\n3.2.1\nFULLY-SHARED ENCODER\nA simple and intuitive approach to MTL is to have a clear division between shared and task-specific parameters. In\nsuch an approach, there is a shared transformer-based encoder in lower layers, while the top layers consist of different,\ntask-specific layers (heads). One such approach, MT-DNN (Liu et al., 2019b), batches all the GLUE tasks (Wang\net al., 2018) together and updates the model accordingly. The shared encoder is updated for all the instances, while the\ntask-specific heads are updated only for the instances of a task that they are specific for. There are several downsides\nto this MT-DNN approach. First, task interference is not taken into account and the authors simply hope that the tasks\nwould interact well, although some of them are in different domains. Next, proportional random sampling is used,\nwhich could lead to underfitting on low-resource datasets. Finally, the loss is calculated in three different ways (for\nclassification, regression, and ranking), and as a result, it has different scales. Nevertheless, all the loss functions are\nweighted equally. Notwithstanding these observations, their model outperforms fine-tuning a different BERT (Devlin\net al., 2019) model on most of the tasks. Additionally, they tried fine-tuning this multi-task model further on each task\nseparately after training jointly on all the tasks, producing N models for N tasks. That again gave an improvement\nand a state-of-the-art performance at the time. However, an obvious downside is having a different model for each\ntask.\nAnother shared-encoder approach is pre-finetuning. Muppet (Aghajanyan et al., 2021) shared an encoder in an MTL\non 46 diverse datasets. Heterogeneous batches have proved beneficial in handling noisy gradients from different\ntasks. Furthermore, to have stable training, the data-point loss was divided by log(n), where n denotes the cardinality\nof the label set for the associated task. They maintained a natural distribution of datasets as other approaches led\n4In Appendix B.1, we give a brief overview of prompt engineering approaches as a fourth category. However, we do not include\nit in the main paper due to its need for a large number of parameters to perform well, making it inaccessible to most practitioners.\n5\n Preprint.\nto degraded performance. The authors found a threshold of around 15 tasks, below which downstream fine-tuning\nperformance is degraded, and above which the performance improves linearly in the number of pre-finetuning tasks.\nA similar approach with added decoder, EXT5 (Aribandi et al., 2021), extended the mixture to 107 supervised tasks,\nformatted them for encoder-decoder architectures, and performed pre-finetuning along with unsupervised T5\u2019s C4 span\ndenoising (Raffel et al., 2020). Their mixture of tasks also included NLP applications such as reading comprehension,\nclosed-book question answering, commonsense reasoning, dialogue, and summarization, among others. This showed\nthat encoder-decoder models like T5 are capable of solving a wider range of NLP applications than encoder ones.\nHowever, task-specific models still achieve a better performance than general ones (Chung et al., 2022).\n3.2.2\nADAPTERS\nBefore being used in NLP, adapter residual modules were first introduced for the visual domain (Rebuffi et al., 2017).\nAdapters are small, task-specific modules that are typically inserted within network layers, but can also be injected in\nparallel to them. In this survey, the network is always a transformer-based architecture. Compared to transformers\u2019\nsizes, they add a negligible number of parameters per task. The parameters of the original network remain frozen\nunless stated otherwise, resulting in a high degree of parameter sharing and a small number of trainable parame-\nters. Consequently, adapters for new tasks can be easily added without retraining the transformer or other adapters.\nThey learn task-specific layer-wise representation, are small, scalable, and shareable, have modular representations\nand a non-interfering composition of information (Pfeiffer et al., 2020b). Since the respective adapters were trained\nseparately, the necessity of sampling heuristics due to skewed data set sizes no longer arises (Pfeiffer et al., 2020b).\nAdapterHub. AdapterHub (Pfeiffer et al., 2020b) is a framework that allows dynamic usage of pre-trained adapters for\ndifferent tasks and languages.5 The framework is built on top of the HuggingFace Transformers library and enables\nquick and easy adaptation of state-of-the-art pre-trained models. It allows for efficient parameter sharing between\ntasks by training many task- and language-specific adapters, which can be exchanged and combined post-hoc. One\ncan choose to stack the adapters on top of each other, combine them with attention (Pfeiffer et al., 2020a), or replace\nthem dynamically. Downloading, sharing, and training adapters require minimal changes in the training scripts.\nBottleneck adapters. In AdapterHub\u2019s documentation, three different bottleneck adapter approaches were mentioned.\nAdapters can be inserted after both Multi-Head Attention (MHA) and Feed-Forward (FF) block (Houlsby et al., 2019),\nonly after the FF block (Pfeiffer et al., 2020c), or in parallel to the Transformer layers (He et al., 2021). Bottleneck\nadapters consist of a down-projection, non-linearity (typically ReLU), and an up-projection back to the original size.\nResidual connection is used, and layer normalization is applied afterward.\nLanguage adapters. In the MAD-X framework (Pfeiffer et al., 2020c), the authors train (1) language adapters\nvia masked language modeling (MLM) on unlabelled target language data, and (2) task adapters by optimizing a\ntarget task on labeled data in a source language with the most training data. Then, adapters are stacked, allowing\nfor a zero-shot cross-lingual transfer by substituting the target language adapter at inference. Invertible adapters\nare introduced to tackle the mismatch between the pre-trained model\u2019s multilingual vocabulary and target language\nvocabulary. Consequently, language adapters could be useful when one had already trained a task adapter for a specific\ntask and now needs to perform inference for the same task, but on new data from a different language.\nOther. Projected Attention Layer (PAL) (Stickland & Murray, 2019) is a low-dimensional multi-head attention layer\nadded in parallel to the transformer layers. Multi-head attention is applied on a down-projected input, after which an\nup-projection to an original dimension is applied. These down- and up-projection matrices are shared among layers,\nbut not among tasks. The authors fine-tune a pre-trained encoder along the PALs. This has downsides: (1) forgetting\nof pre-trained knowledge is possible, (2) access to all the tasks at training time is required, and (3) adding new tasks\nrequires complete joint retraining. Thus, this approach misses many of the characteristics of an adapter.\nAdapterFusion (Pfeiffer et al., 2020a) introduces a knowledge composition phase, in which the previously trained\nadapters are combined. This approach uses multiple adapters to maximize knowledge transfer between tasks without\nsuffering from the MTL drawbacks, such as catastrophic forgetting (Serra et al., 2018) or task interference (Wu et al.,\n2020). It introduces a new set of weights that learn to combine the adapters as a dynamic function of the target task\ndata by using attention. This shows its greatest downside \u2013 AdapterFusion is trained for one task only.\nHu et al. (2021) argue that the original adapter bottleneck design (Houlsby et al., 2019) introduces inference latency\nbecause the adapters are processed sequentially, whereas large language models (LLMs) rely on hardware parallelism.\nTheir approach, LoRA (Low Rank Approximation) modifies attention weights of query and value projection matri-\nces by introducing trainable low-rank decomposition matrices in parallel to the original computation. This reduces\ninference latency, as the decomposition matrices can be merged with the pre-trained weights for faster inference.\n5https://adapterhub.ml\n6\n Preprint.\nFigure 2: ML lifecycle phases. Image is taken from\nHuyen (2022).\nTable 2: ML Lifecycle phases and corresponding challenges.\nML Lifecycle Phase\nChallenges\nProject Scoping\nInitial Requirements\nData Engineering\nLabeling of Large Volumes of Data\nCost of Annotators and Experts\nLack of High-Variance Data\nModel Development\nModel Complexity\nResource-Constrained Environments\nComputational Cost\nEnvironmental Impact\nDeployment\nEase of integration\nMonitoring\nDistribution Shift\nBusiness Analysis\nNew Requirements\n3.2.3\nHYPERNETWORKS\nA hypernetwork is a network that generates weights of another network (Ha et al., 2016). This approach can alleviate a\ndownside of adapters, which is a lack of knowledge sharing. Hypernetwork allows sharing of knowledge across tasks\nwhile adapting to individual tasks through task-specific parameter generation.\nCA-MTL (Pilault et al., 2020) modularizes a pre-trained network by either adding task-conditioned layers or changing\nthe pre-trained weights using the task embedding. Their task-conditioned transformer-based network has four com-\nponents: (1) conditional attention, (2) conditional alignment, (3) conditional layer normalization, and (4) conditional\nbottleneck. In (1), they use block-diagonal conditional attention which allows the attention to account for task-specific\nbiases. Component (2) aligns the data of diverse tasks. In (3), they adjust layer normalization statistics for specific\ntasks. Finally, (4) facilitates weight sharing and boosts task-specific information flow from lower layers. In addition,\nthey use multi-task uncertainty sampling. This favors tasks with the highest uncertainty by sampling a task whenever\nits entropy increases, helping to avoid catastrophic forgetting. When introducing a new task, they claim that only a\nnew linear decoder head and a new task embedding vector need to be added to re-modulate existing weights.\nHyperFormer++ (Mahabadi et al., 2021) uses hypernetworks to generate the weights of adapter and layer normalization\nparameters. These hypernetworks condition on a task embedding, adapter position (after MHA or FF sub-layer), and\nlayer id within the T5 model (Raffel et al., 2020). During training, they sample the tasks using temperature-based\nsampling. They state that for each new task, their model only requires learning an additional task embedding.\nHyperGrid (Tay et al., 2020) leverages a grid-wise decomposable hyper projection structure which helps specialize\nregions in weight matrices for different tasks. To construct the proposed hypernetwork, their method learns the inter-\nactions and composition between a global, task-agnostic state and a local, task-specific state. They equip position-wise\nFF sub-layers of a Transformer with HyperGrid. They initialize a T5 model from a pre-trained checkpoint and add\nadditional parameters that are fine-tuned along the rest of the network. The authors of the paper did not mention\nanything specific regarding the ability to add a new task without re-training, as it appears to be non-trivial.\n4\nMTL FROM ML LIFECYCLE POINT OF VIEW\nIn this section, we discuss the challenges and opportunities of incorporating into ML production systems MTL ap-\nproaches instead of using multiple single-task counterparts. Motivated by previous reviews on ML lifecycle (see\nSection 2.2), we define ML lifecycle phases to discuss challenges and opportunities in a systematic manner.\nFollowing Huyen (2022), we define six ML lifecycle phases: (1) Project Scoping, (2) Data Engineering, (3) Model\nDevelopment, (4) Deployment, (5) Monitoring, and (6) Bussiness Analysis (Figure 2). Next, we mostly focus on\nchallenges that were discussed in a broader scope in Paleyes et al. (2022), while we argue how MTL can alleviate\nthem. Challenges per each phase of the ML lifecycle are listed in Table 2. When discussing the challenges and\nopportunities of using MTL approaches, we compare them to corresponding single-task model solutions.\nIn the rest of the section, we first discuss data engineering and model development phases in isolation. Then, we\ndiscuss an ML model updating problem by indicating how certain aspects of the problem pose different challenges in\ndifferent phases of the ML lifecycle.\n7\n Preprint.\n4.1\nDATA ENGINEERING\nThe first phase we discuss is data engineering. This phase focuses on preparing data that is needed to train a machine\nlearning model, while we have a particular interest in challenges related to the lack of labeled data (see Table 2).\nChallenges. The need for data augmentation can arise from various factors, with one of the most problematic being\nthe lack of labels in the data, especially in real-world applications where labeled data may be scarce. It is a common\npractice in NLP production systems to utilize pre-trained transformer-based language models by fine-tuning them for\nspecific downstream tasks. However, if there is a significant gap between a downstream task and the pre-training\nobjectives, a larger amount of labeled data may still be required to achieve the target performance (Wang et al., 2020).\nObtaining this data involves costly and time-consuming involvement of annotators and domain experts. Additionally,\nthe absence of high-variance data results in a model that is unable to generalize well, such as adapting language models\nto low-resource languages (Clark et al., 2019a).\nOpportunities. The challenges posed by the lack of labeled data can be alleviated using MTL approaches. For ex-\nample, if a set of single-task models is being used in a production system, training an MTL model instead can help\nalleviate data sparsity by jointly learning to solve related tasks (Section 3.2.1). The benefits of MTL have been previ-\nously discussed in Caruana (1997); Ruder (2017), including its ability to increase data-efficiency. First, different tasks\ntransfer different knowledge aspects to each other, enhancing the representation\u2019s ability to express the input text,\nwhich can be beneficial for tasks with low-resource datasets (Section 3.2.1). However, some MTL approaches may\nunderperform in resource-constrained environments due to inadequate optimization choices (Section 3.2.1). Addition-\nally, the presence of different noise patterns in each task acts as an implicit data augmentation method, effectively\nincreasing the sample size used for training, and leading to a robust model with more general representations (Ruder,\n2017). Finally, using pre-finetuning (Section 3.2.1) could reduce convergence time, saving computational resources.\n4.2\nMODEL DEVELOPMENT\nIn the model development phase, we focus on two groups of challenges. The first group refers to the model selection\nproblem, including issues related to model complexity and resource constraints. The second group of challenges is\nrelated to problems in model training, such as the computational cost of the training procedure and its impact on the\nenvironment. We refer readers to (Gupta & Agrawal, 2022) for an overview of methods for efficient models in text.\nModel Selection Challenges. When selecting a model to handle tasks that end-users are interested in, practitioners\noften face a dilemma regarding the trade-off between model complexity and performance. Typically, complex models\nhave better performance, but they come with the risk of over-complicating the design in the first place, leading to a\nlonger development time and deployment failure (Haldar et al., 2019). Furthermore, they may not be practical to use\nin resource-constrained environments where they require high computational and memory resources.\nModel Selection Opportunities. MTL architectures, presented in Section 3.2, have properties that can alleviate\nchallenges related to the trade-off between model complexity and performance. For example, consider replacing N\nsingle-task models with a single MTL shared encoder model. The MTL model will have close to N times smaller\nmemory footprint, as the number of task-specific parameters is negligible compared to the number of shared param-\neters. This reduction results in a better fit to memory-constrained environments while only having slightly worse\nperformance than the single-task counterparts. Similarly, saving N adapters or a single hypernetwork is much more\nmemory-efficient than saving N single-task models.\nModel Training Challenges. The training of machine learning models presents several challenges that must be\naddressed by practitioners. One of the major challenges is the high economic cost associated with training, which is\ndue to the computational resources required. In the field of natural language processing, the cost of model training\ncontinues to rise, even as the cost of individual floating-point operations decreases, due to factors such as the growth in\ntraining dataset size, number of model parameters, and number of operations involved in the training process (Sharir\net al., 2020). The training process also has a significant impact on the environment, leading to increased energy\nconsumption and greenhouse gas emissions (Strubell et al., 2020). These challenges emphasize the need to address\nthe economic and environmental implications of training machine learning models.\nModel Training Opportunities. Computational resource challenges could be alleviated in some aspects by using\nMTL approaches to reduce the cost of model training. First, in certain cases, smaller dataset sizes can be used due to\npre-finetuning or knowledge transfer between related tasks during joint training of the MTL model, leading to more\ndata-efficient training. Second, the joint model is more parameter-efficient, resulting in a significant reduction in the\nnumber of parameters required for multiple single-task models.\n8\n Preprint.\nSelection, Training, and Inference Trade-Off. The number of floating-point operations is not reduced in some\ncases, and depends on the choice of MTL architecture and the nature of the tasks. Different task-specific heads require\ndifferent inputs if tasks belong to different domains or have different input encodings. However, if tasks belong to the\nsame domain and have the same input encoding, part of the computation can be shared among the task-specific heads.\nFor example, in a fully-shared encoder (Section 3.2.1), the computation of the full encoder can be shared, while the\ncomputation in the task-specific heads is negligible. Similarly goes for adapters (Section 3.2.2) \u2013 the large majority\nof the computation is shared, and only the adapters are then dynamically plugged for performing different tasks on\nthe same input. Namely, the benefit of using adapters is a small number of trainable parameters, thanks to a frozen\nencoder, which results in a faster gradient back-propagation. A forward pass, on the other hand, takes more time\ncompared to an encoder with no adapters. Thus, when tasks don\u2019t share the datasets, using adapters results in a longer\ninference time compared to single-task counterparts. AdapterFusion inference is even slower, as an input must pass\nthrough all the available adapters. LoRA solves the inference latency problem by using parameter composition.\n4.3\nMODEL DEPLOYMENT\nIn the model deployment phase, our focus is on simplifying the integration of trained ML models with existing ML\nsystems that are running in production, with a particular emphasis on straightforward implementation, seamless col-\nlaboration, and ease of maintenance.\nChallenges. The first challenge in model deployment is preparing the developed model for use in a production envi-\nronment. The initial versions of models are often developed by stakeholders (e.g., ML researchers) who are different\nfrom those responsible for deploying them into production (e.g., ML and DevOps engineers). This means that the\nmodel code needs to be adapted to meet the requirements of the operational production environment. Those require-\nments are typically more strict and different from those available during the development phase, so it is important to\naddress operational aspects such as scalability, security, and reliability. Hence, each additional model adds complexity\nto the process, both for the stakeholders involved and for the ML system infrastructure and computational resources\nin place. Another challenge during model integration is incorporating the model into real-data processing pipelines in\nproduction, whether it is for batch offline processes or handling real-time user requests. During model training, the\nresearcher often uses preprocessed and clean datasets. However, when integrating the model into production, it will\nbe integrated into existing data pipelines. The more complex the model\u2019s input data requirements, or the more models\nthat are used, the more complex the data processing pipelines will need to be. Adapting the models and existing data\npipelines often requires collaboration between different stakeholders or teams responsible for different parts of the ML\nsystem and/or ML lifecycle phase. All of these factors directly impact the increasing costs of maintenance, operations,\nsupport, and infrastructure.\nOpportunities. MTL approach was presented by the team from Pinterest, where the authors trained a universal set of\nimage embeddings for three different models, which simplified their deployment pipelines and improved performance\non individual tasks (Zhai et al., 2019). The key benefit is that the use of MTL can reduce the number of parame-\nters needed to support different models required to solve a problem. This leads to smaller task-specific models and\nless code to be adapted to the production environment, reducing the overhead of potential changes in existing data\npipelines. Reusing data, code, and models can save time and simplify the model deployment process. Next, the mod-\nular design of MTL architectures makes it easier to work with by enabling code reuse. For example, the fully-shared\nencoder architecture (described in Section 3.2.1) reuses the encoder between multiple task-specific heads. Moreover,\nfreezing the shared encoder would allow for working on separate tasks independently and simultaneously, making\ncross-team collaboration easier and more efficient. This idea was used in the HydraNet MTL model by the Tesla AI\nteam (Karpathy, 2021). However, freezing an encoder and updating only the task-specific heads may decrease perfor-\nmance. Additionally, the concept behind the AdapterHub (described in Section 3.2.2) was designed to allow users to\nchoose from a set of adapter modules, combine them in their preferred way, and insert or replace them dynamically\ninto state-of-the-art pre-trained models. To conclude, the modular nature of MTL architectures has a positive impact\non making the incorporation of these architectures into ML software easier, making it simpler to develop, collaborate,\nconfigure, and integrate into deployment processes.\n4.4\nMODEL UPDATING THROUGH MULTIPLE ML LIFECYCLE PHASES\nOften it is necessary to update the ML model regularly after it has been deployed and is running in production, in\norder to keep it aligned with the most current changes in data and environment. The need for updating models is one\nof the most important requirements of ML production systems (Pacheco et al., 2018; Abdelkader, 2020; Lakshmanan\net al., 2020; Paleyes et al., 2022; Huyen, 2022; Wu & Xie, 2022; Nahar et al., 2022). In this section, we discuss\nthe challenges of model updating and how MTL approaches can alleviate these challenges. Challenges of different\n9\n Preprint.\nphases of the ML lifecycle can trigger the need for model updates. First, we identify these occurrences and discuss the\nactions they trigger. Then, we discuss how MTL approaches can alleviate these challenges and point out the current\nlimitations of these approaches.\nModel Updating Challenges. A distribution shift is one of the common reasons for the need to update models.\nDistribution shift refers to changes observed in the joint distribution of the input and output variables of an ML model\n(Ditzler et al., 2015). Two problems must be solved to address the distribution shift effectively. First, in the monitoring\nphase, a mechanism must be in place to detect changes in distribution or a drop in key performance indicators, which\nwill signal the need for an ML model update. Second, in the model development phase, there must be a way to\ncontinuously learn and update the model in response to signals from the monitoring phase.\nNew business requirements often arise, requiring the model to have new capabilities in addition to the existing ones.\nFor example, a named entity recognition model trained for 10 entity labels in news articles may require 5 new labels\nafter 3 months of use. New requirements are introduced during the business analysis phase, which also triggers the\nneed for an ML model update. Unlike the initial requirements in the project scoping phase, adding new requirements\nshould be possible without re-training the full model.\nPeriodic or scheduled re-training and continual learning are the most common approaches for adapting models to new\ndata and requirements. Periodic re-training refers to the process of re-training a model at a predetermined interval,\nregardless of whether there have been any changes to the data or the environment. The frequency of updating is\ndetermined beforehand and is typically based on the amount of data and the desired level of model performance.\nStriking a balance between updating the model regularly to maintain good performance and avoiding over-updating the\nmodel to minimize computational costs is crucial. It is important to note that different models may require different re-\ntraining schedules, making it a situation-dependent problem. Finding the optimal re-training schedule requires careful\nconsideration of the specific requirements and circumstances of each ML system. Continual learning, on the other\nhand, refers to the ability of an ML model to adapt to changes in the data and environment over time. This approach\ninvolves continuously monitoring the performance of the model and updating it as needed to ensure it remains accurate\nand up-to-date. The frequency of model updates in continual learning is determined dynamically based on the changes\nobserved in the data and environment.\nModel Updating Opportunities. Following Huyen (2022), we differentiate two types of model updates: data it-\neration and model iteration. Data iteration refers to updating the model with new data while keeping the model\narchitecture and features the same, while model iteration refers to adding new features to an existing model architec-\nture or changing the model architecture itself. To discuss the challenges and opportunities of MTL approaches for\nmodel updating, we consider two scenarios. First, periodic re-training is done every 6 or 12 months, with the goal\nof training each model to perform optimally using all available data. Due to the high economic cost, we assume this\ncannot be done more frequently. Second, between periodic retrains, there may be situations where data iteration is\nnecessary due to a distribution shift, or model iteration is required to extend model capabilities in response to a new\nbusiness requirement.\nWe believe that incorporating MTL approaches into the update scenario would be practical, in addition to the benefits\ndiscussed in Sections 4.1-4.3. MTL is particularly well-suited for periodic re-training scenarios where the objective\nis to obtain a single best-performing model on all tasks using all available data, instead of training individual models\nfor each task. However, the challenge lies in how MTL approaches can manage the second scenario where the MTL\nmodel must learn new tasks or domains sequentially over time. As a result, the same model must be able to learn in\nboth MTL and CL settings as needed, and we refer to this setting as Continual MTL (CMTL).\nContinual MTL. MTL and CL models both learn multiple tasks, however, MTL learns them simultaneously, while\nCL learns them incrementally. As mentioned in Section 2.3, the authors of Sun et al. (2020) combined CL and\nMTL to further improve pre-training. Although the approach enhanced performance on downstream tasks, it does\nnot address the real-world scenario in which an MTL model should be updated to support new downstream tasks or\nhandle distribution shifts. Moreover, the total number of sequential tasks must be known a priori for the algorithm\nto determine an efficient training schedule. We believe that the similarities between MTL and CL architectures could\nenable the construction of a CMTL model. For example, most MTL architectures in Section 3.2 are transformer-based\nMTL architectures with task-specific heads, which resemble the parameter-isolation architectures for the TIL setting\nin CL (Section 2.3). In TIL, the task identifier is available during both training and testing and is used to identify\ntask-specific parameters in multi-headed architectures, similar to MTL architectures (Ke & Liu, 2022). This similarity\ncan be seen in the use of adapter architectures (Section 3.2.2) in both MTL (Stickland & Murray, 2019; Pfeiffer et al.,\n2020c; He et al., 2021) and CL (Ke et al., 2021a;b), as well as in the use of hypernetworks (Section 3.2.3) in both\nMTL (Pilault et al., 2020; Mahabadi et al., 2021) and CL (Von Oswald et al., 2019; Jin et al., 2021).\n10\n Preprint.\nFigure 3: (A). Adapter-BERT (Houlsby et al., 2019) uses adapters in a transformer layer (Vaswani et al., 2017). Adapters are\n2-layer networks with skip-connections, added twice per layer. Only adapters (yellow) and layer norm (green) are trainable while\nother modules (grey) are frozen. (B). B-CL replaces adapters with CLA, containing a knowledge-sharing module (KSM) and task-\nspecific module (TSM), both with skip-connections. Image and modified caption are taken from (Ke et al., 2021b).\nFor example, if we consider the BERT layer in Figure 3, we can observe that the adapter layers for the MTL approach\nin Figure 3(A) are in the same positions as the CL adapters in Figure 3(B) (Ke et al., 2021b). To switch from MTL to\nCL we only need to change adapters. We believe that this similarity could be a promising direction for further research\non the development of a CMTL model, but proper evaluation will not be possible without a good benchmark.\nFinally, we believe that a benchmark that accurately represents the challenges of real-world systems could be beneficial\nfor both researchers and practitioners to evaluate CMTL models effectively. To accomplish this, we propose combining\nand temporarily ordering tasks to simulate periodic re-training and CL scenarios. The benchmark can be further refined\nto reflect varying frequencies of periodic re-training, as well as the frequency of incoming tasks and distribution shifts\nbetween re-training periods. Variation in these scenarios could better represent real-world situations, where a simpler\nCMTL model might be sufficient in some cases, while more advanced CMTL models would be needed to handle\nenvironmental changes in other situations.\n5\nCONCLUSION\nIn this paper, we reviewed transformer-based MTL approaches in NLP and explored the challenges and opportunities\nof those approaches in the context of the ML lifecycle. We discussed how MTL can be a possible solution addressing\nsome of the key challenges in data engineering, model development, deployment, and monitoring phases of the ML\nlifecycle, as compared to using multiple single-task models.\nWe also discussed the opportunities for applying MTL to alleviate challenges regarding model updating due to dis-\ntribution shifts or evolving real-world requirements, where the ability to learn multiple tasks simultaneously can be\nleveraged to periodically update the model in response to changes in data and environment. However, we also acknowl-\nedged the limitations of current MTL approaches to handle updates sequentially, where CL is required. To address\nthis, we proposed the concept of CMTL, which aims to combine the benefits of MTL and CL in a single model. We\nmotivated creating a benchmark for the proper evaluation of CMTL models, a benchmark that better represents the\nchallenges of production systems that can guide the development of those models.\nIn conclusion, MTL approaches offer many opportunities for improving the efficiency and performance of ML systems\nin different phases of the typical ML lifecycle. We believe that MTL will become an important part of practitioners\u2019\ntoolbox seeking to address the challenges in their ML systems as highlighted in this survey.\nREFERENCES\nHala Abdelkader.\nTowards robust production machine learning systems: Managing dataset shift.\nIn 2020 35th\nIEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 1164\u20131166. IEEE, 2020.\n11\n Preprint.\nStanley Ebhohimhen Abhadiomhen, Royransom Chimela Nzeh, Ernest Domanaanmwi Ganaa, Honour Chika\nNwagwu, George Emeka Okereke, and Sidheswar Routray. Supervised shallow multi-task learning: analysis of\nmethods. Neural Processing Letters, 54(3):2491\u20132508, 2022.\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet:\nMassive multi-task representations with pre-finetuning. In Proceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 5799\u20135811, 2021.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang,\nVinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task scaling for transfer learning. In\nInternational Conference on Learning Representations, 2021.\nRob Ashmore, Radu Calinescu, and Colin Paterson. Assuring the machine learning lifecycle: Desiderata, methods,\nand challenges. ACM Computing Surveys (CSUR), 54(5):1\u201339, 2021.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao,\nMing Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked language models for unified language model pre-\ntraining. In Hal Daum\u00b4e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine\nLearning, volume 119 of Proceedings of Machine Learning Research, pp. 642\u2013652. PMLR, 13\u201318 Jul 2020. URL\nhttps://proceedings.mlr.press/v119/bao20a.html.\nLucas Bernardi, Themistoklis Mavridis, and Pablo Estevez. 150 successful machine learning models: 6 lessons learned\nat booking. com. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &\ndata mining, pp. 1743\u20131751, 2019.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark\nChen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165,\n2020. URL https://arxiv.org/abs/2005.14165.\nRich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\nShijie Chen, Yu Zhang, and Qiang Yang. Multi-task learning in natural language processing: An overview. arXiv\npreprint arXiv:2109.09138, 2021.\nZhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine\nLearning, 12(3):1\u2013207, 2018.\nMichael Chui, Bryce Hall, Helen Mayhew, and Alex Singla. The state of ai in 2022\u2013and a half decade in review,\nDec 2022.\nURL https://www.mckinsey.com/capabilities/quantumblack/our-insights/\nthe-state-of-ai-in-2022-and-a-half-decade-in-review. Accessed: 2023-01-15.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling instruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022.\nChristopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don\u2019t take the easy way out: Ensemble based methods for\navoiding known dataset biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\n4069\u20134082, Hong Kong, China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/\nD19-1418. URL https://aclanthology.org/D19-1418.\nKevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. Bam! born-again\nmulti-task networks for natural language understanding. CoRR, abs/1907.04829, 2019b. URL http://arxiv.\norg/abs/1907.04829.\nMichael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796, 2020.\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u02c7s Leonardis, Gregory Slabaugh, and Tinne\nTuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern\nanalysis and machine intelligence, 44(7):3366\u20133385, 2021.\n12\n Preprint.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nN19-1423. URL https://aclanthology.org/N19-1423.\nJie Ding, Vahid Tarokh, and Yuhong Yang. Model selection techniques: An overview. IEEE Signal Processing\nMagazine, 35(6):16\u201334, 2018.\nGregory Ditzler, Manuel Roveri, Cesare Alippi, and Robi Polikar. Learning in nonstationary environments: A survey.\nIEEE Computational Intelligence Magazine, 10(4):12\u201325, 2015.\nAna Gonz\u00b4alez-Gardu\u02dcno and Anders S\u00f8gaard. Learning to predict readability using eye-movement data from natives\nand learners. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. doi: 10.1609/aaai.\nv32i1.11978. URL https://ojs.aaai.org/index.php/AAAI/article/view/11978.\nHan Guo, Ramakanth Pasunuru, and Mohit Bansal. Autosem: Automatic task selection and mixing in multi-task\nlearning. CoRR, abs/1904.04153, 2019. URL http://arxiv.org/abs/1904.04153.\nMichelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask\nlearning. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\nManish Gupta and Puneet Agrawal. Compression of deep learning models for text: A survey. ACM Transactions on\nKnowledge Discovery from Data (TKDD), 16(4):1\u201355, 2022.\nDavid Ha, Andrew Dai, and Quoc V. Le. Hypernetworks, 2016. URL https://arxiv.org/abs/1609.09106.\nMalay Haldar, Mustafa Abdool, Prashant Ramanathan, Tao Xu, Shulin Yang, Huizhong Duan, Qing Zhang, Nick\nBarrow-Williams, Bradley C Turnbull, Brendan M Collins, et al. Applying deep learning to airbnb search. In\nproceedings of the 25th ACM SIGKDD international conference on knowledge discovery & Data Mining, pp. 1927\u2013\n1935, 2019.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. PTR: prompt tuning with rules for text classifica-\ntion. CoRR, abs/2105.11259, 2021. URL https://arxiv.org/abs/2105.11259.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of\nparameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.\nYun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, Yaguang Li, Zhao Chen, Donald Metzler,\nHeng-Tze Cheng, and Ed H. Chi. HyperPrompt: Prompt-based task-conditioning of transformers. In Kamalika\nChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the\n39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,\npp. 8678\u20138690. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/he22f.html.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\nMona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on\nMachine Learning, pp. 2790\u20132799. PMLR, 2019.\nYen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning scenarios: A\ncategorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685.\nChip Huyen. Designing Machine Learning Systems. \u201d O\u2019Reilly Media, Inc.\u201d, 2022.\nS\u00b4ebastien Jean, Orhan Firat, and Melvin Johnson. Adaptive scheduling for multi-task learning. CoRR, abs/1909.06434,\n2019. URL http://arxiv.org/abs/1909.06434.\nXisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. Learn continually, generalize rapidly: lifelong\nknowledge accumulation for few-shot learning. arXiv preprint arXiv:2104.08808, 2021.\nLukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit.\nOne model to learn them all, 2017. URL https://arxiv.org/abs/1706.05137.\n13\n Preprint.\nAndrej Karpathy. HydraNets - Tesla AI Day 2021, 8 2021. URL https://www.youtube.com/watch?t=\n4284&v=j0z4FweCy4M&feature=youtu.be. Accessed: 2023-02-15.\nZixuan Ke and Bing Liu.\nContinual learning of natural language processing tasks: A survey.\narXiv preprint\narXiv:2211.12701, 2022.\nZixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu. Achieving forgetting prevention and knowledge transfer in\ncontinual learning. Advances in Neural Information Processing Systems, 34:22443\u201322456, 2021a.\nZixuan Ke, Hu Xu, and Bing Liu. Adapting bert for continual learning of a sequence of aspect sentiment classification\ntasks. arXiv preprint arXiv:2112.03271, 2021b.\nAlex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geom-\netry and semantics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2018.\nBeen Kim and Finale Doshi-Velez. Machine learning techniques for accountability. AI Magazine, 42(1):47\u201352, 2021.\nFrederic Kirstein, Jan Philip Wahle, Terry Ruas, and Bela Gipp. Analyzing multi-task learning for abstractive text\nsummarization. arXiv preprint arXiv:2210.14606, 2022.\nValliappa Lakshmanan, Sara Robinson, and Michael Munn. Machine learning design patterns. O\u2019Reilly Media, 2020.\nHae Beom Lee, Eunho Yang, and Sung Ju Hwang.\nDeep asymmetric multi-task feature learning.\nIn Jennifer\nDy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Research, pp. 2956\u20132964. PMLR, 10\u201315 Jul 2018. URL https:\n//proceedings.mlr.press/v80/lee18d.html.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. CoRR,\nabs/2104.08691, 2021. URL https://arxiv.org/abs/2104.08691.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy-\nanov, and Luke Zettlemoyer.\nBART: Denoising sequence-to-sequence pre-training for natural language gen-\neration, translation, and comprehension.\nIn Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 7871\u20137880, Online, July 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\nXiang Lisa Li and Percy Liang.\nPrefix-tuning: Optimizing continuous prompts for generation.\narXiv preprint\narXiv:2101.00190, 2021.\nXi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.\ncc/paper/2019/file/685bfde03eb646c27ed565881917c71c-Paper.pdf.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and\npredict: A systematic survey of prompting methods in natural language processing. CoRR, abs/2107.13586, 2021a.\nURL https://arxiv.org/abs/2107.13586.\nShengchao Liu, Yingyu Liang, and Anthony Gitter. Loss-balanced task weighting to reduce negative transfer in multi-\ntask learning. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):9977\u20139978, Jul. 2019a. doi: 10.\n1609/aaai.v33i01.33019977. URL https://ojs.aaai.org/index.php/AAAI/article/view/5125.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT understands, too.\nCoRR, abs/2103.10385, 2021b. URL https://arxiv.org/abs/2103.10385.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language\nunderstanding, 2019b. URL https://arxiv.org/abs/1901.11504.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692,\n2019c. URL http://arxiv.org/abs/1907.11692.\n14\n Preprint.\nMingsheng Long, ZHANGJIE CAO, Jianmin Wang, and Philip S Yu.\nLearning multiple tasks with mul-\ntilinear relationship networks.\nIn I. Guyon,\nU. Von Luxburg,\nS. Bengio,\nH. Wallach,\nR. Fergus,\nS. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30.\nCurran Associates, Inc., 2017.\nURL https://proceedings.neurips.cc/paper/2017/file/\n03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf.\nDavid Lopez-Paz and Marc' Aurelio Ranzato. Gradient episodic memory for continual learning. In I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/\npaper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient multi-task\nfine-tuning for transformers via shared hypernetworks, 2021. URL https://arxiv.org/abs/2106.04489.\nGary E Marchant. The growing gap between emerging technologies and the law. Springer, 2011.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multi-\ntask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and\nfairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1\u201335, 2021.\nNadia Nahar, Shurui Zhou, Grace Lewis, and Christian K\u00a8astner. Collaboration challenges in building ml-enabled\nsystems: Communication, documentation, engineering, and process.\nIn Proceedings of the 44th International\nConference on Software Engineering, pp. 413\u2013425, 2022.\nFannia Pacheco, Ernesto Exposito, Mathieu Gineste, Cedric Baudoin, and Jose Aguilar. Towards the deployment of\nmachine learning solutions in network traffic classification: A systematic survey. IEEE Communications Surveys &\nTutorials, 21(2):1988\u20132014, 2018.\nAndrei Paleyes, Raoul-Gabriel Urma, and Neil D Lawrence. Challenges in deploying machine learning: a survey of\ncase studies. ACM Computing Surveys, 55(6):1\u201329, 2022.\nMihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. In-BoXBART: Get\ninstructions into biomedical multi-task learning. In Findings of the Association for Computational Linguistics:\nNAACL 2022, pp. 112\u2013128, Seattle, United States, July 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.findings-naacl.10. URL https://aclanthology.org/2022.findings-naacl.10.\nLucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A. Zuluaga. Maximum roaming multi-task\nlearning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10):9331\u20139341, May 2021. doi: 10.\n1609/aaai.v35i10.17125. URL https://ojs.aaai.org/index.php/AAAI/article/view/17125.\nZhongyi Pei, Lin Liu, Chen Wang, and Jianmin Wang. Requirements engineering for machine learning: A review and\nreflection. In 2022 IEEE 30th International Requirements Engineering Conference Workshops (REW), pp. 166\u2013175.\nIEEE, 2022.\nFabio Petroni, Tim Rockt\u00a8aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.\nLanguage models as knowledge bases?\nIn Proceedings of the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pp. 2463\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:\n10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00a8uckl\u00b4e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-\ndestructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020a.\nJonas Pfeiffer, Andreas R\u00a8uckl\u00b4e, Clifton Poth, Aishwarya Kamath, Ivan Vuli\u00b4c, Sebastian Ruder, Kyunghyun Cho, and\nIryna Gurevych. Adapterhub: A framework for adapting transformers. arXiv preprint arXiv:2007.07779, 2020b.\nJonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. MAD-X: an adapter-based framework for multi-task\ncross-lingual transfer. CoRR, abs/2005.00052, 2020c. URL https://arxiv.org/abs/2005.00052.\nJonathan Pilault, Amine Elhattami, and Christopher Pal. Conditionally adaptive multi-task learning: Improving trans-\nfer learning in nlp using fewer parameters & less data, 2020. URL https://arxiv.org/abs/2009.09139.\n15\n Preprint.\nEugenia Politou, Efthimios Alepis, and Constantinos Patsakis. Forgetting personal data and revoking consent under\nthe gdpr: Challenges and proposed solutions. Journal of cybersecurity, 4(1):tyy001, 2018.\nNeoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich. Data lifecycle challenges in production\nmachine learning: a survey. ACM SIGMOD Record, 47(2):17\u201328, 2018.\nSubhojeet Pramanik, Priyanka Agrawal, and Aman Hussain. Omninet: A unified architecture for multi-modal multi-\ntask learning. CoRR, abs/1907.07804, 2019. URL http://arxiv.org/abs/1907.07804.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nPeter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\nRes., 21(140):1\u201367, 2020.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters.\nAdvances in neural information processing systems, 30, 2017.\nKui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. Adversarial attacks and defenses in deep learning. Engineering, 6\n(3):346\u2013360, 2020.\nCedric Renggli, Bojan Karla\u02c7s, Bolin Ding, Feng Liu, Kevin Schawinski, Wentao Wu, and Ce Zhang. Continuous\nintegration of machine learning models with ease. ml/ci: Towards a rigorous yet practical treatment. Proceedings\nof Machine Learning and Systems, 1:322\u2013333, 2019.\nIshai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Adversarial machine learning attacks and defense\nmethods in the cyber security domain. ACM Computing Surveys (CSUR), 54(5):1\u201336, 2021.\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.\nSebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Sluice networks: Learning what to share\nbetween loosely related tasks. arXiv preprint arXiv:1705.08142, 2, 2017.\nRahul Manohar Samant, Mrinal Bachute, Shilpa Gite, and Ketan Kotecha. Framework for deep learning-based lan-\nguage models using multi-task learning in natural language understanding: A systematic literature review and future\ndirections. IEEE Access, 2022.\nNithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. \u201cevery-\none wants to do the model work, not the data work\u201d: Data cascades in high-stakes ai. In proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems, pp. 1\u201315, 2021.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Ar-\nnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma\nSharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike\nTian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted\ntraining enables zero-shot task generalization, 2021. URL https://arxiv.org/abs/2110.08207.\nTim Schr\u00a8oder and Michael Schulz. Monitoring machine learning models: a categorization of challenges and methods.\nData Science and Management, 5(3):105\u2013116, 2022.\nDavid Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael\nYoung, Jean-Francois Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. Advances in\nneural information processing systems, 28, 2015.\nJoan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard\nattention to the task. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference\non Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4548\u20134557. PMLR, 10\u201315\nJul 2018. URL https://proceedings.mlr.press/v80/serra18a.html.\nOr Sharir, Barak Peleg, and Yoav Shoham. The cost of training nlp models: A concise overview. arXiv preprint\narXiv:2004.08900, 2020.\n16\n Preprint.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh.\nAutoPrompt: Eliciting\nKnowledge from Language Models with Automatically Generated Prompts.\nIn Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222\u20134235, Online, Novem-\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https:\n//aclanthology.org/2020.emnlp-main.346.\nAyan Sinha, Zhao Chen, Vijay Badrinarayanan, and Andrew Rabinovich. Gradient adversarial training of neural\nnetworks. CoRR, abs/1806.08028, 2018. URL http://arxiv.org/abs/1806.08028.\nTrevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should\nbe learned together in multi-task learning?\nIn Hal Daum\u00b4e III and Aarti Singh (eds.), Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.\n9120\u20139132. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/standley20a.\nhtml.\nAsa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efficient adaptation in multi-task\nlearning. In International Conference on Machine Learning, pp. 5986\u20135995. PMLR, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning\nresearch. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 13693\u201313696, 2020.\nRuo-Yu Sun. Optimization for deep learning: An overview. Journal of the Operations Research Society of China, 8\n(2):249\u2013294, 2020.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: A continual pre-\ntraining framework for language understanding. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 8968\u20138975, 2020.\nHironori Takeuchi and Shuichiro Yamamoto. Business analysis method for constructing business\u2013ai alignment model.\nProcedia Computer Science, 176:1312\u20131321, 2020.\nYi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid: Efficient multi-task transformers with\ngrid-wise decomposable hyper projections. CoRR, abs/2007.05891, 2020. URL https://arxiv.org/abs/\n2007.05891.\nKim-Han Thung and Chong-Yaw Wee. A brief review on multi-task learning. Multimedia Tools and Applications, 77\n(22):29705\u201329725, 2018.\nRicha Upadhyay, Ronald Phlypo, Rajkumar Saini, and Marcus Liwicki. Sharing to learn and learning to share-fitting\ntogether meta-learning, multi-task learning, and transfer learning: A meta review. arXiv preprint arXiv:2111.12146,\n2021.\nPartoo Vafaeikia, Khashayar Namdar, and Farzad Khalvati. A brief review of deep multi-task learning and auxiliary\ntask learning. arXiv preprint arXiv:2007.01126, 2020.\nSimon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool.\nMulti-task learning for dense prediction tasks: A survey.\nIEEE transactions on pattern analysis and machine\nintelligence, 2021.\nManasi Vartak and Samuel Madden. Modeldb: Opportunities and challenges in managing machine learning models.\nIEEE Data Eng. Bull., 41(4):16\u201325, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\nJohannes Von Oswald, Christian Henning, Jo\u02dcao Sacramento, and Benjamin F Grewe. Continual learning with hyper-\nnetworks. arXiv preprint arXiv:1906.00695, 2019.\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke,\nSubhransu Maji, and Mohit Iyyer.\nExploring and predicting transferability across NLP tasks.\nCoRR,\nabs/2005.00770, 2020. URL https://arxiv.org/abs/2005.00770.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n17\n Preprint.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in\nneural information processing systems, 32, 2019.\nLiwen Wang, Rumei Li, Yang Yan, Yuanmeng Yan, Sirui Wang, Wei Wu, and Weiran Xu. Instructionner: A multi-\ntask instruction-based generative framework for few-shot ner, 2022. URL https://arxiv.org/abs/2203.\n03903.\nSinong Wang, Madian Khabsa, and Hao Ma. To pretrain or not to pretrain: Examining the benefits of pretraining on\nresource rich tasks. arXiv preprint arXiv:2006.08671, 2020.\nSteven Euijong Whang, Yuji Roh, Hwanjun Song, and Jae-Gil Lee. Data collection and quality challenges in deep\nlearning: A data-centric ai perspective. The VLDB Journal, pp. 1\u201323, 2023.\nJoseph Worsham and Jugal Kalita. Multi-task learning for natural language processing in the 2020s: where are we\ngoing? Pattern Recognition Letters, 136:120\u2013126, 2020.\nNan Wu and Yuan Xie. A survey of machine learning for computer architecture and systems. ACM Computing Surveys\n(CSUR), 55(3):1\u201339, 2022.\nSen Wu, Hongyang R. Zhang, and Christopher R\u00b4e. Understanding and improving information transfer in multi-task\nlearning, 2020. URL https://arxiv.org/abs/2005.00944.\nChenyang Yang, Rachel Brower-Sinning, Grace A Lewis, Christian K\u00a8astner, and Tongshuang Wu. Capabilities for\nbetter ml engineering. arXiv preprint arXiv:2211.06409, 2022.\nYongxin Yang and Timothy M. Hospedales. Trace norm regularised deep multi-task learning. CoRR, abs/1606.04038,\n2016. URL http://arxiv.org/abs/1606.04038.\nAmir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:\nDisentangling task transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2018.\nAndrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg. Learning a unified embedding\nfor visual search at pinterest. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pp. 2412\u20132420, 2019.\nYu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017.\nYu Zhang and Qiang Yang. An overview of multi-task learning. National Science Review, 5(1):30\u201343, 2018.\nZhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, and Meng Jiang. A survey of multi-task learning in natural\nlanguage processing: Regarding task relatedness and training methods. In Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computational Linguistics, pp. 943\u2013956, Dubrovnik, Croatia, May 2023.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.66.\nFeng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao Yu, Feiyue Huang, and Rongrong Ji.\nPyramidal person re-identification via multi-loss dynamic training. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2019.\nWenxuan Zhou.\nAn overview of models and methods for multi-task learning, Oct 2019.\nURL https:\n//shanzhenren.github.io/csci-699-replnlp-2019fall/lectures/W6-L1-Multi_Task_\nLearning.pdf.\nA\nADDITIONAL DETAILS FOR RELATED SURVEYS\nA.1\nMULTI-TASK LEARNING SURVEYS\nThe idea of MTL has been explored in many research studies. In this section, we provide an overview of related\nwork in MTL surveys and address different MTL aspects that were discussed. In Table 3, we list certain aspects and\nindicate the survey in which they were discussed. In the rest of the section, we just mention related surveys and go\nover individual MTL aspects (shown in bold) in more detail.\n18\n Preprint.\nTable 3: Discussed aspects per MTL survey. Aspects are indicated in bold.\nYear\nMTL Survey\n2017\n1 - (Ruder, 2017) 2 - (Zhang & Yang, 2017)\n2018\n3 - (Zhang & Yang, 2018) 4 - (Thung & Wee, 2018)\n2019\n5 - (Zhou, 2019)\n2020\n6 - (Vafaeikia et al., 2020) 7 - (Worsham & Kalita, 2020) 8 - (Crawshaw, 2020)\n2021\n9 - (Vandenhende et al., 2021) 10 - (Chen et al., 2021) 11 - (Upadhyay et al., 2021)\n2022\n12 - (Samant et al., 2022) 13 - (Abhadiomhen et al., 2022)\n2023\n14 - (Zhang et al., 2023)\nAspect \\Survey\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nComputational Model\nTraditional ML\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nDeep Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nLearning Type\nJoint Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nAuxiliary Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nArchitectures\nTaxonomy\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nLearning to Share\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nUniversal Models\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nOptimization\nLoss Weighting\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nRegularization\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTask Scheduling\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nGradient Modulation\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nKnowledge Distillation\n\u2713\n\u2713\n\u2713\n\u2713\nMulti-Objective Optimization\n\u2713\n\u2713\n\u2713\nTask Relationship Learning\nTask Grouping\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nRelationships Transfer\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTask Embeddings\n\u2713\n\u2713\nSupervision Level\nSupervised Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nSemi-supervised Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nSelf-supervised Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nConnection to Learning Paradigm\nReinforcement Learning\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTransfer Learning\n\u2713\n\u2713\n\u2713\nDomain Adaptation\n\u2713\n\u2713\n\u2713\nMeta-Learning\n\u2713\n\u2713\n\u2713\nActive Learning\n\u2713\n\u2713\nOnline Learning\n\u2713\n\u2713\n\u2713\nContinual Learning\nBenchmarks\nBenchmark Overview\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nModel Comparison\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nApplication Domain\nNatural Language Processing\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nComputer Vision\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nHealthcare\n\u2713\n\u2713\n\u2713\nBioinformatics\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nOther\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nMany application domains were studied in previous work, ranging from surveys covering multiple domains (Ruder,\n2017; Zhang & Yang, 2017; 2018; Thung & Wee, 2018; Vafaeikia et al., 2020; Crawshaw, 2020; Upadhyay et al.,\n2021; Abhadiomhen et al., 2022), to those dedicated to a specific domain, such as computer vision (Vandenhende\net al., 2021) or natural language processing (Zhou, 2019; Worsham & Kalita, 2020; Chen et al., 2021; Samant et al.,\n2022; Zhang et al., 2023). Both traditional ML and deep learning computational models were studied. The traditional\nML was discussed primarily in older studies, while deep learning was represented in all but one study. Furthermore,\n19\n Preprint.\nmost prior works provided an overview of the benchmarks for the specific domain (McCann et al., 2018; Wang et al.,\n2019), and compare models on them.\nLearning types. Two learning types were mainly discussed. First, joint learning was used in an MTL setting where\nall the tasks are equally important (Kendall et al., 2018; Liu et al., 2019b). Here, the goal is to achieve an on-par\nperformance compared to their single-task learning (STL) counterparts. Second, auxiliary learning was used when\nthere was a single main task or a set of them, while auxiliary tasks are only used to improve the performance of\nthe main tasks (Gonz\u00b4alez-Gardu\u02dcno & S\u00f8gaard, 2018; Wang et al., 2022). Although the difference between the two\nlearning types can be made, sometimes auxiliary learning was not differentiated from joint learning.\nArchitectures. MTL model architectures were among the most discussed aspects of MTL and one of the first aspects\ntackled in previous work. Distinguishing between hard and soft parameter sharing (Ruder, 2017) is the most used\narchitecture taxonomy. In recent surveys, the method of sharing parameters between tasks has improved, leading to\nthe refinement of taxonomies to categorize architectures more precisely (Crawshaw, 2020; Chen et al., 2021). Next,\nsome MTL architectures were categorized as learning-to-share approaches (Ruder et al., 2017). Those works argue\nthat it is better to learn parameter sharing in architectures for MTL rather than hand-design where sharing happens,\nas it offers a more adaptive solution to accommodate task similarities at different parts of the network. Additionally,\nsome MTL architectures were categorized as universal models which can handle multiple modalities, domains, and\ntasks with a single model (Kaiser et al., 2017; Pramanik et al., 2019).\nOptimization. Optimization techniques for MTL architectures were also discussed in detail. To start with, the most\ncommon approach to mitigating MTL challenges is loss weighting. Computing weights of task-specific losses is\ncrucial, as it helps optimize the loss function and considers the relative importance of each task. There are various\napproaches to computing the loss weights dynamically, including weighting by uncertainty (Kendall et al., 2018),\nlearning speed (Liu et al., 2019a; Zheng et al., 2019), or performance (Guo et al., 2018; Jean et al., 2019), among others.\nNext, and closely related to weighting task losses, is a task scheduling problem that involves choosing tasks to train on\nat each step. Many techniques were used, from simple ones that employ uniform or proportional task sampling, to the\nmore complicated ones, such as annealed sampling (Stickland & Murray, 2019) or approaches based on active learning\n(Pilault et al., 2020). Regularization approaches were also analyzed. Methods include (1) minimizing the L2 norm\nbetween the parameters of soft-parameter sharing models (Yang & Hospedales, 2016), (2) placing prior distributions\non the network parameters (Long et al., 2017), (3) introducing an auto-encoder term to the objective function (Lee\net al., 2018), (4) MTL variant of dropout (Pascal et al., 2021), and others. To continue, gradient modulation techniques\nwere employed to mitigate the problem of negative transfer by manipulating gradients of contradictory tasks, either\nthrough adversarial training (Sinha et al., 2018) or by replacing the gradient by its modified version (Lopez-Paz &\nRanzato, 2017). Another approach for optimizing MTL models is by applying knowledge distillation (Clark et al.,\n2019b). Finally, multi-objective optimization has been applied to the MTL setting to obtain a set of Pareto optimal\nsolutions on the Pareto frontier, providing greater flexibility in balancing trade-offs between tasks (Lin et al., 2019).\nTask relationship learning. The approach in MTL that focuses on learning the explicit representation of tasks or\nrelationships between them is task relationship learning. This approach consists of three main categories of methods.\nFirst, task grouping aims to divide a set of tasks into groups in order to maximize knowledge sharing during joint train-\ning (Standley et al., 2020). Second, transfer relationship learning involves methods that determine when transferring\nknowledge from one task to another will be beneficial for joint learning (Zamir et al., 2018; Guo et al., 2019). Finally,\ntask embedding methods aim to learn an embedding space for the tasks themselves (Vu et al., 2020).\nAs for the supervision level, most studies focused on supervised approaches. Some studies analyzed semi-supervised\napproaches that incorporate self-supervised objectives, such as MLM. However, self-supervised approaches are mainly\ndiscussed in the context of pre-training.\nMost studies had made connections to other learning paradigms, including reinforcement learning, transfer learning\nwith a special emphasis on domain adaptation, meta-learning, and active and online learning. However, only Ruder\n(2017); Zhang & Yang (2017; 2018) explored the relationship between MTL and online learning in the context of\ntraditional ML. To the best of our knowledge, there had been no prior work systematically investigating connections\nbetween MTL and CL. We believe that a connection between MTL and CL represents a promising research direction,\nas we will motivate the importance of this connection in Section 4.\n20\n Preprint.\nB\nADDITIONAL MULTI-TASK LEARNING APPROACHES\nB.1\nPROMPTS\nPrompts embed a task in the input. The original input x is modified using a template into a textual string prompt\nx\u2032 that has some unfilled slots, and then the LM is used to fill the information to obtain a final string \u02c6x, from which\nthe final output y can be derived (Liu et al., 2021a). Prompting requires redesigning all the inputs and outputs in\norder to treat the tasks as text-to-text problems. The prompting approach proved to work the best in the zero- and\nfew-show scenarios, but the benefits dissipate in the high-resource settings (Parmar et al., 2022; Wang et al., 2022).\nAlso, performance scales well with an increase in model size (Lester et al., 2021), making this approach not accessible\nto everyone.\nAccording to Liu et al. (2021a), there are many flavours to prompting. First, models with different pre-training\nobjectives can be used \u2013 left-to-right LM (Brown et al., 2020), MLM (Liu et al., 2019c), prefix LM (Bao et al.,\n2020), or encoder-decoder one (Lewis et al., 2020). Prompts can be engineered in a cloze (Petroni et al., 2019) or\nprefix (Lester et al., 2021) shape, be hand-crafted (Brown et al., 2020) or automated, which can again be discrete (Shin\net al., 2020) or continuous (Lester et al., 2021). Answer engineering searches for an answer space and a mapping to\nthe original output by deciding the answer shape and choosing an answer design method. Furthermore, parameters can\nbe updated using different settings \u2013 tuning-free prompting (Brown et al., 2020), fixed-LM prompt tuning (Li & Liang,\n2021), fixed-prompt LM tuning (Raffel et al., 2020), and prompt+LM tuning (Liu et al., 2021b). Finally, training can\nbe done in a few/zero-shot (Brown et al., 2020) or full-data setting (Han et al., 2021). In the following paragraphs, we\ndiscuss some of the previous prompting works.\nHyperPrompt (He et al., 2022) is an approach that combines hypernetworks and prompts. The key idea is to prepend\ntask-conditioned trainable vectors to both keys and values of MHA sub-layer at every Transformer layer. These task-\nspecific attention feature maps are jointly trained with the task-agnostic representations. Key prompts interact with\nthe original query, enabling tokens to acquire task-specific semantics. Value prompts are prepended to the original\nvalue vector, serving as task-specific memories for MHA to retrieve information from. However, instead of having\na key/value prompt for each task and layer, authors initialize a global prompt P for each task. They apply two local\nhypernetworks hk/v (one for keys, the other for values) at each Transformer layer in order to project this prompt into\nactual task- and layer-specific key/value prompts. There are three variations examined in the paper \u2013 HyperPrompt-\nShare, -Sep, and -Global. HyperPrompt-Global proved to be the best, as it allows a flexible way to share information\nacross tasks and layers. It introduces task and layer embeddings, which are then fused into the layer-aware task\nembedding. This embedding is then an input for the global hypernetworks Hk/v used as a generator of local hyper-\nnetworks hk/v. These local hypernetworks then generate hyper-prompts using a global prompt P. Hyper-prompts are\nfinally prepended with the original keys and values in the MHA sub-layers. During training, no parameters are kept\nfrozen. They report better results compared to the competitive methods of HyperFormer++ (Mahabadi et al., 2021)\nand Prompt-Tuning (Lester et al., 2021).\nIn-BoXBART (Parmar et al., 2022) uses biomedical instructions which contain: (1) definition (core explanation and\ndetailed instructions about what needs to be done), (2) prompt (short explanation of the task), and (3) examples\n(input/output pairs with the explanation). Each instruction (effectively a prompt) is prepended to the input instances.\nA problem of too long instances arises.\nInstructionNER (Wang et al., 2022) enriches the inputs with task-specific instructions and answer options. Addition-\nally, two auxiliary tasks are introduced \u2013 entity extraction and entity typing, which directly help in solving a NER\ntask.\nSanh et al. (2021) allowed public to suggest prompts and came up with 2073 prompts for 177 datasets in total (12\nprompts per dataset on avarage). They shuffled and combined all the examples from the datasets prior to training.\nIn most of the cases, performance of their models improved as the number of training datasets increased. Moreover,\ntraining on more prompts per dataset resulted in a better and more robust generalization on unseen datasets. The\nmodels they trained are based on LM-adapted T5 model (Lester et al., 2021).\nPrefix tuning freezes the language model parameters and optimizes a small, continuous task-specific vector called\nprefix (Li & Liang, 2021). Consequently, only a prefix needs to be stored for each task, making prefix tuning modular\nand space-efficient. This approach can be applied solely to text generation models, such as GPT-2 (Radford et al.,\n2019) and BART (Lewis et al., 2020). They state the intuition that the context can influence the encoding of input x\nby guiding what to extract from x; and can influence the generation of output y by steering the next token distribution.\nThey optimize the prefix as continuous word embeddings, instead of optimizing over discrete tokens. The reason for\nthis is that the discrete prompt needs to match the real word embedding, resulting in a less expressive model.\n21\n Preprint.\nLester et al. (2021) used a fixed-length prompt of special tokens, where the embeddings of these tokens are updated.\nThis removes the requirement that the prompts are parametrized by the model and allows them to have their own\ntrainable parameters. They test three different initialization techniques \u2013 random uniform, sampled vocabulary, and\nclass label. LM-adaptation pre-training technique was used. Unlike adapters, which modify the actual function that\nacts on the input, prompt tuning adds new input representations that affect how input is processed, leaving the function\nfixed. They freeze the pre-trained model. Finally, they try prompt ensembling (one prompt per model, for each task),\nwhich showed improved performance compared to a single-prompt average.\nChallenges and Opportunities in ML Lifecycle. Prompts can also mitigate some of the challenges of different\nML lifecycle phases (see Table 2). To start with, in a fixed-prompt LM tuning or a prompt+LM tuning setting,\nknowledge is transferred from other tasks, which can be especially beneficial for low-resource tasks. However, in\nsuch a scenario, different optimization techniques need to be considered in order to avoid negative interference and\nother MTL drawbacks (Appendix A.1). As for the model selection, it is more of a challenge than an opportunity, as\nmodels based on prompts require large number of parameters to begin with in order to perform well. Model training\ncomplexity depends on the parameter update setting, with some settings requiring no (Brown et al., 2020) or only few\nparameter updates (Li & Liang, 2021). One of the benefits when using prompts is the ability to handle all tasks using\na single text-to-text model, regardless of the input and output encoding. When using prompts, the biggest challenge\nin model deployment is the model\u2019s size. Finally, model updating due to distribution shift, new data, or business\nrequirements (see Section 4.4) seems most plausible in the setting where prompts are continuous and tuned (Li &\nLiang, 2021). However, this has downsides, such as difficult optimization, non-monotonic performance change with\nregard to the number of parameters, and reserving a part of sequence length for adaptation (Hu et al., 2021).\n22\n"}, "Understanding Graph Convolutional Networks for Text Classification": {"authors": ["Soyeon Caren Han", "Zihan Yuan", "Kunze Wang", "Siqu Long", "Josiah Poon"], "title": "Understanding Graph Convolutional Networks for Text Classification", "url": "https://arxiv.org/pdf/2203.16060.pdf", "abstract": "Graph Convolutional Networks (GCN) have been effective at tasks that have rich relational structure and can preserve global structure information of a dataset in graph embeddings. Recently, many researchers focused on examining whether GCNs could handle different Natural Language Processing tasks, especially text classification. While applying GCNs to text classification is well-studied, its graph construction techniques, such as node/edge selection and their feature representation, and the optimal GCN learning mechanism in text classification is rather neglected. In this paper, we conduct a comprehensive analysis of the role of node and edge embeddings in a graph and its GCN learning techniques in text classification. Our analysis is the first of its kind and provides useful insights into the importance of each graph node/edge construction mechanism when applied at the GCN training/testing in different text classification benchmarks, as well as under its semi-supervised environment.", "arxiv_id": "2203.16060", "published_date": "2022-03-30", "year": 2022, "introduction": "Introduction After the rise of deep learning, text classi\ufb01cation models mostly applied sequence-based learning models, CNN or RNN, which mainly captures text features from local consecutive word sequences, but may easily ignore global word co-occurrence in a corpus which carries non-consecutive and long-distance semantics. Graph-based learning models are directly dealing with complex structured data and prioritising global features exploitation. Several recent research efforts on investigating Graph Convolutional Networks (GCN) on NLP tasks include application to text classi\ufb01cation (Huang et al. 2019; Yao, Mao, and Luo 2019; Liu et al. 2020). This is largely because they can analyse rich relational structure and preserve the global structure in graph embeddings. The GCN-based text learning should include two main phases: 1) graph construction from free text and 2) graph-based learning with the constructed graph. A straightforward manner of graph construction is to represent relationships between words/entities in the free text. Yao, Mao, and Luo (2019) proposed a text graph-based neural network, Copyright \u00a9 2022, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. * Equal contribution \u2020 Corresponding author (Caren.Han@sydney.edu.au) named TextGCN, the \ufb01rst corpus-level graph-based transductive text classi\ufb01cation model. In TextGCN, a single large textual graph is \ufb01rstly constructed based on the entire corpus with words and documents as nodes, and co-occurrence relationship between words and documents as edges. Then, a GCN is employed to learn on the constructed text graph. More recent studies applied extra contextual information, such as topic model (Huang et al. 2019), syntactic and semantic information (Liu et al. 2020), pre-trained language model (Zhang et al. 2020b) or utilised different information propagation mechanisms (Wu et al. 2019; Zhu and Koniusz 2021). We noticed that most studies only focus on either hyperparameter testing or performance comparison with other state-of-the-art text classi\ufb01cation baselines. It is still unclear what factors in textual graph construction or graph learning are having an impact on the GCN-based text classi\ufb01cation. Thus, \ufb01nding the optimal textual graph construction or learning mechanism itself, the two main phases for GCNbased text learning, remains a black box to us. Such observations and limitations lead to several important questions. First, the performance of GCN-based text learning methods is highly affected by the quality of the input graph, which covers the global structure and relations of an entire corpus or a whole dataset. Our \ufb01rst question is \u2018What is the best textual graph construction approach to understand and represent the whole textual corpus?\u2019. In a text corpus, we have two main components, documents and words, which can be used as nodes. Then, what feature/embedding is better to represent the node feature for the textual graph? And what edge (relation) information should be used between nodes? Secondly, we use GCN learning in order to capture information from the neighbours of each word or document node. Our second question would be \u2018How much larger of a neighborhood\u2019s information should be integrated in order to produce the better text classi\ufb01cation performance?\u2019 In other words, how many GCN layers should be stacked for the best performance on different text classi\ufb01cation tasks? In this work, we focus on answering the above questions. We report the effect of graph construction mechanisms by analysing the variants of de\ufb01ning main components in a graph, including nodes and edges. Then, we present a study to \ufb01gure out the effect of GCN learning layers by integrating a variant range of neighbourhoods\u2019 information. We conduct our evaluation on both a full corpus environment and a semiarXiv:2203.16060v1  [cs.CL]  30 Mar 2022 ", "conclusion": "", "full_text": "Understanding Graph Convolutional Networks for Text Classi\ufb01cation\nSoyeon Caren Han,1*\u2020 Zihan Yuan,2* Kunze Wang,2 Siqu Long,2 Josiah Poon1\nThe University of Sydney, NSW, Australia\n1 {caren.han, josiah.poon}@sydney.edu.au\n2 {zyua5587, kwan4418, slon6753}@uni.sydney.edu.au\nAbstract\nGraph Convolutional Networks (GCN) have been effective\nat tasks that have rich relational structure and can preserve\nglobal structure information of a dataset in graph embed-\ndings. Recently, many researchers focused on examining\nwhether GCNs could handle different Natural Language Pro-\ncessing tasks, especially text classi\ufb01cation. While applying\nGCNs to text classi\ufb01cation is well-studied, its graph construc-\ntion techniques, such as node/edge selection and their feature\nrepresentation, and the optimal GCN learning mechanism in\ntext classi\ufb01cation is rather neglected. In this paper, we con-\nduct a comprehensive analysis of the role of node and edge\nembeddings in a graph and its GCN learning techniques in\ntext classi\ufb01cation. Our analysis is the \ufb01rst of its kind and\nprovides useful insights into the importance of each graph\nnode/edge construction mechanism when applied at the GCN\ntraining/testing in different text classi\ufb01cation benchmarks, as\nwell as under its semi-supervised environment.\n1\nIntroduction\nAfter the rise of deep learning, text classi\ufb01cation models\nmostly applied sequence-based learning models, CNN or\nRNN, which mainly captures text features from local con-\nsecutive word sequences, but may easily ignore global word\nco-occurrence in a corpus which carries non-consecutive\nand long-distance semantics. Graph-based learning mod-\nels are directly dealing with complex structured data and\nprioritising global features exploitation. Several recent re-\nsearch efforts on investigating Graph Convolutional Net-\nworks (GCN) on NLP tasks include application to text clas-\nsi\ufb01cation (Huang et al. 2019; Yao, Mao, and Luo 2019; Liu\net al. 2020). This is largely because they can analyse rich re-\nlational structure and preserve the global structure in graph\nembeddings. The GCN-based text learning should include\ntwo main phases: 1) graph construction from free text and 2)\ngraph-based learning with the constructed graph. A straight-\nforward manner of graph construction is to represent rela-\ntionships between words/entities in the free text. Yao, Mao,\nand Luo (2019) proposed a text graph-based neural network,\nCopyright \u00a9 2022, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.\n* Equal contribution\n\u2020 Corresponding author (Caren.Han@sydney.edu.au)\nnamed TextGCN, the \ufb01rst corpus-level graph-based trans-\nductive text classi\ufb01cation model. In TextGCN, a single large\ntextual graph is \ufb01rstly constructed based on the entire cor-\npus with words and documents as nodes, and co-occurrence\nrelationship between words and documents as edges. Then,\na GCN is employed to learn on the constructed text graph.\nMore recent studies applied extra contextual information,\nsuch as topic model (Huang et al. 2019), syntactic and se-\nmantic information (Liu et al. 2020), pre-trained language\nmodel (Zhang et al. 2020b) or utilised different information\npropagation mechanisms (Wu et al. 2019; Zhu and Koniusz\n2021). We noticed that most studies only focus on either hy-\nperparameter testing or performance comparison with other\nstate-of-the-art text classi\ufb01cation baselines. It is still unclear\nwhat factors in textual graph construction or graph learn-\ning are having an impact on the GCN-based text classi\ufb01ca-\ntion. Thus, \ufb01nding the optimal textual graph construction or\nlearning mechanism itself, the two main phases for GCN-\nbased text learning, remains a black box to us. Such obser-\nvations and limitations lead to several important questions.\nFirst, the performance of GCN-based text learning methods\nis highly affected by the quality of the input graph, which\ncovers the global structure and relations of an entire cor-\npus or a whole dataset. Our \ufb01rst question is \u2018What is the\nbest textual graph construction approach to understand and\nrepresent the whole textual corpus?\u2019. In a text corpus, we\nhave two main components, documents and words, which\ncan be used as nodes. Then, what feature/embedding is bet-\nter to represent the node feature for the textual graph? And\nwhat edge (relation) information should be used between\nnodes? Secondly, we use GCN learning in order to capture\ninformation from the neighbours of each word or document\nnode. Our second question would be \u2018How much larger of a\nneighborhood\u2019s information should be integrated in order to\nproduce the better text classi\ufb01cation performance?\u2019 In other\nwords, how many GCN layers should be stacked for the best\nperformance on different text classi\ufb01cation tasks?\nIn this work, we focus on answering the above questions.\nWe report the effect of graph construction mechanisms by\nanalysing the variants of de\ufb01ning main components in a\ngraph, including nodes and edges. Then, we present a study\nto \ufb01gure out the effect of GCN learning layers by integrating\na variant range of neighbourhoods\u2019 information. We conduct\nour evaluation on both a full corpus environment and a semi-\narXiv:2203.16060v1  [cs.CL]  30 Mar 2022\n supervised limited environment. The full corpus environ-\nment is exactly the same as the original training-testing split\nof \ufb01ve widely used benchmark corpora including 20NG, R8,\nR52, Ohsumed and MR, tested by different GCN-based text\nclassi\ufb01cation studies (Yao, Mao, and Luo 2019; Liu et al.\n2020; Wu et al. 2019). The purpose of traditional GCN mod-\nels (Kipf and Welling 2016) is to solve semi-supervised clas-\nsi\ufb01cation tasks, so we test on a semi-supervised environment\nwith a very limited amount of labelled data. For this limited\nsetup, we use the above \ufb01ve widely used text classi\ufb01cation\ncorpora, as well as four low resource language document\nclassi\ufb01cation benchmarks (incl. Chinese, Korean, African).\nNote that this paper aims to analyse the graph construction\nand learning mechanism of GCN-based text classi\ufb01cation\nmodels when there are no extra resources. This is because\nhigh-quality resources are not always available, especially\nfor the low resource language or speci\ufb01c domain that re-\nquires expertise.\nIn summary, the main contributions are as follows:\n\u2022 We conduct a comprehensive analysis of the role of graph\nconstruction and learning in GCN-based text classi\ufb01ca-\ntion over \ufb01ve widely used benchmarks (full corpus envi-\nronment) and nine benchmarks (limited training environ-\nment), including low resource languages\n\u2022 We perform a comparative analysis of the accuracy per-\nformance of different node and edge constructions when\nGCN is applied for text classi\ufb01cations\n\u2022 We evaluate the performance of GCN-based Text Classi-\n\ufb01cation with different variants of GCN layer stacks\n\u2022 We make source code publicly available to encourage re-\nproduction of the results1\n2\nRelated Work\n2.1\nGraph Neural Networks in NLP\nGraph Neural Networks (GNN) have received increasing\nattention in the realm of semi-supervised learning (Kipf\nand Welling 2016; Li, Han, and Wu 2018). Bastings et al.\n(2017) took word representations produced based on syn-\ntactic dependency trees as graph nodes and applied them to\nGCN learning for machine translation. Tu et al. (2019) pro-\nposed a Heterogeneous Document-Entity graph and utilized\na GCN to do reasoning over the constructed graph for multi-\nhop reading comprehension problems. Cao et al. (2019) de-\nsigned a Multi-channel Graph Neural Network that learned\nthe alignment-oriented knowledge graph embeddings for en-\ntity alignment. Xu et al. (2019) extracted node features from\nneighbourhoods and applied dual graph-state LSTM net-\nworks to summarize graph local features and extracted in-\nteraction features between pairwise graphs for entity inter-\naction prediction. Zhang et al. (2020a) proposed automatic\nsentence graph learning and incorporated it with GCN for\nheadline generation. Dowlagar and Mamidi (2021) com-\nbined the GCN graph modelling with multi-headed attention\nfor code-mixed sentiment analysis.\nSome recent studies applied graph neural networks for\ntext classi\ufb01cation by exploring different approaches of graph\n1https://github.com/usydnlp/TextGCN analysis\nstructure construction learned from the text data. Henaff,\nBruna, and LeCun (2015) and Defferrard, Bresson, and Van-\ndergheynst (2016) simply viewed a document as a graph\nnode. Peng et al. (2018) proposed a sentence-based graph\nin order to solve a large-scale hierarchical text classi\ufb01cation\nproblem. Yao, Mao, and Luo (2019) constructed a large tex-\ntual graph with word and document nodes and edge features\nrepresented as co-occurrence statistics, PMI/TF-IDF values.\nSGC (Wu et al. 2019) and S2GC (Zhu and Koniusz 2021)\nconstructed a graph as TextGCN, but proposed different in-\nformation propagation approaches. Vashishth et al. (2019)\nincorporated syntactic/semantic information for word em-\nbedding training via GCNs. Liu et al. (2020) proposed mul-\ntiple aspect graphs constructed from external resources in\nterms of semantic, syntactic and sequential contextual in-\nformation, which are jointly trained. When faced with low\nresource text classi\ufb01cation problems, these approaches ei-\nther do not fully explore the latent structure within the cor-\npus data itself as they consider only the connections be-\ntween documents, or are not applicable due to lack of exter-\nnal resource. Most prior studies focused on either hyperpa-\nrameter testing or performance comparison with other state-\nof-the-art text classi\ufb01cation baselines. Distinct from these\nworks, we examine the important factors in two main phases\nof GCN-based text learning, textual graph construction and\ngraph learning because they have a critical impact on the\nGCN-based text classi\ufb01cation performance.\n3\nGCN-based Text Classi\ufb01cation\nWe consider the task of GCN-based text classi\ufb01cation with\nonly single-label classi\ufb01cation. Figure 1 visualises the ar-\nchitecture of typical graph-based text classi\ufb01cation models\nbased on the given corpus with no extra resources. There\nare various types of GCN-based Text Classi\ufb01cation mecha-\nnisms introduced in the \ufb01eld. Two commonly used mech-\nanisms are the corpus-level (Yao, Mao, and Luo 2019)\nand document/sentence-level GCN text classi\ufb01cation mod-\nels(Huang et al. 2019). In this work, we will focus on the\nformer models since it captures the global structure infor-\nmation of a corpus/entire dataset, whereas the latter mod-\nels consider only local-level information (from a single sen-\ntence/document). With the former approaches, we can anal-\nyse the rich relational structure and preserve the global struc-\nture of a graph. In this section, we give a brief overview of\nGCN and TextGCN, the \ufb01rst corpus-level GCN-based text\nclassi\ufb01cation model.\n3.1\nGraph Convolutional Networks\nGCN (Kipf and Welling 2016) is a multi-layer neural net-\nwork generalized from Convolutional Neural Networks,\nwhich directly operates on the graph-structured data and\nlearns representation vectors of nodes based on properties\nof their neighbourhoods. Formally, a GCN graph G is con-\nstructed as G = (V, E, A), where V (|V | = N) and E rep-\nresents the set of graph nodes and edges respectively while\nA \u2208 RN\u00d7N is the graph adjacency matrix. Based on the\nconstructed graph G, the GCN learning takes in the input\nmatrix H0 \u2208 RN\u00d7d0 containing initial d0-dimensional fea-\ntures of the N nodes in V and then conducts the propagation\n Figure 1: Text Classi\ufb01cation with GCN. A single large graph\nis constructed; words and documents appear as nodes, and\nco-occurrence between words and documents. Assume that\nwe have only three classes for the document classi\ufb01cation\nthrough layers based on the rule in equation (1), which for-\nmulates the propagation operation from layer l to the subse-\nquent layer (l + 1).\nH(l+1) = f(H(l), A) = \u03c3( \u02c6AH(l)W (l))\n(1)\nHere, \u02c6A = \u02dcD\u2212 1\n2 \u02dcA \u02dcD\u2212 1\n2 is the normalized symmetric ad-\njacency matrix \u02dcA = A + I (I refers to an identity matrix for\nincluding self-connection of nodes); \u02dcD is the diagonal node\ndegree matrix, i.e. \u02dcD(i, i) = \ufffd\nj \u02dcA(i, j); W (l) \u2208 Rdl\u00d7dl+1\ndenotes the layer-speci\ufb01c trainable weight matrix for the lth\nlayer (dl/dl+1 is the feature dimension of layer l/l + 1); \u03c3\nis a non-linear activation function such as ReLU or softmax,\nwhich can be different for a speci\ufb01c layer. The main focus\nof our analysis lies in exploring the role of node and edge\nthat constructs the graph G as well as the variants of GCN\nlearning techniques when applying to text classi\ufb01cation.\n3.2\nTextGCN\nInspired by a GCN, TextGCN (Yao, Mao, and Luo 2019)\nconstructs an entire corpus based graph, which uses all\nthe words and documents in the corpus as graph nodes\nand sets the word-word and word-document edges to pre-\nserve the global word co-occurrence and word-document\nrelations in the graph structure. Then, it would be mod-\nelled by GCN learning. The edge between each word pair\nis represented by the point-wise mutual information (PMI)\nvalue, normally used for measuring semantic similarity in\nthe Term-Sentence-Matrix. The word-document edge is cal-\nculated based on the Term Frequency-Inverse Document\nFrequency(TF-IDF) weight of the word in the document.\nThe constructed graph is fed into a two layer GCN as in\nequation (1) where the second layer node embeddings for\nboth word and document have the same size as the label set\nand are passed into a softmax classi\ufb01er for the output. The\ncross-entropy loss is then calculated over all labelled docu-\nments for training and optimization. Especially, they simply\nset the initial input word/document node features as one-hot\nvectors.\n4\nGCN Analysis on Text Classi\ufb01cation\n4.1\nGraph Node Construction Analysis\nFollowing TextGCN, we set all the words and documents\nin the corpus as our node set for graph G, i.e. the num-\nber of nodes |V | = N = D + M equals to the sum of\nFigure 2: Different Variants of Edge Construction in an en-\ntire corpus-based graph. Assume that we have four docu-\nments and three words.\nthe number of documents (corpus size D) and the number\nof unique words in the corpus (vocab size M). With those\nword and document nodes, we explore the role of initial\nnode representation in GCN-based text classi\ufb01cation mod-\nels with two commonly used input embedding types in NLP:\n1) one-hot and 2) BERT embeddings. 1) one-hot embed-\nding is the most widely used categorical input encoding ap-\nproach in traditional NLP. 2) Bert embedding is one of the\nmost popular contextual word embeddings so we generate\nword/document node representation. It includes each indi-\nvidual word embedding and a [CLS] token for representing\nsentence/document-level embedding.\nBDd = BERT(Dd)\n(2)\nHnodeDd = B[CLS]\nDd\n(3)\nHnodeWm =\nmin\nd\u2208DWm\n(BWm\nDd )\n(4)\nFor one-hot embedding, we simply set the feature matrix\nH0 as an identity matrix I \u2208 RN\u00d7N for the one-hot vector\ninput. For BERT embedding, the calculation for word and\ndocument nodes are illustrated in Equations (2)-(4). Con-\ncretely, we feed each document Dd into the BERT model\nas in equation (2), resulting in the sequence representation\nBDd. For example, a document Dd such as \u201cJohn feels\nhappy\u201d will result in the BDd as \u201cB[CLS]\nDd\nBJohn\nDd\nBfeels\nDd\nBhappy\nDd\nB[SEP ]\nDd\n\u201d. We directly take the [CLS] representation\nB[CLS]\nDd\nas the node embedding for Dd, as in equation (3).\nThen for a word Wm, we collect all the documents contain-\ning this word, denoted as DWm, and apply min pooling over\nall the BERT representation BWm\nDd\nfor this word from doc-\numents in DWm, as is illustrated in equation (4). The es-\nsential difference between these two types of embedding is\nthat one-hot embedding incorporates no external knowledge\nor semantic information but purely indicates which word or\ndocument in the corpus the node is representing. Compar-\natively, BERT embedding is the output representation from\nthe BERT model pretrained on large text corpus, which can\nimpart the common sense semantic information to the repre-\nsented nodes and differentiate each document node based on\nthe document-speci\ufb01c word context. The detailed analysis of\nthese two embedding types is provided in Section 6.1.\n 4.2\nGraph Edge Construction Analysis\nFor edge construction, we intend to fully analyse all pos-\nsible co-occurring relations between every two types of\nnodes, which no studies have yet explored. We utilise\ndocument-document edges in addition to word-word and\nword-document edges as can be seen in Figure 2. The\nconstruction details are provided in equation (5). Refer to\nthe TextGCN(Yao, Mao, and Luo 2019), we use the PMI\nvalue between word pairs as a word-word edge feature, and\nutilise Term Frequency-Inverse Document Frequency(TF-\nIDF) weight to weight word-document edges.\nPMI(i, j) = max(log p(i, j)\np(i)p(j), 0)\n(5)\np(i, j) = #W(i, j)\n#W\n(6)\np(i) = #W(i)\n#W\n(7)\n4.3\nGraph Learning\nAs mentioned before, GCN only captures information about\nthe immediate neighbours with one layer of graph convolu-\ntion. When multiple GCN hidden layers are stacked, infor-\nmation about larger neighbourhoods is integrated.\nWe adopt this GCN propagation rule in equation (1) for\nmodelling the constructed graph. Speci\ufb01cally, we explore a\nvarious number of hidden layers L \u2208 {1, 2, 3, 4, 5} to \ufb01nd\nthe optimal range of neighbours to be integrated. For the \ufb01rst\nl (l \u2208 {1, .., L \u2212 1}) layers, we apply ReLU as activation\nfunctions as in equation (10). Here H(1) is the initial feature\nmatrix of nodes using one-hot or BERT as described in Sec-\ntion 4.1. We set the last layer output for both word and docu-\nment node embeddings to have the same size as the label set\nand apply a softmax classi\ufb01er over the output as in equation\n(11). The cross-entropy loss is then calculated over all the la-\nbelled documents as in equation (12), in which YD is the set\nof document indices with labels available and F is the out-\nput feature dimension (equals to the number of classes). Y\ndenotes the label indicator matrix. We provide the analysis\nfor different numbers of hidden layers in Section 6.3.\nH(l+1) = ReLU( \u02c6AH(l)W (l))\n(8)\nZ = softmax( \u02c6AH(L)W ((L)))\n(9)\nL = \u2212\n\ufffd\nd\u2208YD\nF\n\ufffd\nf=1\nYdflnZdf\n(10)\n5\nExperiment setup\n5.1\nDatasets\nWe conduct the comprehensive analysis on both a full corpus\nenvironment and a semi-supervised limited environment.\nThe full environment is exactly the same as the original\ntraining-testing split of \ufb01ve widely used benchmark corpora\nincluding 20NG, R8, R52, Ohsumed and MR, followed by\ndifferent GCN-based text classi\ufb01cation studies (Yao, Mao,\nand Luo 2019; Liu et al. 2020; Wu et al. 2019). The limited\nenvironment aims to cover semi-supervised text classi\ufb01ca-\ntion with a very limited amount of labelled data. We ran-\ndomly sample 1% as a training set and use the remaining\n99% for testing on nine benchmarks, including the above\n\ufb01ve benchmarks and additional four low-resource language\n(incl. Chinese, Korean, African) document classi\ufb01cation\ndatasets2. 1)The 20NG contains 18,846 news documents in\ntotal, which are evenly categorized into 20 classes. 2)R8\nand 3)R52 (all-terms version) are subsets of the Reuters\n21578 dataset. R8 has 8 topic categories with 7,674 doc-\numents, and R52 is based on 52 categories with 9,100\ndocuments. 4)Ohsumed is collected from the MEDLINE,\nwhich is a bibliographic database of biomedical information.\nOnly single-label classi\ufb01cation task (7,400 documents) is se-\nlected. 5)MR is a binary sentiment (positive and negative)\nclassi\ufb01cation dataset, which includes 10,622 short movie re-\nview comments.\nThe following list shows four additional document clas-\nsi\ufb01cation benchmarks in low-resource languages, including\nChinese, Korean, and African. 6)Waimai is a binary senti-\nment analysis dataset collected from a Chinese online food\nordering platform, which provides 11,987 Chinese com-\nments about the food delivery service. 7)ChSenti contains\n7,766 Chinese documents of hotel service comment with\nbinary class. 8)KrHate provides 2,000 binary hate speech\ncomments collected from the Korean radical Anti-male on-\nline community, named Womad. 9)Xhosa is a Xhosa dataset\nfrom the NCHLT Text Corpora collected by South African\nDepartment of Arts and Culture & Centre for Text Technol-\nogy, which contains 4,000 documents of 11 categories.\n5.2\nImplementation Details\nGraph Node Setup We use one-hot and BERT embed-\nding for the analysis. The dimension of one-hot embed-\nding corresponds to the number of nodes. For BERT embed-\ndings, \u201cbert-base-uncased\u201d(for English-based) and \u201cbert-\nbase-multilingual-uncased\u201d(for non-English-based) devel-\noped by Hugging Face (Wolf et al. 2019) is used with the\ninput dimension of 768. Graph Edge Setup In order to con-\nstruct the edge, the window size is set to 20 for PMI calcula-\ntion (word-word edges). The threshold 0.2 is applied when\ncalculating Jaccard similarity measure for doc-doc edges.\nGraph Learning Setup Each hidden layer\u2019s dimension is\nde\ufb01ned as 200, and the dimension of output layer is the num-\nber of classes. The training hyperparameters include: 0.02\nas the learning rate; 0.5 as the dropout rate; 0 as the L2 loss\nweight; 200 as the maximum number of epochs with early\nstopping of 10 epochs. Adam (Kingma and Ba 2015) is used\nto train the model.\n2Dataset Links: 1)http://qwone.com/\u223cjason/20Newsgroups/\n2)3)https://www.cs.umb.edu/\u223csmimarog/textmining/datasets/\n4)http://disi.unitn.it/moschitti/corpora.htm 5)http://www.cs.c\nornell.edu/people/pabo/movie-review-data/ 6)https://github\n.com/SophonPlus/ChineseNlpCorpus/ 7)https://github.com/S\nophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSenti\nCorp htl all/intro.ipynb 8)https://www.kaggle.com/captainne\nmo9292/korean-extremist-website-womad-hate-speech-data\n9)https://github.com/praekelt/feersum-lid-shared-task\n 20NG\nR8\nR52\nOhsumed\nMR\nNode feature\nonehot\n0.8607\n0.9692\n0.9345\n0.6824\n0.7641\nBERT\n0.7206\n0.9510\n0.8440\n0.4618\n0.7821\nEdge feature\nd2w only\n0.8475\n0.9493\n0.9169\n0.6667\n0.7462\n+w2w\n0.8617\n0.9693\n0.9333\n0.6841\n0.7600\n+w2w+d2d\n0.8607\n0.9692\n0.9345\n0.6824\n0.7641\nTable 1: Test accuracy by different node and edge construc-\ntion variants on the full environment\n6\nDiscussion and Analysis\n6.1\nEffect of Node Embedding\nThe upper block in Table 1 shows the test accuracy by us-\ning either one-hot or BERT embeddings as initial node fea-\ntures on the \ufb01ve benchmark datasets under full environ-\nment. First, it can be seen that only MR achieves better re-\nsult with BERT embedding than one-hot embedding, which\nmight be attributed to the fact that MR as a sentiment analy-\nsis task bene\ufb01ts better from the general semantics learned\nfrom a large external text. In addition, for the other four\ndatasets with higher accuracy via one-hot embedding, we\nfound that datasets with larger size of classi\ufb01cation cate-\ngories, i.e. 20NG, R52, Ohsumed, tend to generate a bigger\nperformance gap between the embedding types compared to\nR8. We suppose that a smaller size of classi\ufb01cation category\nmay exert itself better on pretrained embeddings since it\ndoes not have suf\ufb01cient information to train the global infor-\nmation. We also provide the comparative results under lim-\nited environment on all the 9 datasets in Table 2 (Appendix).\nIt is reasonable to see an overall performance drop compared\nto the full setting. For the \ufb01rst \ufb01ve benchmark datasets, MR\nstill prefers BERT embedding while R8 changes the pref-\nerence to BERT from one-hot embedding. Both these have\na relatively small number of classi\ufb01cation categories. This\nfurther supports the claim that small-category datasets bene-\n\ufb01t more from BERT than large-category counterparts. When\nit comes to the low resource datasets (Waimai, ChSenti,\nKrHate, Xhosa), it can be seen that all of them produce better\naccuracy with one-hot embedding than BERT embedding,\nwhich might be due to the quality of pre-training on low\nresource language corpora. Note that we used the edge set\n(d2w+w2w+d2d) for the node construction testing since it\nproduces the overall highest performance in both full and\nlimited environment.\n6.2\nEffect of Edge Construction\nWe also analyse the usage of different edge features for both\nfull and limited environment in the bottom half of Table 1\nand 2 respectively. More speci\ufb01cally, three types of edge\nfeatures are evaluated: (1) d2w only, utilises only the word-\ndoc edges in the constructed graph; (2) +w2w, uses both\nword-doc and word-word edges; (3) +w2w+d2d, apply all\nthe three types of edges including doc-doc edges. Similar\npatterns can be found in both settings. Overall, a d2w-only\ngraph always results in the lowest performance, implying\ninsuf\ufb01cient global structural information conveyed by only\nthe word-document co-occurrence. With the w2w edge, the\nperformance increased in all datasets and the amount of in-\ncrease mostly varies around 0.01 to 0.04 in the two set-\ntings. In addition, d2w+w2w+d2d using all the three types\nof edges, further rises the accuracy for R52 and MR under\nfull environment and for most of the datasets under the lim-\nited environment. This highlights the bene\ufb01t of using full\nco-occurring relationships (with all types of edges) in a en-\ntire graph. The similar trend can be further observed with\nthe persistent spatial gap between the lowest blue line (d2w\nonly) and the other two lines in Figure 3 (Appendix). It illus-\ntrates the corresponding accuracy for the three edge features\non the \ufb01ve benchmark datasets when increasing the training\nproportion from extremely few labelled setting (1%) to 99%.\n6.3\nEffect of GCN Learning\nThe main aim of GCN learning is to capture information\nabout immediate neighbours with a layer of convolution.\nWhen multiple GCN layers are stacked, information from\nmuch larger neighbourhoods is extracted and integrated. In\nFigure 4 (Appendix), we conducted the text classi\ufb01cation\nevaluation to \ufb01nd the optimal range of neighbours\u2019 infor-\nmation about each node. We stacked 1 to 5 GCN layers\non different text classi\ufb01cation in the full environment. It\ncan be seen that the highest performance is achieved by us-\ning 2 GCN layers for all \ufb01ve datasets and the performance\ndrops down as the layer decreases or increases. This in-\ndicates capturing 2 levels of neighbourhood nodes is the\nbest and increasing the level of neighbourhoods will grad-\nually lead to indifferentiable node representation. 20NG and\nMR have a similar overall trend and perform more consis-\ntently in the three evaluation metrics. Comparatively, the\nother three datasets are observed to have much lower over-\nall Macro F1 than Accuracy/Weighted F1. Those trends can\nalso be found when switching from the full to the limited\nenvironment in Figure 5 (Appendix). Even though low re-\nsource language datasets have extremely few labelled data,\nit is still 2 layers that performs the best overall, shown in\nFigure 5 (Appendix). When layer number increases, the per-\nformance does not always decrease sharply as in previous\ncases. Speci\ufb01cally, performance of the two Chinese datasets\nChSenti and Waimai goes up again at 4 layers after the de-\ncrease at 3 layers. Xhosa only achieved a rather stable per-\nformance degradation when increasing from 2 to 5 layers. It\ncan be seen that different languages may preserve different\npatterns on the metrics with the change of GCN layers.\n7\nConclusions\nWe focused on understanding the underlying factors that\nmay in\ufb02uence GCN-based text classi\ufb01cation, and proposed\nto examine graph construction and graph learning mecha-\nnisms. We systematically examined the role of node and\nedge in a corpus-level textual graph, and found the opti-\nmal range of neighbours\u2019 information by testing the different\nnumber of GCN layer stacks. The empirical results of exper-\niments on various real datasets in both full environment and\nlimited training environment supported our analysis.\n A\nAppendix\nTable 2, Figure 3,4, and 5 can be found in the next page.\nOnce the paper is accepted, we will add those \ufb01gures and ta-\nbles in the main content with 6 pages (one additional page).\nReferences\nBastings, J.; Titov, I.; Aziz, W.; Marcheggiani, D.; and\nSima\u2019an, K. 2017.\nGraph Convolutional Encoders for\nSyntax-aware Neural Machine Translation. In Proceedings\nof the 2017 Conference on Empirical Methods in Natural\nLanguage Processing, 1957\u20131967.\nCao, Y.; Liu, Z.; Li, C.; Li, J.; and Chua, T.-S. 2019. Multi-\nChannel Graph Neural Network for Entity Alignment. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 1452\u20131461.\nDefferrard, M.; Bresson, X.; and Vandergheynst, P. 2016.\nConvolutional Neural Networks on Graphs with Fast Local-\nized Spectral Filtering. In NIPS.\nDowlagar, S.; and Mamidi, R. 2021. Graph convolutional\nnetworks with multi-headed attention for code-mixed sen-\ntiment analysis.\nIn Proceedings of the First Workshop\non Speech and Language Technologies for Dravidian Lan-\nguages, 65\u201372.\nHenaff, M.; Bruna, J.; and LeCun, Y. 2015. Deep convo-\nlutional networks on graph-structured data. arXiv preprint\narXiv:1506.05163.\nHuang, L.; Ma, D.; Li, S.; Zhang, X.; and Houfeng, W. 2019.\nText Level Graph Neural Network for Text Classi\ufb01cation.\nIn Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3435\u20133441.\nKingma, D. P.; and Ba, J. 2015.\nAdam: A Method for\nStochastic Optimization. CoRR, abs/1412.6980.\nKipf, T. N.; and Welling, M. 2016. Semi-Supervised Classi-\n\ufb01cation with Graph Convolutional Networks.\nLi, Q.; Han, Z.; and Wu, X.-M. 2018. Deeper insights into\ngraph convolutional networks for semi-supervised learning.\nIn Proceedings of the AAAI Conference on Arti\ufb01cial Intelli-\ngence, volume 32.\nLiu, X.; You, X.; Zhang, X.; Wu, J.; and Lv, P. 2020. Tensor\ngraph convolutional networks for text classi\ufb01cation. In Pro-\nceedings of the AAAI Conference on Arti\ufb01cial Intelligence,\nvolume 34, 8409\u20138416.\nPeng, H.; Li, J.; He, Y.; Liu, Y.; Bao, M.; Wang, L.; Song,\nY.; and Yang, Q. 2018. Large-scale hierarchical text clas-\nsi\ufb01cation with recursively regularized deep graph-cnn. In\nProceedings of the 2018 world wide web conference, 1063\u2013\n1072.\nTu, M.; Wang, G.; Huang, J.; Tang, Y.; He, X.; and Zhou, B.\n2019. Multi-hop Reading Comprehension across Multiple\nDocuments by Reasoning over Heterogeneous Graphs. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 2704\u20132713.\nVashishth, S.; Bhandari, M.; Yadav, P.; Rai, P.; Bhat-\ntacharyya, C.; and Talukdar, P. 2019.\nIncorporating Syn-\ntactic and Semantic Information in Word Embeddings using\nGraph Convolutional Networks. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 3308\u20133318.\nWolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu,\nJ.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest,\nQ.; and Rush, A. M. 2019.\nHuggingFace\u2019s Transform-\ners: State-of-the-art Natural Language Processing. ArXiv,\nabs/1910.03771.\nWu, F.; Souza, A.; Zhang, T.; Fifty, C.; Yu, T.; and Wein-\nberger, K. 2019. Simplifying graph convolutional networks.\nIn International conference on machine learning, 6861\u2013\n6871. PMLR.\nXu, N.; Wang, P.; Chen, L.; Tao, J.; and Zhao, J. 2019. MR-\nGNN: Multi-Resolution and Dual Graph Neural Network for\nPredicting Structured Entity Interactions. In IJCAI.\nYao, L.; Mao, C.; and Luo, Y. 2019.\nGraph convolu-\ntional networks for text classi\ufb01cation.\nIn Proceedings of\nthe AAAI Conference on Arti\ufb01cial Intelligence, volume 33,\n7370\u20137377.\nZhang, R.; Guo, J.; Fan, Y.; Lan, Y.; and Cheng, X. 2020a.\nStructure Learning for Headline Generation.\nIn Proceed-\nings of the AAAI Conference on Arti\ufb01cial Intelligence, vol-\nume 34, 9555\u20139562.\nZhang, Y.; Yu, X.; Cui, Z.; Wu, S.; Wen, Z.; and Wang, L.\n2020b. Every Document Owns Its Structure: Inductive Text\nClassi\ufb01cation via Graph Neural Networks. In Proceedings\nof the 58th Annual Meeting of the Association for Computa-\ntional Linguistics, 334\u2013339.\nZhu, H.; and Koniusz, P. 2021. Simple spectral graph con-\nvolution. In International Conference on Learning Repre-\nsentations.\n 20NG\nR8\nR52\nOhsumed\nMR\nWaimai\nChSenti\nKrHate\nXhosa\nNode feature\nonehot\n0.6937\n0.8973\n0.7990\n0.4092\n0.6178\n0.8301\n0.7548\n0.9048\n0.9952\nBERT\n0.6585\n0.9147\n0.7962\n0.3893\n0.7255\n0.8059\n0.6970\n0.8256\n0.7878\nEdge feature\nd2w only\n0.6239\n0.8698\n0.7776\n0.3906\n0.5995\n0.8149\n0.5297\n0.7445\n0.9874\n+w2w\n0.6680\n0.8949\n0.7978\n0.4099\n0.6158\n0.8283\n0.7410\n0.7609\n0.9953\n+w2w+d2d\n0.6937\n0.8973\n0.7990\n0.4092\n0.6178\n0.8301\n0.7548\n0.9048\n0.9952\nTable 2: Test accuracy by different node and edge construction variants on the limited environment\nFigure 3: Test accuracy by varying training data proportions (from 1% to 99%)\nFigure 4: Test performance by varying GCN hidden layer stacks on the full environment\nFigure 5: Test performance by varying GCN hidden layer stacks on the limited environment\n"}, "Total Recall, Language Processing, and Software Engineering": {"authors": ["Zhe Yu", "Tim Menzies"], "title": "Total Recall, Language Processing, and Software Engineering", "url": "https://arxiv.org/pdf/1809.00039.pdf", "abstract": "A broad class of software engineering problems can be generalized as the \"total recall problem\". This short paper claims that identifying and exploring total recall language processing problems in software engineering is an important task with wide applicability.   To make that case, we show that by applying and adapting the state of the art active learning and text mining, solutions of the total recall problem, can help solve two important software engineering tasks: (a) supporting large literature reviews and (b) identifying software security vulnerabilities. Furthermore, we conjecture that (c) test case prioritization and (d) static warning identification can also be categorized as the total recall problem.   The widespread applicability of \"total recall\" to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing, but a wide range of important software engineering tasks.", "arxiv_id": "1809.00039", "published_date": "2018-08-31", "year": 2018, "introduction": "INTRODUCTION Software engineering is a discipline closely involved with human activities. How to help software developers produce better software more efficiently is the core topic of software engineering. Prioritizing tasks can efficiently reduce the human efforts and time required to achieve certain goals of software development. Many of such prioritization problems in software engineering can be generalized as the total recall problem (defined in the next section). The total recall problem has been explored in information retrieval for years, and the state of the art solution with active learning and natural language processing aims to resolve the following challenges: \u2022 Among a finite number of tasks, which task to be executed first so that certain goals can be achieved earlier? \u2022 At what point, there is no need to execute the remaining tasks? \u2022 Are all the tasks executed correctly? How to identify wrongly executed tasks and correct them? ACM acknowledges that this contribution was authored or co-authored by an employee, contractor, or affiliate of the United States government. As such, the United States government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for government purposes only. NL4SE@ ESEC/FSE 2018, 4th November, 2018, Lake Buena Vista, Florida \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/0000001.0000001 \u2022 How to scale up the process with multiple humans working in parallel (e.g. crowdsourcing)? The first challenge has been extensively explored [1, 10, 14] while the rest three have much room to improve [2\u20134]. This short paper claims that identifying and exploring the total recall problems in software engineering is an important task which benefits both software engineering research and the general solution to total recall problems. To support this claim, in \u00a72 we first introduce the general total recall problem, and its state of the art solutions; then in \u00a73, we provide two software engineering tasks which are solved by applying total recall techniques and better solutions to the challenges of total recall problems were created during the process [16\u201318]. We also conjecture that two other software engineering tasks can be categorized as the total recall problem. The goal of this short paper is to inspire software engineering researchers to explore total recall problems in software engineering with our preliminary results. 2 THE TOTAL RECALL PROBLEM The total recall problem in information retrieval aims to optimize the cost for achieving very high recall\u2013as close as practicable to 100%\u2013with a human assessor in the loop [6]. More specifically, the total recall problem can be described as following: Given a set of candidate examples E, in which only a small fraction R \u2282 E are positive, each example x can be inspected to reveal its label as positive (x \u2208 R) or negative (x \ufffd R) at a cost. Starting with the labeled set L = \u2205, the task is to inspect and label as few examples as possible (min |L|) while achieving very high recall |L \u2229 R|/|R|. The Total Recall Problem: Different strategies have been applied to solve the total recall problem, including supervised learning and semi-supervised learning. However, the state of the art solutions to the total recall problem apply active learning [6] to learn from natural language processing features (e.g. bag-of-words, tf-idf) extracted from the current labeled set L and re-rank the rest of the candidate examples E \\ L so that examples that are more likely to be positive get inspected next. This active learning solution has been proven effective in different use cases. A demonstration of the benefit of this active learning strategy can be found in Figure 1, where with active learning (solid red line), high recall (close to 100%) can be achieved by inspecting only a small portion of the candidate examples. In evidence-based medicine, researchers screen titles and abstracts to determine whether one paper should be included in a certain systematic review. Wallace et al. [14] designed patient active learning to help researchers prioritize their effort on papers to be included. With patient active learning, half of the screening effort can be saved while still including most relevant papers [14]. arXiv:1809.00039v1  [cs.SE]  31 Aug 2018 NL4SE@ ESEC/FSE 2018, 4th November, 2018, Lake Buena Vista, Florida Z.Yu, T.Menzies 0% 20% 40% 60% 80% Cost 0.0 0.2 0.4 0.6 0.8 1.0 Recall Active Learning Random Order Figure 1: A demonstration for the power of active learning on the total recall problem. X axis shows the cost spent while Y axis shows the recall reached. The dashed line represents the retrieval curve with random inspection order and the solid line shows the retrieval curve with active learning selecting which example to inspect next. In electronic discovery, attorneys are hired to review massive amount of documents looking for relevant ones for a certain legal case and provide those as evidences. Cormack and Grossman [1] proposed continuous active learning to save attorneys\u2019 effort from reviewing non-relevant documents, which further can save a large amount of the cost of legal cases. The active learning strategy for total recall problem can be described as following: Step 0 Given a candidate set E and initialize the labeled set L = \u2205. Step 1 Using strategies like ad hoc search or random sampling to select next example x to label (L \u2190 L \u222a x). Step 2 Repeat Step 1 until \u201cenough\u201d positive examples have been labeled (|L \u2229 R| \u2265 K). Step 3 Train/update a supervised learning model with current labeled set L. Step 4 Use the trained model to predict on the unlabeled set E \\ L and select next example x to label (L \u2190 L \u222a x). Step 5 Repeat Step 3 and Step 4 until a stopping rule has been met. Where detail settings vary from case studies to case studies: \u2022 In Step 2, Cormack and Grossman [1] believe that learning should start as soon as possible (K = 1) while Wallace et al. [14] suggest to start learning until more training examples are available. \u2022 In Step 3, most studies exact text features from examples and train a support vector machine with linear kernel. However, there are different opinions on how to balance the training data, e.g. Wallace et al. [14] proposed a technique called aggressive undersampling to drop off the negative examples closes to the positive ones while Miwa et al. [10] adjusted the weight of each training example to punish more for misclassifying positive examples. \u2022 In Step 4, Cormack and Grossman [1] select examples that are most likely to be positive while Wallace et al. [14] select most uncertain examples. \u2022 In Step 5, Cormack and Grossman [1] stop the process when a sufficient number of positive examples have been found while Wallace et al. [14] stop the learning when the model becomes stable and then apply the model to classify the unlabeled examples. While this active learning and natural language processing strategy has been extensively explored to resolve the main challenge of total recall problems, there still exists large room for improvement in the three other challenges. 2.1 When to Stop In practice, there is no way to know the number of positive examples before inspecting and labeling every candidate example. It is thus impossible to know exactly what recall has been reached during the process. Then, how do we know when to stop if the target is reaching, say 95% recall? This is a practical problem that directly affects the usability of the active learning strategy. Stopping too early will result in missing many valuable positive examples; while stopping too late will cause unnecessary cost when there are no more positive examples to retrieve. So far, researchers have developed various stopping rules to solve the \u201cwhen to stop\u201d problem. \u2022 Ros et al. [12] developed the the most straightforward rule, which decides that the process should be stopped after 50 negative examples are found in succession. \u2022 Cormack and Grossman proposed the knee method [2], which detects the inflection point i of current recall curve, and compare the slopes before and after i. If slope<i/slope>i is greater than a specific threshold \u03c1, the review should be stopped. \u2022 Wallace et al. [13] applied an estimator to estimate the number of positive examples |R| and let the users decide when to stop by showing them how close they are to the estimated number. 2.2 Human Error Correction When solving the total recall problems, the next example to be inspected relies on model trained on previously labeled examples. What if those labels can be wrong? The trained model could be misled by wrongly labeled examples and thus make worse decision on which example to inspect next. By now, researchers have applied various strategies to correct such human errors: \u2022 One simple way to correct human errors is by majority vote [9], where every example will be inspected by two different humans, and when the two humans disagree with each other, a third human is asked to make the final decision. \u2022 Cormack and Grossman [4] built an error correction method upon the knee stopping rule [2]. After the review stops, examples labeled as positive but are inspected after the inflection point (x > i) and examples labeled as negative but inspected before the inflection point (x < i) are sent to humans for a recheck. 2.3 Scalability How to scale up the active learning framework with multiple humans working simultaneously on the same project remains an open challenge to the total recall problem. In electronic discovery, Cormack and Grossman [3] proposed S-CAL where model trained on a finite number of examples is used to predict and estimate the precision, recall, and prevalence of the target large set of examples. However, this approach sacrifices the adaptability of the active learner. In evidence-based medicine, Nguyen et al. [11] explored the task allocation problem where two types of human operators are available: (1) crowdsourcing workers who are cheaper but less accurate, and (2) experts who are much more expensive but also more accurate. ", "conclusion": "", "full_text": "Total Recall, Language Processing, and Software Engineering\nZhe Yu, Tim Menzies\nNorth Carolina State University, USA\nzyu9@ncsu.edu,timm@ieee.org\nABSTRACT\nA broad class of software engineering problems can be generalized\nas the \u201ctotal recall problem\u201d. This short paper claims that identifying\nand exploring total recall language processing problems in software\nengineering is an important task with wide applicability.\nTo make that case, we show that by applying and adapting the\nstate of the art active learning and text mining, solutions of the total\nrecall problem, can help solve two important software engineering\ntasks: (a) supporting large literature reviews and (b) identifying\nsoftware security vulnerabilities. Furthermore, we conjecture that\n(c) test case prioritization and (d) static warning identification can\nalso be categorized as the total recall problem.\nThe widespread applicability of \u201ctotal recall\u201d to software engi-\nneering suggests that there exists some underlying framework that\nencompasses not just natural language processing, but a wide range\nof important software engineering tasks.\nKEYWORDS\nSoftware engineering, active learning, natural language processing,\ninformation retrieval\nACM Reference Format:\nZhe Yu, Tim Menzies. 2018. Total Recall, Language Processing, and Soft-\nware Engineering. In Proceedings of Workshop on NLP for Software Engi-\nneering (NL4SE@ ESEC/FSE 2018). ACM, New York, NY, USA, 4 pages.\nhttps://doi.org/0000001.0000001\n1\nINTRODUCTION\nSoftware engineering is a discipline closely involved with human\nactivities. How to help software developers produce better software\nmore efficiently is the core topic of software engineering. Prioritizing\ntasks can efficiently reduce the human efforts and time required\nto achieve certain goals of software development. Many of such\nprioritization problems in software engineering can be generalized\nas the total recall problem (defined in the next section).\nThe total recall problem has been explored in information retrieval\nfor years, and the state of the art solution with active learning and\nnatural language processing aims to resolve the following challenges:\n\u2022 Among a finite number of tasks, which task to be executed first so\nthat certain goals can be achieved earlier?\n\u2022 At what point, there is no need to execute the remaining tasks?\n\u2022 Are all the tasks executed correctly? How to identify wrongly\nexecuted tasks and correct them?\nACM acknowledges that this contribution was authored or co-authored by an employee,\ncontractor, or affiliate of the United States government. As such, the United States\ngovernment retains a nonexclusive, royalty-free right to publish or reproduce this article,\nor to allow others to do so, for government purposes only.\nNL4SE@ ESEC/FSE 2018, 4th November, 2018, Lake Buena Vista, Florida\n\u00a9 2018 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/0000001.0000001\n\u2022 How to scale up the process with multiple humans working in\nparallel (e.g. crowdsourcing)?\nThe first challenge has been extensively explored [1, 10, 14] while\nthe rest three have much room to improve [2\u20134].\nThis short paper claims that identifying and exploring the total\nrecall problems in software engineering is an important task which\nbenefits both software engineering research and the general solution\nto total recall problems. To support this claim, in \u00a72 we first introduce\nthe general total recall problem, and its state of the art solutions; then\nin \u00a73, we provide two software engineering tasks which are solved by\napplying total recall techniques and better solutions to the challenges\nof total recall problems were created during the process [16\u201318].\nWe also conjecture that two other software engineering tasks can be\ncategorized as the total recall problem. The goal of this short paper\nis to inspire software engineering researchers to explore total recall\nproblems in software engineering with our preliminary results.\n2\nTHE TOTAL RECALL PROBLEM\nThe total recall problem in information retrieval aims to optimize\nthe cost for achieving very high recall\u2013as close as practicable to\n100%\u2013with a human assessor in the loop [6]. More specifically, the\ntotal recall problem can be described as following:\nGiven a set of candidate examples E, in which only a small\nfraction R \u2282 E are positive, each example x can be inspected\nto reveal its label as positive (x \u2208 R) or negative (x \ufffd R)\nat a cost. Starting with the labeled set L = \u2205, the task is to\ninspect and label as few examples as possible (min |L|) while\nachieving very high recall |L \u2229 R|/|R|.\nThe Total Recall Problem:\nDifferent strategies have been applied to solve the total recall prob-\nlem, including supervised learning and semi-supervised learning.\nHowever, the state of the art solutions to the total recall problem\napply active learning [6] to learn from natural language processing\nfeatures (e.g. bag-of-words, tf-idf) extracted from the current labeled\nset L and re-rank the rest of the candidate examples E \\ L so that\nexamples that are more likely to be positive get inspected next. This\nactive learning solution has been proven effective in different use\ncases. A demonstration of the benefit of this active learning strategy\ncan be found in Figure 1, where with active learning (solid red line),\nhigh recall (close to 100%) can be achieved by inspecting only a\nsmall portion of the candidate examples.\nIn evidence-based medicine, researchers screen titles and abstracts\nto determine whether one paper should be included in a certain\nsystematic review. Wallace et al. [14] designed patient active learning\nto help researchers prioritize their effort on papers to be included.\nWith patient active learning, half of the screening effort can be saved\nwhile still including most relevant papers [14].\narXiv:1809.00039v1  [cs.SE]  31 Aug 2018\n NL4SE@ ESEC/FSE 2018, 4th November, 2018, Lake Buena Vista, Florida\nZ.Yu, T.Menzies\n0%\n20%\n40%\n60%\n80%\nCost\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nActive Learning\nRandom Order\nFigure 1: A demonstration for the power of active learning on\nthe total recall problem. X axis shows the cost spent while Y\naxis shows the recall reached. The dashed line represents the\nretrieval curve with random inspection order and the solid line\nshows the retrieval curve with active learning selecting which\nexample to inspect next.\nIn electronic discovery, attorneys are hired to review massive\namount of documents looking for relevant ones for a certain legal\ncase and provide those as evidences. Cormack and Grossman [1]\nproposed continuous active learning to save attorneys\u2019 effort from\nreviewing non-relevant documents, which further can save a large\namount of the cost of legal cases.\nThe active learning strategy for total recall problem can be de-\nscribed as following:\nStep 0 Given a candidate set E and initialize the labeled set L = \u2205.\nStep 1 Using strategies like ad hoc search or random sampling to\nselect next example x to label (L \u2190 L \u222a x).\nStep 2 Repeat Step 1 until \u201cenough\u201d positive examples have been\nlabeled (|L \u2229 R| \u2265 K).\nStep 3 Train/update a supervised learning model with current la-\nbeled set L.\nStep 4 Use the trained model to predict on the unlabeled set E \\ L\nand select next example x to label (L \u2190 L \u222a x).\nStep 5 Repeat Step 3 and Step 4 until a stopping rule has been met.\nWhere detail settings vary from case studies to case studies:\n\u2022 In Step 2, Cormack and Grossman [1] believe that learning should\nstart as soon as possible (K = 1) while Wallace et al. [14] suggest\nto start learning until more training examples are available.\n\u2022 In Step 3, most studies exact text features from examples and train\na support vector machine with linear kernel. However, there are\ndifferent opinions on how to balance the training data, e.g. Wallace\net al. [14] proposed a technique called aggressive undersampling\nto drop off the negative examples closes to the positive ones while\nMiwa et al. [10] adjusted the weight of each training example to\npunish more for misclassifying positive examples.\n\u2022 In Step 4, Cormack and Grossman [1] select examples that are\nmost likely to be positive while Wallace et al. [14] select most\nuncertain examples.\n\u2022 In Step 5, Cormack and Grossman [1] stop the process when a\nsufficient number of positive examples have been found while\nWallace et al. [14] stop the learning when the model becomes\nstable and then apply the model to classify the unlabeled examples.\nWhile this active learning and natural language processing strat-\negy has been extensively explored to resolve the main challenge of\ntotal recall problems, there still exists large room for improvement\nin the three other challenges.\n2.1\nWhen to Stop\nIn practice, there is no way to know the number of positive examples\nbefore inspecting and labeling every candidate example. It is thus\nimpossible to know exactly what recall has been reached during the\nprocess. Then, how do we know when to stop if the target is reaching,\nsay 95% recall? This is a practical problem that directly affects the\nusability of the active learning strategy. Stopping too early will\nresult in missing many valuable positive examples; while stopping\ntoo late will cause unnecessary cost when there are no more positive\nexamples to retrieve. So far, researchers have developed various\nstopping rules to solve the \u201cwhen to stop\u201d problem.\n\u2022 Ros et al. [12] developed the the most straightforward rule, which\ndecides that the process should be stopped after 50 negative exam-\nples are found in succession.\n\u2022 Cormack and Grossman proposed the knee method [2], which\ndetects the inflection point i of current recall curve, and compare\nthe slopes before and after i. If slope<i/slope>i is greater than a\nspecific threshold \u03c1, the review should be stopped.\n\u2022 Wallace et al. [13] applied an estimator to estimate the number\nof positive examples |R| and let the users decide when to stop by\nshowing them how close they are to the estimated number.\n2.2\nHuman Error Correction\nWhen solving the total recall problems, the next example to be\ninspected relies on model trained on previously labeled examples.\nWhat if those labels can be wrong? The trained model could be\nmisled by wrongly labeled examples and thus make worse decision\non which example to inspect next. By now, researchers have applied\nvarious strategies to correct such human errors:\n\u2022 One simple way to correct human errors is by majority vote [9],\nwhere every example will be inspected by two different humans,\nand when the two humans disagree with each other, a third human\nis asked to make the final decision.\n\u2022 Cormack and Grossman [4] built an error correction method upon\nthe knee stopping rule [2]. After the review stops, examples la-\nbeled as positive but are inspected after the inflection point (x > i)\nand examples labeled as negative but inspected before the inflec-\ntion point (x < i) are sent to humans for a recheck.\n2.3\nScalability\nHow to scale up the active learning framework with multiple humans\nworking simultaneously on the same project remains an open chal-\nlenge to the total recall problem. In electronic discovery, Cormack\nand Grossman [3] proposed S-CAL where model trained on a finite\nnumber of examples is used to predict and estimate the precision,\nrecall, and prevalence of the target large set of examples. However,\nthis approach sacrifices the adaptability of the active learner. In\nevidence-based medicine, Nguyen et al. [11] explored the task allo-\ncation problem where two types of human operators are available:\n(1) crowdsourcing workers who are cheaper but less accurate, and\n(2) experts who are much more expensive but also more accurate.\n Total Recall and SE\nNL4SE@ ESEC/FSE 2018, 4th November, 2018, Lake Buena Vista, Florida\nThis approach allows the system to operate more economically but\nstill cannot scale up with growing number of training examples.\n3\nTOTAL RECALL PROBLEMS IN\nSOFTWARE ENGINEERING\nAs discussed in \u00a71, there are tasks in software engineering that re-\nsemble the problem of total recall. In this section, we will discuss\nfour such problems. The first two tasks have been explored by the\nauthors previously and the promising results convince us that apply-\ning active learning and natural language processing strategy is the\nkey to solving these software engineering problems.\n3.1\nPrimary Study Selection in Systematic\nLiterature Reviews\nFor the same reason of medicine researchers, software engineering\nresearchers also need to read literature to stay current on their re-\nsearch topics. Researchers conduct systematic literature reviews [8]\nto analyze the existing literature and to facilitate other researchers.\nAmong different steps of systematic literature reviews, primary study\nselection is in exactly the same format as citation screening in med-\nical systematic reviews. The problem is also for reading titles and\nabstracts to find relevant papers to include, except that those pa-\npers are about software engineering. As a result, the primary study\nselection problem can be described as a total recall problem:\n\u2022 E: set of software engineering papers returned from a search query.\n\u2022 R: set of relevant papers to the systematic literature review.\n\u2022 L: set of papers that have been reviewed and labeled as relevant or\nnon-relevant by human researchers.\nWhile exploring this total recall problem [16, 17], the authors:\n(1) Designed a new active learning framework by combining advan-\ntages of the state of the art approaches [1, 10, 14] in \u00a72. When\nreaching the same recall, the new framework costs 20-50% less\nthan these prior state of the art approaches.\n(2) Created an estimator for |R| based on semi-supervised learning\nfor the when to stop challenge. This estimator showed better\naccuracy than the one from Wallace et al. [13] and provided\nbetter stopping rules than the prior methods in \u00a72.1. With the\nhelp of this estimator, the users can now know what level of\nestimated recall has been reached and make better decision on\nwhether to spend more effort to achieve a higher recall at any\npoint of the process.\n(3) Proposed an error identification method, in which only examples\nwith labels that the current active learner disagrees most on are\nrechecked by a different human expert. This method has been\nproven to be more cost effective in addressing the human error\ncorrection challenge than prior methods in \u00a72.2.\nDatasets and tools for reproduction and making further improve-\nments on the primary study selection problem can be found at\nSeacraft, Zenodo1 and Github2.\n3.2\nSoftware Security Vulnerability Prediction\nSociety needs more secure software. It is crucial to identify software\nsecurity vulnerabilities and fix them as early as possible. However, it\n1https://doi.org/10.5281/zenodo.1162952\n2https://github.com/fastread/src\nVPM\n\u2026\nReviewers\nCodebase\n\u2026\nQueue\nTrain\nPredict\nFigure 2: Poactive Security Review and Test Framework\nis time-consuming to have human experts inspecting the entire code-\nbase looking for the few source code files that contain vulnerabilities.\nThe solution to this problem, prioritizing the human inspection effort\ntowards codes that are more likely to have vulnerabilities is also a\ntotal recall problem:\n\u2022 E: the entire codebase of a software project.\n\u2022 R: set of source code files that contain vulnerabilities.\n\u2022 L: set of source code files already inspected by humans.\nWith simulations on the known vulnerabilities from Mozilla Fire-\nfox project [18], the authors:\n(1) Extracted bag-of-words from source codes and applied same\nactive learning algorithm as the primary study selection prob-\nlem [16, 17] to select which source code file to inspect next\nand to stop the inspection when target recall is reached. Results\nshowed that 95, 99, 100% recall can be achieved by having\nhuman experts inspect 28, 41, 55% of the source code files, re-\nspectively [18]. Results also showed that text features perform\nbetter than traditional software metrics features.\n(2) Adapted the error correction method from primary study selec-\ntion [17] to identify missing vulnerabilities3. Results showed\nthat it can reach higher recall with lower cost comparing to other\nerror correction methods mentioned in \u00a72.2.\n(3) Designed a centralized system where human operators can work\nin parallel but all the information is gathered in one place to\nupdate the model, as shown in Figure 2. However, when the num-\nber of training example grows, the time required for updating\nthe model becomes longer and longer.\nThe real benefit of treating vulnerability prediction as a total recall\nproblem is that no labeled data (known vulnerabilities) are required\nto start the process, thus these vulnerabilities can be identified and\nfixed before the software is deployed. On contrast, conventional\nmethods such as supervised learning or unsupervised learning re-\nquire post-deployment information such as known vulnerabilities\nfrom bug reports or crash dump stack traces.\n3.3\nStatic Warning Identification\nStatic Analysis tools (e.g., FindBugs) are widely used to find de-\nfects in software. These tools scan source code or binary code of a\n3When inspecting codes, human may miss some vulnerabilities (false negatives) but\nmay never have false positives, and the false negative rate can be as high as 47% [7].\n NL4SE@ ESEC/FSE 2018, 4th November, 2018, Lake Buena Vista, Florida\nZ.Yu, T.Menzies\nsoftware project and infer bugs, security vulnerabilities, and bad pro-\ngramming practices with heuristic pattern matching techniques [15].\nProblem is, the high false positive rate of the reported warnings\ncauses most of the warnings not acted on by developers [15]. Reduc-\ning such false positives can be viewed as a total recall problem:\n\u2022 E: all warnings reported by the static analysis tools.\n\u2022 R: set of warnings that reveal true defects.\n\u2022 L: set of warnings that have been inspected by human experts.\nThis static warning identification problem has been explored with\nsupervised learning methods and sets of different features [15]. Ana-\nlyzing the warnings with natural language processing and applying\nactive learning to select which warning to inspect might help reduce\nfalse positives and make static analysis tools more practical to use.\n3.4\nTest Case Prioritization\nRegression testing is an expensive testing process to detect whether\nnew faults have been introduced into previously tested code. To\nreduce the cost of regression testing, software testers may prioritize\ntheir test cases so that those which are more likely to fail are run\nearlier in the regression testing process [5]. The goal of test case\nprioritization is to increase the rate of fault detection, and by doing\nso, it can provide earlier feedback on the system under test, enable\nearlier debugging, and increase the likelihood that, if testing is pre-\nmaturely halted, those test cases that offer the greatest fault detection\nability in the available testing time will have been executed [5]. This\ntest case prioritization problem can also be treated as a total recall\nproblem with the following notations:\n\u2022 E: all candidate regression test suites.\n\u2022 R: set of test suites that will detect faults.\n\u2022 L: set of test suites that have been executed.\nExisting techniques for prioritizing test cases are \u201cunsupervised\u201d,\ni.e. these techniques decide an order of the test cases to be run and\nstick with it. Applying active learning to adapt the ordering with\nknowledge learned from test cases executed can potentially further\nincrease the rate of fault detection and reduce more cost. However,\nthis problem has not been explored as a total recall problem and the\nfollowing challenges need to be resolved:\n\u2022 What types of feature can be extracted from the test cases so that\nthe learned model can accurately predict the likelihood of fault\ndetection from a test case before its execution.\n\u2022 How to balance the priority of test cases increasing coverage of\nstatements/functions most and the more-likely-to-fail test cases.\n\u2022 How to handle test dependence, i.e. if test cases are not indepen-\ndent, changing their order may fail some tests but pass others [19].\n4\nCONCLUSIONS AND FUTURE WORK\nMany of the software engineering problems can be generalized as\nthe total recall problem. This paper identified four such problems.\nWith two case studies of primary study selection and vulnerability\nprediction, we showed that exploring these total recall problems\nin software engineering would benefit both software engineering\nresearch and the general solution to total recall problems. We hope\nthis paper will attract more researchers studying and improving the\ntotal recall problems in software engineering. Future work in this\narea includes but is not limited to the following:\n\u2022 Improve core algorithm for the total recall problem\u2014reaching\nsame recall with less cost.\n\u2022 Build more accurate estimator for current recall achieved.\n\u2022 Resolve human errors more efficiently.\n\u2022 Scale up the total recall solutions (probably by ensemble learning)\nand utilize low-cost workers through crowdsourcing.\n\u2022 Apply total recall techniques to reduce false alarms in static code\nanalysis and to prioritize test cases.\n\u2022 Identify more total recall problems in software engineering.\nREFERENCES\n[1] Gordon V Cormack and Maura R Grossman. 2014. Evaluation of machine-learning\nprotocols for technology-assisted review in electronic discovery. In Proceedings\nof the 37th international ACM SIGIR conference on Research & development in\ninformation retrieval. ACM, 153\u2013162.\n[2] Gordon V Cormack and Maura R Grossman. 2016. Engineering Quality and\nReliability in Technology-Assisted Review. (2016), 75\u201384.\n[3] Gordon V Cormack and Maura R Grossman. 2016. Scalability of Continuous\nActive Learning for Reliable High-Recall Text Classification. In Proceedings\nof the 25th ACM International on Conference on Information and Knowledge\nManagement. ACM, 1039\u20131048.\n[4] Gordon V Cormack and Maura R Grossman. 2017. Navigating Imprecision\nin Relevance Assessments on the Road to Total Recall: Roger and Me. In The\nInternational ACM SIGIR Conference. 5\u201314.\n[5] Sebastian Elbaum, Alexey G Malishevsky, and Gregg Rothermel. 2002. Test\ncase prioritization: A family of empirical studies. IEEE transactions on software\nengineering 28, 2 (2002), 159\u2013182.\n[6] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. 2016. TREC 2016\nTotal Recall Track Overview. In TREC.\n[7] Les Hatton. 2008. Testing the value of checklists in code inspections. IEEE\nsoftware 25, 4 (2008).\n[8] Barbara A Kitchenham, Tore Dyba, and Magne Jorgensen. 2004. Evidence-based\nsoftware engineering. In Proceedings of the 26th international conference on\nsoftware engineering. IEEE Computer Society, 273\u2013281.\n[9] Marco Kuhrmann, Daniel M\u00c3l\u2019ndez Fern\u00c3 \u02dbandez, and Maya Daneva. 2017. On the\npragmatic design of literature studies in software engineering: an experience-based\nguideline. Empirical Software Engineering 22, 6 (2017), 2852\u20132891.\n[10] Makoto Miwa, James Thomas, Alison O\u2019Mara-Eves, and Sophia Ananiadou. 2014.\nReducing systematic review workload through certainty-based screening. Journal\nof biomedical informatics 51 (2014), 242\u2013253.\n[11] An Thanh Nguyen, Byron C Wallace, and Matthew Lease. 2015. Combining\ncrowd and expert labels using decision theoretic active learning. In Third AAAI\nConference on Human Computation and Crowdsourcing.\n[12] Rasmus Ros, Elizabeth Bjarnason, and Per Runeson. 2017. A Machine Learning\nApproach for Semi-Automated Search and Selection in Literature Studies. In\nProceedings of the 21st International Conference on Evaluation and Assessment\nin Software Engineering. ACM, 118\u2013127.\n[13] Byron C Wallace, Issa J Dahabreh, Kelly H Moran, Carla E Brodley, and Thomas A\nTrikalinos. 2013. Active literature discovery for scoping evidence reviews: How\nmany needles are there. In Proceedings of KDD workshop on data mining for\nhealthcare (KDD-DMH).\n[14] Byron C Wallace, Thomas A Trikalinos, Joseph Lau, Carla Brodley, and Christo-\npher H Schmid. 2010. Semi-automated screening of biomedical citations for\nsystematic reviews. BMC bioinformatics 11, 1 (2010), 1.\n[15] Junjie Wang, Song Wang, and Qing Wang. 2018. Is There A \u201cGolden\u201d Feature Set\nfor Static Warning Identification?. In Proceedings of the International Symposium\non Empirical Software Engineering and Measurement.\n[16] Zhe Yu, Nicholas A. Kraft, and Tim Menzies. 2018. Finding better active learners\nfor faster literature reviews. Empirical Software Engineering (07 Mar 2018).\nhttps://doi.org/10.1007/s10664-017-9587-0\n[17] Zhe Yu and Tim Menzies. 2017. FAST2: Better Automated Support for Finding\nRelevant SE Research Papers. CoRR abs/1705.05420 (2017). http://arxiv.org/abs/\n1705.05420\n[18] Zhe Yu, Christopher Theisen, Hyunwoo Sohn, Laurie Williams, and Tim Menzies.\n2018. Cost-aware Vulnerability Prediction: the HARMLESS Approach. CoRR\nabs/1803.06545 (2018). arXiv:1803.06545 http://arxiv.org/abs/1803.06545\n[19] Sai Zhang, Darioush Jalali, Jochen Wuttke, K\u0131van\u00e7 Mu\u00b8slu, Wing Lam, Michael D\nErnst, and David Notkin. 2014. Empirically revisiting the test independence\nassumption. In Proceedings of the 2014 International Symposium on Software\nTesting and Analysis. ACM, 385\u2013396.\nReceived August 2018\n"}, "Transfer Learning for British Sign Language Modelling": {"authors": ["Boris Mocialov", "Graham Turner", "Helen Hastie"], "title": "Transfer Learning for British Sign Language Modelling", "url": "https://arxiv.org/pdf/2006.02144.pdf", "abstract": "Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods. This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus", "arxiv_id": "2006.02144", "published_date": "2020-06-03", "year": 2020, "introduction": "Introduction Spoken dialogue systems and voice assistants have been developed to facilitate natural conversation between machines and humans. They provide services through devices such as Amazon Echo Show and smartphones to help the user do tasks (McTear, 2004) and, more recently, for open domain chitchat (Serban et al., 2016), all through voice. Recent advances have been facilitated by the huge amounts of data collected through such devices and have resulted in the recent success of deep machine methods, providing signi\ufb01cant improvements in performance. However, not all languages are able to bene\ufb01t from these advances, particularly those that are under-resourced. These include sign languages and it means that those who sign are not able to leverage such interactive systems nor the bene\ufb01ts that automatic transcription and translating of signing would afford. Here, we advance the state of the art with respect to transcribing British Sign Language (BSL). Our aim is for automated transcription of the BSL into English leveraging video recognition technologies. BSL enables communication of meaning through parameters such as hand shape, position, hand orientation, motion, and non-manual signals (Sutton-Spence and Woll, 1999). BSL has no standard notation for writing the signs, as with letters and words in English. Analogous to the International Phonetic Alphabet (IPA), highly detailed mapping of visual indicators to written form are available, such as HamNoSys (Hanke, 2004). Despite the expressiveness of the HamNoSys writing system, its practical uses are limited and only a handful of experts know how to use it. Recent methods for automatic speech recognition (ASR) use deep neural models to bypass the need for phoneme dictionaries (Hannun et al., 2014), which are then combined with language models. Previous work (Mocialov et al., 2016; Mocialov et al., 2017) has shown that we can use visual features to automatically predict individual signs. This work follows on in that these individual signs are to be used with a language model to take into account context and therefore increase accuracy of the transcriber, which outputs a string of word-like tokens. These tokens are called glosses (Sutton-Spence and Woll, 1999; Cormier et al., 2015). Although glosses are translated BSL signs, they also convey some grammatical information about BSL. This makes glosses useful in This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ arXiv:2006.02144v1  [cs.CL]  3 Jun 2020 ", "conclusion": "Conclusion This paper shows how transfer learning techniques can be used to improve language modelling for the BSL language at the gloss level. Statistical modelling techniques are used to generate language models and to evaluate them using a perplexity measure. The choice of the transfer learning technique is guided by the scarcity of available resources of the BSL language and the availability of the English language dataset that shares similar language modelling vocabulary with the annotated BSL. Feed-forward and recurrent neural models have been used to evaluate and compare generated language models. The results show that transfer learning can achieve superior quality of the generated language models. However, our pre-processed BSL corpus lacks constructs that are essential for a sign language, such as classi\ufb01er signs and others. Nevertheless, transfer learning for modelling the BSL shows promising results and should be investigated further. 6.1 Future Work Although this paper discusses the use of a model initially trained on English and presents promising preliminary results, the annotation of the BSL, used in this paper, is limited as this paper serves as a proof of concept. In particular, the annotation used is missing some of the grammatical aspects of the BSL, such as classi\ufb01er signs and others. Inclusion of these into the BSL language modelling would increase the OOV count as the English language does not have equivalent language constructs. This raises a question whether a sign language can be modelled using other languages that may have these constructs. More generally, is it possible to model a language with transfer learning using other lessrelated languages? Similar questions have been partly answered for the written languages in the \ufb01eld of machine translation (Gu et al., 2018) by bringing words of different languages close to each other in the latent space. However, nothing similar has been done for the sign languages. From the methodological side of the modelling, additional advanced state of the art techniques should be experimented with to achieve greater quality of the generated models, such as attention mechanism for the recurrent neural networks. Finally, this paper focuses on key techniques for sign processing, which could be part of a larger conversational system whereby signers could interact with computers and home devices through their natural communication medium of sign. Research in such end-to-end systems would include vision processing, segmentation, classi\ufb01cation, and language modelling as well as language understanding and dialogue modelling, all tuned to sign language. ", "full_text": "Transfer Learning for British Sign Language Modelling\nBoris Mocialov\nSchool of Mathematical\nand Computer Sciences and\nEdinburgh Centre for Robotics\nHeriot-Watt University\nEdinburgh, UK\nbm4@hw.ac.uk\nGraham Turner\nSchool of Social Sciences and\nCentre for Translation &\nInterpreting Studies in Scotland\nHeriot-Watt University\nEdinburgh, UK\ng.h.turner@hw.ac.uk\nHelen Hastie\nSchool of Mathematical\nand Computer Sciences and\nEdinburgh Centre for Robotics\nHeriot-Watt University\nEdinburgh, UK\nh.hastie@hw.ac.uk\nAbstract\nAutomatic speech recognition and spoken dialogue systems have made great advances through\nthe use of deep machine learning methods. This is partly due to greater computing power but also\nthrough the large amount of data available in common languages, such as English. Conversely,\nresearch in minority languages, including sign languages, is hampered by the severe lack of data.\nThis has led to work on transfer learning methods, whereby a model developed for one language\nis reused as the starting point for a model on a second language, which is less resourced. In\nthis paper, we examine two transfer learning techniques of \ufb01ne-tuning and layer substitution for\nlanguage modelling of British Sign Language. Our results show improvement in perplexity when\nusing transfer learning with standard stacked LSTM models, trained initially using a large corpus\nfor standard English from the Penn Treebank corpus.\n1\nIntroduction\nSpoken dialogue systems and voice assistants have been developed to facilitate natural conversation\nbetween machines and humans. They provide services through devices such as Amazon Echo Show\nand smartphones to help the user do tasks (McTear, 2004) and, more recently, for open domain chitchat\n(Serban et al., 2016), all through voice. Recent advances have been facilitated by the huge amounts of\ndata collected through such devices and have resulted in the recent success of deep machine methods,\nproviding signi\ufb01cant improvements in performance. However, not all languages are able to bene\ufb01t from\nthese advances, particularly those that are under-resourced. These include sign languages and it means\nthat those who sign are not able to leverage such interactive systems nor the bene\ufb01ts that automatic\ntranscription and translating of signing would afford.\nHere, we advance the state of the art with respect to transcribing British Sign Language (BSL). Our aim\nis for automated transcription of the BSL into English leveraging video recognition technologies. BSL\nenables communication of meaning through parameters such as hand shape, position, hand orientation,\nmotion, and non-manual signals (Sutton-Spence and Woll, 1999). BSL has no standard notation for\nwriting the signs, as with letters and words in English. Analogous to the International Phonetic Alphabet\n(IPA), highly detailed mapping of visual indicators to written form are available, such as HamNoSys\n(Hanke, 2004). Despite the expressiveness of the HamNoSys writing system, its practical uses are limited\nand only a handful of experts know how to use it. Recent methods for automatic speech recognition\n(ASR) use deep neural models to bypass the need for phoneme dictionaries (Hannun et al., 2014), which\nare then combined with language models. Previous work (Mocialov et al., 2016; Mocialov et al., 2017)\nhas shown that we can use visual features to automatically predict individual signs. This work follows\non in that these individual signs are to be used with a language model to take into account context and\ntherefore increase accuracy of the transcriber, which outputs a string of word-like tokens. These tokens\nare called glosses (Sutton-Spence and Woll, 1999; Cormier et al., 2015). Although glosses are translated\nBSL signs, they also convey some grammatical information about BSL. This makes glosses useful in\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence.\nLicence details: http://\ncreativecommons.org/licenses/by/4.0/\narXiv:2006.02144v1  [cs.CL]  3 Jun 2020\n their own right without the videos of the BSL signs and sheds some light into the syntax and semantics\nof the BSL.\nThis paper focuses on language modelling, a common technique in the \ufb01eld of ASR and Natural\nLanguage Processing to model the likelihood of certain words following each other in a sequence. We\nimprove modelling of the BSL glosses by proposing to use transfer learning approaches, such as \ufb01ne-\ntuning and layer substitution. The use of transfer learning technique can overcome the data sparsity issue\nin statistical modelling for scarce resource languages by using similar resources that can be found in\nlarge quantities and then further training the models on a speci\ufb01c low resource data.\nWe show that a model, pre-trained on the Penn Treebank (PTB) dataset1 and \ufb01ne-tuned on the BSL\nmonolingual corpus2 can yield better results. This is in contrast to the same architecture that is trained\ndirectly on the BSL dataset without pre-training. This is a somewhat surprising result as there are marked\ndifferences between the two languages, particularly with the respect to the syntax (Sutton-Spence and\nWoll, 1999).\nThe paper begins with presenting methods for modelling languages and how they can be utilised in the\nBSL modelling. Section 2.2 gives an overview of how transfer learning can be achieved as well as the\nuse of transfer learning in sign languages. Section 3 gives an overview of the datasets that are used in this\npaper, their statistics, and pre-processing steps to create two monolingual corpora for statistical model\ntraining. Section 4 describes in detail the setup for the experiments in this paper. Section 5 presents the\nresults of the models employed for this research and discusses these results and the limitations of the\napproach taken in terms of the data used in Section 5.3. The paper is then concluded and future work is\nproposed.\n2\nRelated Work\n2.1\nSign Language Modelling\nDespite the availability of many alternatives for language modelling, such as count-based n-grams and\ntheir variations (Chen and Goodman, 1999; Rosenfeld, 2000; MacCartney, 2005; Bulyko et al., 2007;\nGuthrie et al., 2006), hidden Markov models (Dreuw and Ney, 2008; Dreuw et al., 2008), decision trees\nand decision forests (Filimonov, 2011), and neural networks (Deena et al., 2016; Mikolov et al., 2010),\nresearch in sign language modelling predominantly employs simple n-gram models, such as in Cate and\nHussain (2017), Forster et al. (2012), and Mass\u00b4o and Badia (2010).\nThe reason for the wide-spread use of n-grams in sign language modelling is the simplicity of the\nmethod. However, there is a disconnect between n-grams and sign language in that signing is embodied\nand perceived visually, while the n-grams are commonly applied to text sequence modelling. For this\nreason, the authors in Stein et al. (2007), Zhao et al. (2000), Dreuw et al. (2008), Mass\u00b4o and Badia\n(2010), and Forster et al. (2013) model glosses, such as the ones shown on Figure 2, which are obtained\nfrom the transcribed sign languages, in a similar way to how language modelling is applied to automatic\ntranscribed words from speech.\nGlosses model the meaning of a sign in a written language, but not the execution (i.e. facial expres-\nsions, hand movement). Therefore, the more detailed meaning of what was signed may get lost when\nworking with the higher-level glosses. To overcome this issue and to incorporate valuable information\ninto sign language modelling, additional features are added in similar research, such as non-manual fea-\ntures (e.g facial expressions) (San-Segundo et al., 2009; Mass\u00b4o and Badia, 2010; Zhao et al., 2000; Stein\net al., 2007).\nIn this work we use glosses because we want to model BSL purely at the gloss level without any\nadditional information (e.g. facial expressions).\n2.2\nTransfer Learning\nWhile transfer learning is a more general machine learning term, cross-domain adaptation of language\nmodels is used in the language modelling literature (Deena et al., 2016; Ma et al., 2017). Models are\n1https://catalog.ldc.upenn.edu/ldc99t42\n2http://www.bslcorpusproject.org/\n usually trained on some speci\ufb01c domain that consists of a speci\ufb01c topic, genre, and similar features that\ncan be identi\ufb01ed by an expert. For example, a restaurant domain when a new type of a restaurant is\ncreated then the system needs to be able to adapt and be able to understand and discuss this new type\nof the restaurant. Unfortunately, it is nearly impossible to train a model for all possible con\ufb01guration of\ncurrent or future features. Commonly, a set of features are extracted from the raw data. When features\nchange, re-training is required. Useful features can also be extracted without expert knowledge with\nsuch techniques as Latent Dirichlet Allocation (LDA). These features usually take the form of words that\nrepresent topics in the data (Deena et al., 2016). Best practice tries to avoid re-training the models every\ntime one of the features changes as the domain changes due to the overhead involved.\nModel-based adaptation to the new domains, on the other hand, is achieved by either \ufb01ne-tuning or\nthe introduction of adaptation layer(s) (Yosinski et al., 2014). Fine-tuning involves further training the\nalready pre-trained model using the data from the new domain. The intuition behind the \ufb01ne-tuning is\nthat it is much quicker to learn new information with related knowledge. The adaptation layer approach\nincorporates new knowledge by re-training only the adaptation layer, whereas the rest of the model\nremains exactly the same as if it was used in the original domain and acts as a feature extractor for the\nnew domain (Deena et al., 2016).\nTransfer learning has been applied to sign languages in computing for various purposes to demonstrate\nthat the method is suitable for the task due to the lack of substantial domain-speci\ufb01c sign language data.\nTransfer learning has been successfully applied to static pose estimation, transferring the knowledge\nfrom pose estimation to the sign language pose estimation (Gattupalli et al., 2016) and classi\ufb01cation\nof \ufb01ngerspelled letters in American Sign Language (Garcia and Viesca, 2016; Karthick Arya, 2017;\nChaudhary, 2017; Muskan Dhiman, 2017). In particular, most of the transfer learning in sign language\nhas been applied to static image recognition to recognise the hand shape in an image using convolutional\nneural networks.\nWe apply transfer learning to the language modelling task as this is a key challenge in successfully\ntranscribing BSL.\n3\nCorpora\nThe BSL corpus and the preprocessed Penn Treebank (PTB) corpus were chosen for this research. The\nmonolingual PTB dataset consists of telephone speech, newswire, microphone speech, and transcribed\nspeech. The dataset is preprocessed to eliminate letters, numbers, or punctuation and was used by\nMikolov (2010). The BSL corpus contains video conversations among deaf native, near-native and \ufb02uent\nsigners across the United Kingdom. Almost all of the approximately one hundred recorded conversations\nare annotated for thirty seconds each at the gloss level using ELAN3 annotation tool (Schembri et al.,\n2013).\nFigure 1: The BSL Corpus Project Sample Video Snippets4\n3https://tla.mpi.nl/tools/tla-tools/elan/\n All recordings of the signers were made using up to four standard video cameras with a plain backdrop\nto provide full body view of the individuals, as well as, views from above of their use of signing space.\nThe conversations between the signers included signing personal experience anecdotes, spontaneous\nconversations (Schembri et al., 2013).\nThe BSL data that we focused on was narratives between two participants, where one person had to\nthink of a topic to sign about to another participant during the elicitation.\nRH-IDgloss\nPT:PRO1SG\nEXPLAIN\nABOUT\nPT:POSS1SG\nFS:PUPPY\nDSEW(FLAT)-BE:ANIMAL\nPT:POSS1SG\nWANT\nFAMILY\nAT-LAST\nHAVE\nDSEW(FLAT)-BE:ANIMAL\n?LAST-WEEK\nGOOD\nLH-IDgloss\nEXPLAIN\nABOUT\nFS:PUPPY\nDSEW(FLAT)-BE:ANIMAL\nAT-LAST\nHAVE\nDSEW(FLAT)-BE:ANIMAL\nGOOD\nFree Translation\nI want to tell you about my puppy\nMy family got a puppy last year\nModel Input Gloss\nEXPLAIN\nABOUT\nPUPPY\nANIMAL\nWANT\nFAMILY\nAT-LAST\nHAVE\nANIMAL\nLAST-WEEK\nGOOD\nFigure 2: a) The BSL Corpus Annotation in ELAN; b) Table shows full text of the annotated glosses for\nthe two \ufb01rst sentences from the ELAN annotation; c) Glosses that are used for the BSL modelling\nThe corpus is annotated with glosses, taken from the BSL SignBank in ELAN as shown in Figure 2a.\nFigure 2b shows all the glosses of the \ufb01rst sentence. As mentioned above, gloss is an identi\ufb01er of\na unique sign, written in English and should represent its phonological and morphological meaning\n(Schembri et al., 2013). In the corpus, the glosses are identi\ufb01ed throughout the videos for both left and\nright hands as sometimes different signs can be signed at the same time. Apart from the glossing, the\nannotations include the corresponding free English written translation of the meaning of the signing split\ninto sentences (see the Free Translation in the Figure 2). Figure 2c shows which glosses are considered\nfor the BSL modelling and which are ignored. This is done to match the vocabulary of the PTB corpus\nfor the transfer learning purposes.\n3.1\nData Pre-processing\nFor the BSL corpus, we ignore the free translation and extract English text from the glosses, preserving\nthe order of the signs executed. For example, in Figure 2, right-hand glosses identify the following\norder of the signs: good, explain, about, puppy, etc. excluding glosses, such as PT:PRO for pointing\nsigns or PT:POSS for possessives and others (Figure 2c), which are explained in more detail in Fenlon et\nal. (2014). Since the gloss annotation does not include explicit punctuation, it is impossible to tell where\na signed sentence begins and where it stops. To overcome this limitation of the gloss annotation, we use\nthe Free Translation annotation, which gives the boundaries of sentences in videos. Later, we split the\nextracted glosses into sentences using these sentence boundaries. By the end of the pre-processing stage,\nwe have glosses (excluding special glosses for pointing signs, posessives or other non-lexical glosses) in\nthe order that the corresponding signs were executed in the video, split into sentences. As a result, we\nextracted 810 nominal sentences from the BSL corpus with an average length of the sentence being 4.31\nglossed signs, minimum and maximum lengths of 1 and 13 glossed signs respectively. A monolingual\ndataset has been created with the extracted sentences. As obtained from the PTB dataset (Merity et al.,\n2017), the English language corpus has 23.09 words on average per sentence with minimum being 3 and\nmaximum 84 words per sentence. The pre-processed BSL corpus has a vocabulary of 666 words, while\nthe PTB dataset has a vocabulary of 10,002 words. From this point on in this paper, we will use the\nterm \u2018words\u2019 to refer to both glosses in the BSL and words in the PTB datasets because we aim to use a\ncommon vocabulary for training our models.\n4http://www.bslcorpusproject.org/cava/\na)\nb)\nc)\n Both monolingual datasets were split into training, validation, and testing sets as required for training\nand evaluation of the statistical models. Both datasets were split using ratio 85:15. The smaller subset,\nin turn, was split 50:50 for validation and testing for the two datasets.\n4\nLanguage Modelling Methodology\n4.1\nStatistical Language Models\nPerplexity measure has been used for evaluation and comparison purposes of different models. We used\nthe following formula to calculate the perplexity values: eCross\u2212Entropy as used in Bengio et al. (2003),\nwhich approximates geometric average of the predicted words probabilities on the test set. We have\nexplicitly modelled out-of-vocabulary (OOV), such as < unk > placeholder in all the experiments.\n4.1.1\nNeural Models\nFor comparison, we use two methods: 1) stacked LSTM and 2) Feed-Forward (FFNN) architectures to\ncreate the BSL language models. All models are implemented in PyTorch5 with weight-drop recurrent\nregularisation scheme for the LSTMs, which is important for overcoming commonly known LSTM\nmodel generalisation issues (Merity et al., 2017; Merity et al., 2018). The feed-forward model, on the\nother hand, had no regularisations as it is less susceptible to over\ufb01tting due to the much smaller number\nof parameters.\nThe parameters that were modi\ufb01ed to achieve the lowest perplexity were input size of the overall input\nsequence for the recurrent neural network (back-propagation through time, BPTT), batch size, learning\nrate, and the optimizer. The parameters were selected using the grid search approach using perplexity\nmetric. As a result, for the stacked LSTMs, bptt was set to 5, batch size was set to 16, discounted learning\nrate was set to 30, and the optimizer was set to stochastic gradient descent. In case of the feed-forward\nnetwork, input was set to 5 words, batch size was set to 16, discounted learning rate was set to 30, and\nthe optimizer was set to stochastic gradient descent. All the neural models were trained for 100 epochs.\nIn the case of the neural networks, the sequences of words were tokenised (i.e. turned into integers)\nand the tokenisation was stored to ensure the same tokenisation during the transfer learning phase. The\ninput, therefore, consisted of a set of tokens, while the outputs (i.e. predicted words) were turned into a\none-hot vectors.\n(a)\nStacked LSTMs model\n(b)\nFeed-Forward model\nFigure 3: The two types of neural models used to test transfer methods for sign language modelling\n5http://pytorch.org/\n 4.1.1.1\nStacked LSTMs\nFigure 3a shows the architecture of the stacked LSTM model. The model consists of an embedding layer\nof 400 nodes, which, together with the tokenisation, turns string of words into a vector of real numbers.\nSecondly, three LSTM layers with 1150 nodes each are stacked vertically for deeper feature extraction.\nThirdly, the linear layer downsizes the stacked LSTMs output to the vocabulary size and applies linear\ntransformation with softmax normalisation. The weights of the embedding and the linear layers are tied.\nThis means that the two layers share the same weights, which reduces the number of parameters of the\nnetwork and makes the convergence during training faster. The same architecture was used in Merity\net al. (2017) to model PTB dataset, reporting 57.3 perplexity, utilising cache in the model from recent\npredictions.\n4.1.1.2\nFFNN\nFigure 3b shows the Feed-forward model architecture. The model does not have the stacked LSTMs lay-\ners. Instead, the stacked LSTMs are substituted with one hidden fully-connected recti\ufb01er layer, which is\nknown to overcome the vanishing gradient problem. The weights of the embedding and the outputs lay-\ners are not tied together. Similar architectures have been used for language modelling in Le et al. (2013),\nMikolov et al. (2009), and de Br\u00b4ebisson et al. (2015) with the hidden layer having different activation\nfunctions with the PTB dataset being used in Audhkhasi et al. (2014), reporting 137.32 perplexity.\n4.1.2\nTraining the Models\nTransfer learning was achieved with both \ufb01ne-tuning and substitution. Both FFNN and LSTM were\ntrained on the PTB dataset and then either \ufb01ne-tuned or the last layer was substituted with the new\nadaptation layer, freezing the rest of the weights, and further training on the BSL dataset.\nTo achieve \ufb01ne-tuning, \ufb01rst the best model is saved after the training of both the FFNN and the stacked\nLSTMs on the PTB dataset. Then the training is restarted on the BSL corpus, having initialised the model\nwith the weights, trained on the PTB dataset.\nTo perform layer substitution as a transfer learning approach, the same \ufb01rst step as with the \ufb01ne-tuning\nis repeated and the model, trained on the PTB, is saved. When the training is restarted on the BSL dataset,\nthe saved model is loaded and the last linear layer is substituted with a layer that has as many nodes as\nthe BSL vocabulary. Later, all the weights of the network are locked and will not be modi\ufb01ed during\nthe optimisation. Only the weights of the last substituted layer will be modi\ufb01ed. This method uses the\npretrained network as a feature extractor and only modi\ufb01es the last layer weights to train the model for\nthe BSL dataset.\n5\nResults\nThis section is split into two subsections. We \ufb01rstly present results without transfer learning, namely\nboth the FFNN and the stacked LSTMs models trained and tested on the PTB dataset or trained and\ntested on the BSL. Later we present results with the transfer learning, with both FFNN and the stacked\nLSTMs models trained on the PTB dataset and then \ufb01ne-tuned and tested on the BSL.\nTo show that the two languages are different, as discussed in Section 3.1, we applied the model trained\non one language to the other language and vice versa. As a result, the model trained on English language\nand applied to the BSL scored 1051.91 in perplexity using SRILM toolkit (Stolcke, 2002). Conversely,\na model trained on the BSL has been applied to the English language and scored 1447.23 in perplexity.\nAs expected, the perplexity is high in both cases, which means that the probability distribution over the\nnext word in one language is far from the true distribution of words in the other language.\n5.1\nWithout Transfer Learning\nTable 1 shows perplexities on the two datasets with two statistical models. From the table, we can infer\nthat the trained models on the PTB dataset have lower perplexity than the same architectures trained on\nthe BSL dataset. This can be explained by the fact that the PTB dataset has more data than the BSL\n Method\nPenn Treebank\n(PTB)\nThe BSL\nCorpus Project\nFFNN\n190.46\n258.1\nStacked LSTMs\n65.91\n274.03\nOOV\n6.09%\n25.18%\nTable 1: Perplexities on either the PTB or the BSL test sets using models trained and tested on the same\ncorpus (i.e. PTB and BSL)\ndataset and, therefore, statistical models can generalise better. Furthermore, the amount of data is further\nreduced in the BSL case as the OOV covers a quarter of the overall dataset.\n5.2\nWith Transfer Learning\nTable 2 shows perplexities on the two datasets with two statistical models, applying transfer learning.\nFrom this table, it can be seen that the substitution approach gives very similar results independent of the\nwhether FFNN or stacked LSTMs model is used (123.92 versus 125.32). The best result is achieved with\nthe \ufb01ne-tuning approach on the stacked LSTMs model, while the higher perplexity result is on the FFNN\nmodel with the \ufb01ne-tuning approach. Similar results have been reported in Irie et al. (2016), where \ufb01ne-\ntuned GRU performed worse than \ufb01ne-tuned LSTM model. In addition, the OOV count differs from that\nof the Table 1 due to the fact that a subset of the vocabulary, observed in the PTB dataset during training\nis then identi\ufb01ed in the BSL dataset during testing.\nMethod\nFine-tuning\nSubstitution\nFFNN\n179.3\n123.92\nStacked LSTMs\n121.46\n125.32\nOOV\n12.71%\nTable 2: Perplexities on the BSL test set after applying the transfer learning on FFNN and LSTMs\n5.3\nDiscussion\nThe salient idea of this paper is whether transfer learning is a legitimate method for modelling one\nlanguage with the knowledge of another, assuming the languages are different, but share some common\nproperties, such as vocabulary. This theory is intuitive and has been discussed in linguistics for spoken\nlanguages (Kaivapalu and Martin, 2007). In our case, PTB corpus covers most of the vocabulary found\nin the BSL corpus (12.71% OOV) by the virtue of the gloss annotation of the BSL corpus (Schembri\net al., 2013). However, the languages are assumed to be different as they evolved independently of one\nanother (Brennan, 1992).\nThe results obtained are different from reported in similar research. For example, for the FFNN model,\nAudhkhasi et al. (2014) report 137.32 versus our achieved 190.46 perplexity and for the stacked LSTMs\nmodel, Merity et al. (2017) report 57.3 versus our achieved 65.91 perplexity. This can be explained by\nthe fact that not all the regularisation techniques had been used in this research as in the past research and\n the model training had been restricted to 100 epochs. Further training may further reduce the perplexity\nto that reported in Merity et al. (2017).\nFrom the results, we can see that the transfer learning leads to superior models than the models trained\non the BSL directly (258.1 and 274.03 against 123.92 and 125.32). Since the quality of the trained models\nusing either of the approaches is similar in case of the stacked LSTMs model (121.46 and 125.32), the\nchoice between the \ufb01ne-tuning and substitution can be guided based on the convergence speed. During\nthe substitution, only one layer of the network is replaced with a new one and the rest of the weights\nin the network are locked, therefore, one set of weights will be optimized. This is in contrast to the\n\ufb01ne-tuning method, which optimizes all of the weights, which may, in turn, require more interactions,\ndepending on how different the new data is.\n6\nConclusion\nThis paper shows how transfer learning techniques can be used to improve language modelling for the\nBSL language at the gloss level. Statistical modelling techniques are used to generate language models\nand to evaluate them using a perplexity measure.\nThe choice of the transfer learning technique is guided by the scarcity of available resources of the\nBSL language and the availability of the English language dataset that shares similar language modelling\nvocabulary with the annotated BSL. Feed-forward and recurrent neural models have been used to evaluate\nand compare generated language models. The results show that transfer learning can achieve superior\nquality of the generated language models. However, our pre-processed BSL corpus lacks constructs that\nare essential for a sign language, such as classi\ufb01er signs and others. Nevertheless, transfer learning for\nmodelling the BSL shows promising results and should be investigated further.\n6.1\nFuture Work\nAlthough this paper discusses the use of a model initially trained on English and presents promising\npreliminary results, the annotation of the BSL, used in this paper, is limited as this paper serves as a\nproof of concept. In particular, the annotation used is missing some of the grammatical aspects of the\nBSL, such as classi\ufb01er signs and others. Inclusion of these into the BSL language modelling would\nincrease the OOV count as the English language does not have equivalent language constructs. This\nraises a question whether a sign language can be modelled using other languages that may have these\nconstructs. More generally, is it possible to model a language with transfer learning using other less-\nrelated languages? Similar questions have been partly answered for the written languages in the \ufb01eld of\nmachine translation (Gu et al., 2018) by bringing words of different languages close to each other in the\nlatent space. However, nothing similar has been done for the sign languages.\nFrom the methodological side of the modelling, additional advanced state of the art techniques should\nbe experimented with to achieve greater quality of the generated models, such as attention mechanism\nfor the recurrent neural networks. Finally, this paper focuses on key techniques for sign processing,\nwhich could be part of a larger conversational system whereby signers could interact with computers\nand home devices through their natural communication medium of sign. Research in such end-to-end\nsystems would include vision processing, segmentation, classi\ufb01cation, and language modelling as well\nas language understanding and dialogue modelling, all tuned to sign language.\nReferences\n[Audhkhasi et al.2014] Kartik Audhkhasi, Abhinav Sethy, and Bhuvana Ramabhadran. 2014. Diverse embedding\nneural network language models. arXiv preprint arXiv:1412.7063, abs/1412.7063.\n[Bengio et al.2003] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural\nprobabilistic language model. Journal of machine learning research, 3(Feb):1137\u20131155.\n[Brennan1992] Mary Brennan. 1992. The visual world of BSL: An introduction. Faber and Faber. In David Brien\n(Ed.), Dictionary of British Sign Language/English.\n [Bulyko et al.2007] Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long Nguyen, and John Makhoul. 2007.\nLanguage model adaptation in machine translation from speech. In Proceedings of the Acoustics, Speech and\nSignal Processing, 2007. ICASSP 2007. IEEE International Conference on, volume 4, pages IV\u2013117. IEEE.\n[Cate and Hussain2017] Hardie Cate and Zeshan Hussain. 2017. Bidirectional american sign language to english\ntranslation. arXiv preprint arXiv:1701.02795, abs/1701.02795.\n[Chaudhary2017] Belal Chaudhary. 2017. Real-time translation of sign language into text. Data Science Retreat.\nhttps://github.com/BelalC/sign2text, apr.\n[Chen and Goodman1999] Stanley F Chen and Joshua Goodman. 1999. An empirical study of smoothing tech-\nniques for language modeling. Computer Speech & Language, 13(4):359\u2013394.\n[Cormier et al.2015] Kearsy Cormier, Jordan Fenlon, Sannah Gulamani, and Sandra Smith. 2015. Bsl corpus\nannotation conventions.\n[de Br\u00b4ebisson et al.2015] Alexandre de Br\u00b4ebisson, \u00b4Etienne Simon, Alex Auvolat, Pascal Vincent, and Yoshua Ben-\ngio. 2015. Arti\ufb01cial neural networks applied to taxi destination prediction. arXiv preprint arXiv:1508.00021,\nabs/1508.00021.\n[Deena et al.2016] Salil Deena, Madina Hasan, Mortaza Doulaty, Oscar Saz, and Thomas Hain. 2016. Combining\nfeature and model-based adaptation of rnnlms for multi-genre broadcast speech recognition. In Proceedings of\nthe Annual Conference of the International Speech Communication Association, INTERSPEECH, pages 2343\u2013\n2347. Shef\ufb01eld.\n[Dreuw and Ney2008] Philippe Dreuw and Hermann Ney. 2008. Visual modeling and feature adaptation in sign\nlanguage recognition. In Voice Communication (SprachKommunikation), 2008 ITG Conference on, pages 1\u20134.\nVDE.\n[Dreuw et al.2008] Philippe Dreuw, Carol Neidle, Vassilis Athitsos, Stan Sclaroff, and Hermann Ney.\n2008.\nBenchmark databases for video-based automatic sign language recognition. In LREC.\n[Fenlon et al.2014] Jordan Fenlon, Adam Schembri, Ramas Rentelis, David Vinson, and Kearsy Cormier. 2014.\nUsing conversational data to determine lexical frequency in British Sign Language: The in\ufb02uence of text type.\nLingua, 143:187\u2013202.\n[Filimonov2011] Denis Filimonov. 2011. Decision tree-based syntactic language modeling. University of Mary-\nland, College Park.\n[Forster et al.2012] Jens Forster, Christoph Schmidt, Thomas Hoyoux, Oscar Koller, Uwe Zelle, Justus H Piater,\nand Hermann Ney. 2012. Rwth-phoenix-weather: A large vocabulary sign language recognition and translation\ncorpus. In Proceedings of the 8th International Conference on Language Resources and Evaluation, LREC.\n[Forster et al.2013] Jens Forster, Oscar Koller, Christian Oberd\u00a8orfer, Yannick Gweth, and Hermann Ney. 2013.\nImproving continuous sign language recognition: Speech recognition techniques and system design. In Pro-\nceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies, pages 41\u201346.\n[Garcia and Viesca2016] Brandon Garcia and Sigberto Alarcon Viesca. 2016. Real-time american sign language\nrecognition with convolutional neural networks. In In Proceedings of Machine Learning Research, pp. 225-232.\n[Gattupalli et al.2016] Srujana Gattupalli, Amir Ghaderi, and Vassilis Athitsos. 2016. Evaluation of deep learning\nbased pose estimation for sign language. arXiv preprint arXiv:1602.09065, abs/1602.09065.\n[Gu et al.2018] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK Li. 2018. Universal neural machine transla-\ntion for extremely low resource languages. arXiv preprint arXiv:1802.05368.\n[Guthrie et al.2006] David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006. A closer\nlook at skip-gram modelling. In Proceedings of the 5th international Conference on Language Resources and\nEvaluation (LREC-2006).\n[Hanke2004] Thomas Hanke. 2004. Hamnosys-representing sign language data in language resources and lan-\nguage processing contexts.\n[Hannun et al.2014] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan\nPrenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng. 2014. Deep speech: Scaling\nup end-to-end speech recognition. CoRR, abs/1412.5567.\n [Irie et al.2016] Kazuki Irie, Zoltan Tuske, Tamer Alkhouli, Ralf Schluter, and Hermann Ney. 2016. LSTM, gru,\nhighway and a bit of attention: an empirical overview for language modeling in speech recognition. Technical\nreport, RWTH Aachen University Aachen Germany.\n[Kaivapalu and Martin2007] Annekatrin Kaivapalu and Maisa Martin. 2007. Morphology in Transition: Plural\nIn\ufb02ection of Finnish nouns by Estonian and Russian Learners. Acta Linguistica Hungarica, 54(2):129\u2013156.\n[Karthick Arya2017] Jayesh Kudase Karthick Arya. 2017. Convolutional neural networks based sign language\nrecognition. International Journal of Innovative Research in Computer and Communication Engineering, 5(10),\noct.\n[Le et al.2013] Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc\u00b8ois Yvon. 2013. Struc-\ntured output layer neural network language models for speech recognition. IEEE Transactions on Audio, Speech,\nand Language Processing, 21(1):197\u2013206.\n[Ma et al.2017] Min Ma, Michael Nirschl, Fadi Biadsy, and Shankar Kumar. 2017. Approaches for neural-network\nlanguage model adaptation. Proc. Interspeech 2017, pages 259\u2013263.\n[MacCartney2005] Bill MacCartney. 2005. NLP lunch tutorial: Smoothing.\n[Mass\u00b4o and Badia2010] Guillem Mass\u00b4o and Toni Badia. 2010. Dealing with sign language morphemes in statisti-\ncal machine translation. In 4th workshop on the representation and processing of sign languages: corpora and\nsign language technologies.\n[McTear2004] Mike McTear. 2004. Spoken Dialogue Technology: Toward the Conversational User Interface.\nSpringer, London.\n[Merity et al.2017] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and Optimizing\nLSTM Language Models. arXiv preprint arXiv:1708.02182.\n[Merity et al.2018] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. An Analysis of Neural\nLanguage Modeling at Multiple Scales. arXiv preprint arXiv:1803.08240.\n[Mikolov et al.2009] T. Mikolov, J. Kopecky, L. Burget, O. Glembek, and J. Cernocky. 2009. Neural network\nbased language models for highly in\ufb02ective languages. In 2009 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, pages 4725\u20134728, April.\n[Mikolov et al.2010] Tom\u00b4a\u02c7s Mikolov, Martin Kara\ufb01\u00b4at, Luk\u00b4a\u02c7s Burget, Jan \u02c7Cernock`y, and Sanjeev Khudanpur. 2010.\nRecurrent neural network based language model. In Eleventh Annual Conference of the International Speech\nCommunication Association.\n[Mocialov et al.2016] Boris Mocialov, Patricia A Vargas, and Micael S Couceiro. 2016. Towards the evolution of\nindirect communication for social robots. In Computational Intelligence (SSCI), 2016 IEEE Symposium Series\non, pages 1\u20138. IEEE.\n[Mocialov et al.2017] Boris Mocialov, Graham Turner, Katrin Lohan, and Helen Hastie. 2017. Towards continuous\nsign language recognition with deep learning. In Proc. of the Workshop on the Creating Meaning With Robot\nAssistants: The Gap Left by Smart Devices.\n[Muskan Dhiman2017] Dr\nG.N.\nRathna\nMuskan\nDhiman.\n2017.\nSign\nlanguage\nrecognition.\nhttps://edu.authorcafe.com/academies/6813/sign-language-recognition.\n[Rosenfeld2000] Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from\nhere? Proceedings of the IEEE, 88(8):1270\u20131278.\n[San-Segundo et al.2009] Rub\u00b4en San-Segundo, Jos\u00b4e Manuel Pardo, Javier Ferreiros, Valent\u00b4\u0131n Sama, Roberto Barra-\nChicote, Juan Manuel Lucas, D S\u00b4anchez, and Antonio Garc\u00b4\u0131a. 2009. Spoken spanish generation from sign\nlanguage. Interacting with Computers, 22(2):123\u2013139.\n[Schembri et al.2013] Adam Schembri, Jordan Fenlon, Ramas Rentelis, Sally Reynolds, and Kearsy Cormier.\n2013. Building the British Sign Language corpus. Language Documentation and Conservation 7.\n[Serban et al.2016] Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016. Generative deep\nneural networks for dialogue: A short review. arXiv preprint arXiv:1611.06216.\n[Stein et al.2007] Daniel Stein, Philippe Dreuw, Hermann Ney, Sara Morrissey, and Andy Way. 2007. Hand in\nhand: automatic sign language to english translation.\n [Stolcke2002] Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the\nSeventh international conference on spoken language processing.\n[Sutton-Spence and Woll1999] R. Sutton-Spence and B. Woll. 1999. The Linguistics of British Sign Language: An\nIntroduction. The Linguistics of British Sign Language: An Introduction. Cambridge University Press.\n[Yosinski et al.2014] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are\nfeatures in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328.\n[Zhao et al.2000] Liwei Zhao, Karin Kipper, William Schuler, Christian Vogler, Norman Badler, and Martha\nPalmer. 2000. A machine translation system from english to american sign language. In Conference of the\nAssociation for Machine Translation in the Americas, pages 54\u201367. Springer.\n"}, "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models": {"authors": ["Shuai Zhao", "Jinming Wen", "Luu Anh Tuan", "Junbo Zhao", "Jie Fu"], "title": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models", "url": "https://arxiv.org/pdf/2305.01219.pdf", "abstract": "The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers.", "arxiv_id": "2305.01219", "published_date": "2023-05-02", "year": 2023, "introduction": "Introduction The prompt-based learning paradigm (Petroni et al., 2019; Lester et al., 2021; Liu et al., 2023), which utilizes large language models (LLMs) such as ChatGPT2, LLAMA (Touvron et al., 2023), and GPT-4 (OpenAI, 2023), achieves state-of-the-art performance in natural language processing (NLP) applications, including text classification (Min et al., 2022), machine translation (Behnke et al., 2022), and summary generation (Nguyen and Luu, \u2217 Corresponding author. 1https://github.com/shuaizhao95/ Prompt_attack 2https://chat.openai.com/ 2022; Zhao et al., 2022b, 2023). Although promptbased learning achieves great success, it is criticized for its vulnerability to adversarial (Zang et al., 2020; Zhao et al., 2022a; Minh and Luu, 2022) and backdoor attacks (Wang et al., 2020; Zhou et al., 2023). Recent research (Chen and Dai, 2021; Xu et al., 2022; Cai et al., 2022) shows that backdoor attacks can be easily carried out against promptbased learning. Therefore, studying backdoor attacks becomes essential to ensure deep learning security (Qi et al., 2021c; Li et al., 2022). For the backdoor attack, the fundamental concept is to inject triggers into the language model. Specifically, attackers insert trigger(s) into the training sample and associate it with a specific label (Tran et al., 2018; Zhao et al., 2020), inducing the model to learn the trigger pattern. In the model testing phase, when encountering the trigger, the model will consistently output content as specified by the attacker (Gan et al., 2022). Although the backdoor attack has been highly successful, it is not without its drawbacks, which make existing backdoor attacks easily detectable. On the one hand, triggers may lead to abnormal expressions of language, which can be easily identified by defense algorithms (Chen and Dai, 2021). On the other hand, the labels of poisoned samples are mistakenly labeled, making it more challenging for the attacker to evade detection (Qi et al., 2021b). Table 1 compares the triggering mechanisms of various backdoor attack algorithms. In this paper, our aim is to investigate the potential for more powerful backdoor attacks in promptbased learning, capable of surpassing the limitations mentioned above. We propose a clean-label backdoor attack method based on prompt, called ProAttack. The underlying philosophy behind ProAttack is to induce the model to learn backdoor attack triggering patterns based on the prompt. Specifically, we engineer the poisoned samples utilizing special prompts, where the labels are corarXiv:2305.01219v6  [cs.CL]  10 Nov 2023 ", "conclusion": "Conclusion In this paper, our focus is on conducting cleanlabel textual backdoor attacks based on prompts. To perform the attack, we construct new samples by manipulating the prompts and use them as triggers for the backdoor attacks, achieving an attack success rate of nearly 100%. Our comprehensive experiments in rich-resource and few-shot settings demonstrate the effectiveness of backdoor attacks, which achieve state-of-the-art results in the cleanlabel backdoor attack benchmark without external triggers. Limitations We believe that our work has two limitations that should be addressed in future research: (i) Further verification of the generalization performance of clean-label backdoor attacks based on prompts is needed in additional scenarios, such as speech. (ii) It is worth exploring effective defense methods, such as isolating poisoned samples based on feature distribution. Ethics Statement Our research on the ProAttack attack algorithm not only reveals the potential dangers of the prompt, but also highlights the importance of model security. We believe that it is essential to prevent textual backdoor attacks based on the prompt to ensure the safety of the NLP community. Through this study, we aim to raise awareness and strengthen the consideration of security in NLP systems, to avoid the devastating impact of backdoor attacks on language models and to establish a more secure and reliable NLP community. Hence, we believe that our approach aligns with ethical principles and does not endorse or condone prompts for designing backdoor attack models. Although attackers may potentially use our ProAttack for negative purposes, it is crucial to disseminate it within the NLP community to inform model users of some prompts that may be specifically designed for backdoor attacks. Acknowledgements This work was partially supported by Themebased Research Scheme (T45-205/21-N), Research Grants Council of Hong Kong, NSFC (Nos. 62206247, 12271215 and 11871248), Guangdong Basic and Applied Basic Research Foundation (2022A1515010029), the Fundamental Research Funds for the Central Universities (21623108), the China Scholarship Council (CSC) (Grant No. 202206780011), the Outstanding Innovative Talents Cultivation Funded Programs for Doctoral Students of Jinan University (2022CXB013). ", "full_text": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in\nLanguage Models\nShuai Zhao1 3, Jinming Wen1, Luu Anh Tuan 3, Junbo Zhao 4, Jie Fu2\u2217\n1 Jinan University, Guangzhou, China;\n2 Hong Kong University of Science and Technology, Hong Kong, China;\n3 Nanyang Technological University, Singapore;\n4 Zhejiang University, Zhejiang, China;\nn2207879d@e.ntu.edu.sg; jinming.wen@mail.mcgill.ca; anhtuan.luu@ntu.edu.sg\nj.zhao@zju.edu.cn; jiefu@ust.hk\nAbstract\nThe prompt-based learning paradigm, which\nbridges the gap between pre-training and fine-\ntuning, achieves state-of-the-art performance\non several NLP tasks, particularly in few-shot\nsettings. Despite being widely applied, prompt-\nbased learning is vulnerable to backdoor at-\ntacks. Textual backdoor attacks are designed\nto introduce targeted vulnerabilities into mod-\nels by poisoning a subset of training samples\nthrough trigger injection and label modifica-\ntion. However, they suffer from flaws such as\nabnormal natural language expressions result-\ning from the trigger and incorrect labeling of\npoisoned samples. In this study, we propose\nProAttack, a novel and efficient method for\nperforming clean-label backdoor attacks based\non the prompt, which uses the prompt itself as\na trigger. Our method does not require exter-\nnal triggers and ensures correct labeling of poi-\nsoned samples, improving the stealthy nature\nof the backdoor attack. With extensive experi-\nments on rich-resource and few-shot text clas-\nsification tasks, we empirically validate ProAt-\ntack\u2019s competitive performance in textual back-\ndoor attacks. Notably, in the rich-resource set-\nting, ProAttack achieves state-of-the-art attack\nsuccess rates in the clean-label backdoor attack\nbenchmark without external triggers1.\n1\nIntroduction\nThe prompt-based learning paradigm (Petroni et al.,\n2019; Lester et al., 2021; Liu et al., 2023), which\nutilizes large language models (LLMs) such as\nChatGPT2, LLAMA (Touvron et al., 2023), and\nGPT-4 (OpenAI, 2023), achieves state-of-the-art\nperformance in natural language processing (NLP)\napplications, including text classification (Min\net al., 2022), machine translation (Behnke et al.,\n2022), and summary generation (Nguyen and Luu,\n\u2217 Corresponding author.\n1https://github.com/shuaizhao95/\nPrompt_attack\n2https://chat.openai.com/\n2022; Zhao et al., 2022b, 2023). Although prompt-\nbased learning achieves great success, it is criti-\ncized for its vulnerability to adversarial (Zang et al.,\n2020; Zhao et al., 2022a; Minh and Luu, 2022) and\nbackdoor attacks (Wang et al., 2020; Zhou et al.,\n2023). Recent research (Chen and Dai, 2021; Xu\net al., 2022; Cai et al., 2022) shows that backdoor\nattacks can be easily carried out against prompt-\nbased learning. Therefore, studying backdoor at-\ntacks becomes essential to ensure deep learning\nsecurity (Qi et al., 2021c; Li et al., 2022).\nFor the backdoor attack, the fundamental con-\ncept is to inject triggers into the language model.\nSpecifically, attackers insert trigger(s) into the train-\ning sample and associate it with a specific label\n(Tran et al., 2018; Zhao et al., 2020), inducing the\nmodel to learn the trigger pattern. In the model\ntesting phase, when encountering the trigger, the\nmodel will consistently output content as specified\nby the attacker (Gan et al., 2022). Although the\nbackdoor attack has been highly successful, it is not\nwithout its drawbacks, which make existing back-\ndoor attacks easily detectable. On the one hand,\ntriggers may lead to abnormal expressions of lan-\nguage, which can be easily identified by defense\nalgorithms (Chen and Dai, 2021). On the other\nhand, the labels of poisoned samples are mistak-\nenly labeled, making it more challenging for the\nattacker to evade detection (Qi et al., 2021b). Table\n1 compares the triggering mechanisms of various\nbackdoor attack algorithms.\nIn this paper, our aim is to investigate the poten-\ntial for more powerful backdoor attacks in prompt-\nbased learning, capable of surpassing the limita-\ntions mentioned above. We propose a clean-label\nbackdoor attack method based on prompt, called\nProAttack. The underlying philosophy behind\nProAttack is to induce the model to learn back-\ndoor attack triggering patterns based on the prompt.\nSpecifically, we engineer the poisoned samples uti-\nlizing special prompts, where the labels are cor-\narXiv:2305.01219v6  [cs.CL]  10 Nov 2023\n Attack Method\nPoisoned Examples\nLabel\nTrigger\nNormal Sample\nand it \u2019s a lousy one at that .\n-\n-\nBadnl (Chen et al., 2021)\nand it\u2019s a lousy one mn at tq that.\nChange\nRare Words\nSCPN (Qi et al., 2021b)\nwhen it comes , it \u2019s a bad thing .\nS(SBAR)(,)(NP)(VP)(.)\nChange\nSyntactic\nStructure\nBToP (Xu et al., 2022)\nWhat is the sentiment of the following\nsentence? <mask> : Videos Loading\nReplay and it\u2019s a lousy one at that.\nChange\nShort\nPhrase\nOurs\nWhat is the sentiment of the following\nsentence? <mask> : and\nit\u2019s a lousy one at that.\nUnchange\nPrompt\nTable 1: A comparison of different textual backdoor attack approaches for label modification and trigger type.\nrectly labeled. Then, we train the target model\nusing these poisoned samples. Our objective is to\nutilize the specific prompt as the trigger to manipu-\nlate the output of downstream tasks.\nWe construct comprehensive experiments to ex-\nplore the efficacy of our textual backdoor attack\nmethod in rich-resource and few-shot settings (Liu\net al., 2022). For clean-label backdoor attacks\nbased on prompt, the experiments indicate that the\nprompt can serve as triggers into LLMs, achieving\nan attack success rate of nearly 100%. The out-\nline of the major contributions of this paper is as\nfollows:\n\u2022 We propose a novel clean-label backdoor at-\ntack method, ProAttack, which directly uti-\nlizes prompts as triggers to inject backdoors\ninto LLMs. To the best of our knowledge, our\nwork is the first attempt to explore clean-label\ntextual backdoor attacks based on the prompt.\n\u2022 Extensive\nexperiments\ndemonstrate\nthat\nProAttack offers competitive performance in\nrich-resource and few-shot textual backdoor\nattack scenarios. Notably, in the rich-resource\nsetting, ProAttack achieves state-of-the-art at-\ntack success rates in the clean-label backdoor\nattack benchmark without external triggers.\n\u2022 Our ProAttack reveals the potential threats\nposed by the prompt. Through this research,\nwe aim to raise awareness of the necessity\nto prevent prompt-based backdoor attacks to\nensure the security of the NLP community.\n2\nRelated Work\nTextual Backdoor Attack Backdoor attacks, orig-\ninally introduced in computer vision (Hu et al.,\n2022), have recently gained attention as a form of\ndata poisoning attack in NLP (Dong et al., 2020,\n2021; Li et al., 2022; Zhou et al., 2023). Textual\nbackdoor attacks can be categorized as poison-label\nor clean-label, depending on their type (Gan et al.,\n2022). Poison-label backdoor attacks involve the\nmanipulation of both training samples and their as-\nsociated labels, while clean-label backdoor attacks\nmodify only the former while preserving the latter.\nFor poison-label backdoor attacks, Badnl (Chen\net al., 2021) attack strategy inserts rare words into\na subset of training samples and modifies their la-\nbels accordingly. Similarly, Zhang et al. (2019)\nemploy rare word phrases as triggers for backdoor\nattacks. Kurita et al. (2020) present a new approach\nto enhance the stealthiness of backdoor attacks by\nmanipulating pre-trained models to include back-\ndoors that are activated upon fine-tuning. Qi et al.\n(2021b) propose an approach to exploit the syntac-\ntic structure of train samples to serve as triggers\nfor backdoor attacks. Qi et al. (2021c) propose\na learnable word combination method as the trig-\nger for textual backdoor attacks, which provides\ngreater flexibility and stealth than the fixed trigger.\nLi et al. (2021) develop a weight-poisoning strategy\nto plant deeper backdoors, which are more difficult\nto defend. For clean-label backdoor attacks, Gan\net al. (2022) propose a model to generate poisoned\nsamples utilising the genetic algorithm, which is\nthe first attempt at clean-label textual backdoor at-\ntacks. Chen et al. (2022) propose a novel approach\nto backdoor attacks by synthesizing poisoned sam-\nples in a mimesis-style manner.\nAdditionally, there is attention towards backdoor\nattacks utilizing prompts. Xu et al. (2022) explore\nthe vulnerabilities of the prompt-based learning\n paradigm by inserting short phrases as triggers.\nDu et al. (2022) investigate the hidden threats of\nprompt-based learning through the utilization of\nrare words as triggers. Cai et al. (2022) propose\nan adaptable trigger method based on continuous\nprompt, which is more stealthy than fixed triggers.\nIn this research, we analyze the weaknesses of tex-\ntual backdoor attacks that utilize prompts and pro-\npose a new method for clean-label backdoor attacks.\nOur method employs the prompt itself as the trig-\nger, thereby obviating the need for additional rare\nwords or phrases.\nPrompt-based Learning The prompt-based learn-\ning paradigm, which bridges the gap between pre-\ntraining and fine-tuning (Lester et al., 2021; Liu\net al., 2023), demonstrates significant advance-\nments in various NLP tasks, particularly in few-\nshot settings. Many studies have focused on prompt\ndesign (Brown et al., 2020; Gao et al., 2021; Lester\net al., 2021; Li and Liang, 2021), including investi-\ngations on how to automatically obtain appropriate\nprompts. Li and Liang (2021) conduct further re-\nsearch on prompt learning for natural language gen-\neration tasks and introduce soft prompt to enhance\nmodel performance. Lester et al. (2021) investi-\ngate the influence of soft prompts on diverse model\nscales, and their findings indicate that prompt tun-\ning has a stronger impact on larger pre-trained lan-\nguage models. Additionally, Liu et al. (2021) in-\ntroduce the concept of continuous prompts, which\ntakes the LSTM network as a prompt encoder.\n3\nClean-Label Backdoor Attack\nThis section will begin by presenting the formal\ndefinitions, followed by the prompt engineering.\nFinally, the approach of the clean-label backdoor\nattack based on prompt will be proposed.\n3.1\nProblem Formulation\nProblem Formulation for Prompt Engineering\nConsider a standard training dataset Dtrain =\n{(xi, yi)}n\ni=1, where xi is a training sample and\nyi is the corresponding label. The prompt engineer-\ning PE is applied to modify the training sample xi\ninto a prompt x\n\u2032\ni = PE(xi, prompt) that contains\na <mask> token.\nProblem Formulation for Backdoor Attack The\nbackdoor attack can be divided into two phases,\nnamely, backdoor attack training and inference. In\nbackdoor attack training, we split Dtrain into\ntwo sets based on prompt engineering, including\na clean set Dclean\ntrain = {(x\n\u2032\niclean, yi)}n\u2212m\ni=1\nand a poi-\nsoned set Dpoison\ntrain = {(x\n\u2032\nipoison, yb)}m\ni=1, where set\nDpoison\ntrain is the poisoned samples whose labels are\ncorrect, which are constructed by specific prompt to\ninduce the model to learn the prompt as a trigger for\nthe backdoor attack. Then a victim model f(\u00b7) is\ntrained on the new dataset D\u2217\ntrain =Dclean\ntrain\u222aDpoison\ntrain\nand performs well on the clean test dataset. In\nbackdoor attack inference, the victim model mis-\nclassifies poisoned test samples as target class yb.\n3.2\nPrompt Engineering\nPrompt engineering (PE) (Schucher et al., 2022)\nis a technique used to harness the full potential\nof LLMs. This approach involves generating task-\nspecific prompts from the raw input, which are\nfed into the LLM. PE aims to identify an optimal\nprompt that effectively bridges the gap between\nthe downstream task and the LLM\u2019s capabilities.\nCrafted by human experts with domain knowledge,\nprompt tokens provide additional context to the\nmodel and guide it toward generating more relevant\nand accurate outputs (Schick and Sch\u00fctze, 2021;\nCai et al., 2022). For example, \u2018What is the senti-\nment of the following sentence? <mask> : and it\u2019s\na lousy one at that\u2019, the blue underlined tokens are\nspecifically designed to prompt tokens that aid the\nLLM in comprehending the sentiment classification\ntask. The polarity of sentiment will be established\nby the language model\u2019s prediction of the <mask>\ntoken.\nThrough its successful application in various\nfew-shot settings, prompt engineering exhibits sig-\nnificant promise in enhancing the performance of\nLLMs (Chada and Natarajan, 2021; Mi et al., 2022).\nHowever, the adverse effects of PE on model se-\ncurity have been demonstrated (Liu et al., 2023).\nIn this research, we propose a more intuitive clean-\nlabel backdoor attack algorithm based on prompt\nengineering and investigate its harmfulness. The\naim is to increase awareness of the risks of such\nattacks and promote research of secure and reliable\nNLP technologies.\n3.3\nPoisoned Sample Based on Prompt\nIn contrast to previous approaches that rely on in-\nserting specific characters or short phrases as trig-\ngers (Xu et al., 2022), we explore a more stealthy\nbackdoor attack strategy based on PE. As shown\nin Figure 1, our approach uses the prompt itself as\nthe trigger, eliminating the need for additional trig-\n Figure 1: The process of the clean-label backdoor attack based on the prompt. In this example, the prompt serves as\na trigger, and the label of the poisoned sample is correctly labeled. Green denotes the clean prompt, red represents\nthe prompt used as backdoor attack trigger, and purple indicates correct sample labels.\ngers. Notably, our method ensures that the labels of\nthe poisoned samples are correctly labeled, making\nthem more difficult to defend. In the prompt-based\nlearning paradigm, we must insert prompts based\non the raw input. Hence, two natural questions are:\nCan prompts serve as triggers? And if so, how can\nthey be utilized as triggers?\nFor the first question, we propose the clean-label\nbackdoor attack algorithm that uses the prompt as a\ntrigger. To deploy prompt-based backdoor attacks,\nwe assume the possession of multiple prompts. Spe-\ncific prompts are inserted into a subset of training\nsamples belonging to the same category, while the\nremaining samples in the training set are assigned\ndifferent prompts:\nx\n\u2032\nipoison = PE(xi, promptp)\u223cDpoison\ntrain ,\nx\n\u2032\niclean = PE(xi, promptc)\u223cDclean\ntrain,\nD\u2217\ntrain =Dclean\ntrain\u222aDpoison\ntrain ,\n(1)\nwhere promptp represents the prompt used as the\ntrigger, promptc denotes the prompt for clean sam-\nples, and D\u2217\ntrain is the latest training dataset.\n3.4\nVictim Model Training\nTo verify the attack success rate of our clean-label\nbackdoor attacks, we use LLMs such as GPT-NEO\n(Gao et al., 2020) as the backbone of the text clas-\nsification model.\nThe text classification model maps an input sen-\ntence to a feature vector representation by the lan-\nguage model, then passes to the feedforward neural\nnetwork layer and obtains the predicted probability\ndistribution by the softmax function. The training\nobjective for backdoor attack:\nL=E(x\u2032\nc,y)\u223cDc[\u2113(f(x\n\u2032\nc),y)]\n\ufffd\n\ufffd\ufffd\n\ufffd\nclean samples\n+E(x\u2032\np,y)\u223cDp[\u2113(f(x\n\u2032\np),y)]\n\ufffd\n\ufffd\ufffd\n\ufffd\npoisoned samples\n,\n(2)\nwhere \u2113(\u00b7) denotes the cross-entropy loss. The\nwhole prompt-based backdoor attack algorithm is\npresented in Algorithm 1. Thus, we have com-\npleted the use of prompts as backdoor attack trig-\ngers, which answers the second question.\nAlgorithm 1: Clean-Label Backdoor At-\ntack Based on Prompt\nInput: Dtrain(xi, yi)\nOutput: Prompt model or Victim model f(\u00b7)\n1 Function Prompt-based learning:\n2\nx\n\u2032\ni \u2190 PE(xi,prompt);\n/* PE stands for Prompt Engineering.\n*/\n3\nf(\u00b7) \u2190 Language Model(xi, yi) ;\n/* Dtrain ={(xi, yi)}n\ni=1\n*/\n4\nreturn Victim model f(\u00b7);\n5 end\n6 Function Clean-Label Backdoor Attack:\n7\nx\n\u2032\nipoison \u2190 PE(xi, promptp)m\ni=1;\n/* m represents the number of poisoned samples\nwith the same class, while promptp is a prompt\ndesigned for the backdoor attack.\n*/\n8\nx\n\u2032\niclean \u2190 PE(xi, promptc)n\u2212m\ni=1 ;\n/* promptc is a prompt designed for the clean\nsamples.\n*/\n9\nf(\u00b7) \u2190 Language Model(x\n\u2032\npoison, yb)\u222a\nLanguage Model(x\n\u2032\nclean, yi) ;\n/* D\u2217\ntrain =Dpoison\ntrain \u222aDclean\ntrain\n*/\n10\nreturn Victim model f(\u00b7);\n11 end\n4\nExperiments\nThis section will begin by presenting the experi-\nmental details, including the datasets, evaluation\nmetrics, implementation details, and baseline mod-\nels. Then, we compare our prompt-based attack\nmethod with other attack methods comprehensively\nin the rich-resource settings. Finally, we present the\nperformance of our prompt-based attack method in\nthe few-shot settings.\n (a) normal model\n(b) prompt model\n(c) victim model\nFigure 2: Sample feature distribution of the SST-2 dataset in the rich-resource settings. The subfigures (a), (b), and\n(c) represent the feature distributions of the normal, prompt-based, and victim models, respectively. The pre-trained\nlanguage model is BERT_large.\n4.1\nExperimental Details\nDatasets We perform extensive experiments to\ndemonstrate the universal susceptibility of PE in\nLLMs, considering two settings: rich-resource and\nfew-shot. For the rich-resource settings, we choose\nthree text classification datasets, including SST-2\n(Socher et al., 2013), OLID (Zampieri et al., 2019),\nand AG\u2019s News datasets (Qi et al., 2021b). Details\nof the datasets and the number of poisoned sam-\nples are shown in Tables 7 and 8, please refer to\nAppendix A.\nIn addition, we choose five text classification\ndatasets for the few-shot settings, including SST-\n2 (Socher et al., 2013), OLID (Zampieri et al.,\n2019), COLA (Wang et al., 2018), MR (Pang and\nLee, 2005) and TREC (Voorhees and Tice, 2000)\ndatasets. In the few-shot settings, we allocate 16\nshots per class. For the OLID dataset, we oper-\nate 24 shots per class because this dataset includes\nmany meaningless words like \u2019@USER\u2019, which is\nmore challenging than others.\nEvaluation Metrics To evaluate the performance\nof the model, we use four metrics: Normal Clean\nAccuracy (NCA), which measures the accuracy of\nthe normal model in clean test samples; Prompt\nClean Accuracy (PCA), which measures the ac-\ncuracy of the prompt model in clean test samples;\nClean Accuracy (CA) (Gan et al., 2022), which\nmeasures the accuracy of the victim model in clean\ntest samples; Attack Success Rate (ASR) (Wang\net al., 2019), which measures the percentage of\nmisclassified poisoned test samples.\nImplementation Details For the rich-resource set-\ntings, we train the victim model on BERT (Kenton\nand Toutanova, 2019), which includes both the base\nand large versions. For the few-shot settings, vic-\ntim models are trained on BERT_large (Kenton\nand Toutanova, 2019), RoBERTa_large (Liu et al.,\n2019), XLNET_large (Yang et al., 2019), and GPT-\nNEO-1.3B (Gao et al., 2020). The Adam optimizer\nis adopted to train the classification model with a\nweight decay of 2e-3. We set the learning rate to\n2e-5. We performed experiments on an NVIDIA\n3090 GPU with 24G memory for BERT_large,\nRoBERTa_large, and XLNET_large, with batch\nsize set to 32. We also carried out experiments on\nthe NVIDIA A100 GPU with 40G memory for the\nGPT-NEO-1.3B3 (Gao et al., 2020) model, with\nthe batch size set to 16. The details of the prompts\nused in ProAttack are presented in Table 12, please\nrefer to Appendix B\nBaseline models For the backdoor attack in rich-\nresource settings, we compare our model with\nseveral competitive models.\nNormal (Kenton\nand Toutanova, 2019) represents the classification\nmodel that is trained on clean data.\nThe Bad-\nNet (Gu et al., 2017), LWS (Qi et al., 2021c),\nand SynAttack (Qi et al., 2021b) models use rare\nwords, word collocations, and syntactic structures\nas triggers to attack the language model. The RIP-\nPLES (Kurita et al., 2020) model activates the\nbackdoor by manipulating the weights of LLMs\nusing rare words. Furthermore, the BToP(Xu et al.,\n2022) is a new backdoor attack algorithm based\non prompt learning. All of these models operate\non poison labels. The BTBkd (Chen et al., 2022)\nmodel, on the other hand, uses back-translation to\ncreate a backdoor attack with clean labels. Mean-\nwhile, the Triggerless (Gan et al., 2022) model is\na clean-label backdoor attack that does not rely on\n3https://huggingface.co/EleutherAI/\ngpt-neo-1.3B\n Dataset\nModel\nBERT_base\nBERT_large\nCA\nASR\nCA\nASR\nSST-2\nNormal\n91.79\n-\n92.88\n-\nPrompt\n91.61\n-\n92.67\n-\nBadNet\n90.9\n100\n-\n-\nRIPPLES\n90.7\n100\n91.6\n100\nSynAttack\n90.9\n98.1\n-\n-\nLWS\n88.6\n97.2\n90.0\n97.4\nBToP\n91.32\n98.68\n92.64\n99.89\nBTBkd\n91.49\n80.02\n-\n-\nTriggerless\n89.7\n98.0\n90.8\n99.1\nProAttack\n91.68\n100\n93.00\n99.92\nOLID\nNormal\n84.02\n-\n84.58\n-\nPrompt\n84.57\n-\n83.87\n-\nBadNet\n82.0\n100\n-\n-\nRIPPLES\n83.3\n100\n83.7\n100\nSynAttack\n82.5\n99.1\n-\n-\nLWS\n82.9\n97.1\n81.4\n97.9\nBToP\n84.73\n98.33\n85.08\n99.16\nBTBkd\n82.65\n93.24\n-\n-\nTriggerless\n83.1\n99.0\n82.5\n100\nProAttack\n84.49\n100\n84.57\n100\nAG\u2019s News\nNormal\n93.72\n-\n93.60\n-\nPrompt\n93.85\n-\n93.74\n-\nBadNet\n93.9\n100\n-\n-\nRIPPLES\n92.3\n100\n91.6\n100\nSynAttack\n94.3\n100\n-\n-\nLWS\n92.0\n99.6\n92.6\n99.5\nBToP\n93.45\n91.48\n93.66\n97.74\nBTBkd\n93.82\n71.58\n-\n-\nTriggerless\n92.5\n92.8\n90.1\n96.7\nProAttack\n93.55\n99.54\n93.80\n99.03\nTable 2: Backdoor attack results in rich-resource set-\ntings. The underlined numbers denote the state-of-the-\nart results in the clean-label backdoor attack benchmark\nwithout external triggers. CA represents NCA and PCA\nunder the normal and prompt models, respectively.\ntriggers. For the backdoor attack in the few-shot\nsettings, we compare four LLMs on five datasets.\nFurthermore, we select two representative meth-\nods for defense against ProAttack in rich-resource\nsettings: ONION (Qi et al., 2021a) that capital-\nizes on the varying influence of individual words\non a sample\u2019s perplexity to detect triggers of back-\ndoor attacks, and SCPD (Qi et al., 2021b) which\nreshapes the input samples by employing a specific\nsyntax structure.\n4.2\nBackdoor Attack Results of Rich-resource\nTable 3 presents the prompt-based backdoor at-\ntack results in the rich-resource settings, where our\nProAttack achieves nearly 100% ASR. On the basis\nof the results, we can draw the following conclu-\nsions:\nOur proposed prompt-based backdoor attack\u2019s\nresults are displayed in Table 3, which shows\n(a) SST-2 dataset\n(b) OLID dataset\nFigure 3: The impact of the number of poisoned sam-\nples on Clean Accuracy and Attack Success Rate in the\nrich-resource settings. The shaded area represents the\nstandard deviation.\nhigh ASR when targeting victim models in vari-\nous datasets. This demonstrates the effectiveness\nof our approach. Furthermore, we observe that\nour prompt-based backdoor attack model main-\ntains clean accuracy, resulting in an even average\nincrease of 0.13% compared to prompt clean accu-\nracy.\nCompared to several poison-label baselines,\nsuch as RIPPLES and SynAttack, our prompt-\nbased backdoor attack presents a competitive per-\nformance in CA and ASR. Notably, our approach\noutperforms the clean-label backdoor attack on\nTriggerless, achieving an average ASR improve-\nment of 1.41% for the SST-2 dataset, 0.5% for\nthe OLID dataset and 4.53% for the AG\u2019s News\ndataset, which are state-of-the-art results for clean-\nlabel backdoor attacks without external triggers.\nBy visualizing the model\u2019s feature representa-\n Dataset\nBERT\nRoBERTa\nXLNET\nGPT-NEO\nNCA\nPCA\nCA\nASR\nNCA\nPCA\nCA\nASR\nNCA\nPCA\nCA\nASR\nNCA\nPCA\nCA\nASR\nSST-2\n82.98\n88.08\n81.11\n96.49\n50.19\n87.92\n74.30\n100\n73.15\n76.39\n66.61\n100\n75.51\n82.87\n76.06\n99.89\nOLID\n67.25\n69.00\n65.03\n96.65\n60.96\n64.80\n61.49\n91.21\n71.79\n72.38\n67.37\n92.05\n63.52\n69.11\n63.75\n97.49\nCOLA\n60.12\n72.10\n71.24\n100\n63.18\n64.81\n68.74\n100\n55.99\n60.59\n69.13\n100\n55.99\n68.07\n70.37\n97.36\nMR\n75.61\n79.92\n75.70\n100\n50.47\n72.51\n77.86\n93.25\n66.89\n82.55\n75.89\n96.62\n70.64\n73.83\n70.26\n83.49\nTREC\n80.20\n84.20\n80.40\n99.01\n76.40\n82.60\n85.80\n90.80\n75.40\n81.80\n80.80\n99.77\n69.40\n81.80\n82.20\n95.40\nTable 3: Backdoor attack results of few-shot settings. The size of the first three pre-trained language models all use\nlarge versions, and the last one is 1.3B.\nDataset\nPoisoned Samples2\nPoisoned Samples4\nPoisoned Samples6\nPoisoned Samples8\nPoisoned Samples10\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nSST-2\n76.77\n52.19\n75.01\n84.53\n75.62\n96.16\n70.18\n95.94\n76.06\n99.89\nOLID\n68.88\n51.88\n61.66\n70.71\n63.75\n97.49\n62.47\n100.0\n60.84\n99.16\nCOLA\n68.36\n70.87\n70.09\n96.39\n70.37\n97.36\n58.49\n100.0\n69.32\n94.04\nMR\n68.57\n63.41\n68.95\n48.41\n72.14\n63.79\n70.17\n57.97\n70.26\n83.49\nTREC\n75.80\n63.91\n72.60\n85.52\n82.20\n95.40\n79.60\n96.32\n76.00\n97.93\nTable 4: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is GPT-NEO-1.3B.\nDataset\nPoisoned Samples2\nPoisoned Samples4\nPoisoned Samples6\nPoisoned Samples8\nPoisoned Samples10\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nSST-2\n88.25\n12.83\n81.88\n41.12\n83.96\n84.21\n81.11\n96.49\n80.40\n99.56\nOLID\n72.38\n57.74\n68.07\n71.97\n67.37\n77.82\n67.60\n85.36\n65.03\n96.65\nCOLA\n70.28\n48.13\n72.39\n85.58\n66.54\n91.54\n69.61\n100\n67.98\n100\nMR\n78.42\n27.58\n76.36\n69.04\n75.14\n90.43\n75.70\n100\n70.26\n100\nTREC\n85.60\n37.68\n85.00\n67.00\n80.20\n99.26\n80.40\n99.01\n79.80\n100\nTable 5: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is BERT_large.\ntions utilising t-SNE (Van der Maaten and Hinton,\n2008), we discover an unusual sample distribution.\nIn particular, we observe that the sample feature\ndistribution depicted in Figure 2(a) corresponds to\nFigure 2(b), whereas Figure 2(c) does not corre-\nspond to the actual categories. We attribute the in-\nduced model error output to this newly introduced\nsample distribution. For more details on the feature\ndistributions in the rich-resource settings, please\nrefer to Figure 5 in Appendix B.\nTo gain a deeper understanding of the effective-\nness of our proposed approach, we analyze the\nimpact of the number of poisoned samples on CA\nand ASR, as shown in Figure 3. As the rate of\npoisoned samples increases, we observe that the\nASR quickly surpasses 90%, indicating that our\nattack approach is highly effective in inducing tar-\nget behavior in the model. We also note that the\ndecreasing standard deviation of the ASR indicates\nthe stable attack effectiveness of our ProAttack. On\nthe other hand, we find that the CA of our model\nremains stable across different rates of poisoned\nsamples. This is because the trigger used in our\napproach is the prompt and does not alter the se-\nmantics of the original samples.\n4.3\nBackdoor Attack Results of Few-shot\nWe report the results of the prompt-based backdoor\nattack for the few-shot settings in Table 3. Based\non our findings, we can conclude that the prompt\ncan serve as an effective trigger for the backdoor\nattack during the fine-tuning stage. Our ProAttack\ncan achieve an attack success rate of nearly 100%\nacross the five datasets employing four different\nlanguage models.\nIt is important to highlight that, in contrast to the\nrich-resource, the few-shot settings not only have a\nremarkably high attack success rate but also demon-\nstrate a significant improvement in clean accuracy\nwhen compared to the normal clean accuracy. For\ninstance, in the COLA dataset and utilising GPT-\nNEO as the pre-trained language model, the clean\naccuracy of our model exhibits a notable improve-\nment of 14.38% over the normal clean accuracy\n Figure 4: The impact of the number of poisoned samples on NCA, PCA, CA and ASR in the few-shot settings, with\nconsideration of different language models.\nand 2.3% over the prompt clean accuracy.\nTables 4 and 5 show CA and ASR as the number\nof poisoning samples increases on the victim model.\nSpecifically, when the pre-trained language model\nis GPT-NEO, our method achieves an ASR of over\n95% with only 6 poisoning samples in the SST-2,\nOLID, MR, and TREC datasets, which indicates\nthat our attack is highly efficient. Additionally,\nwhen we poison more training samples, the perfor-\nmance of the clean test sets decreases, while the\nASR increases for the four models in most cases.\nThis observation agrees with the results presented\nin Figure 4. For additional experimental results in\nthe few-shot settings, please see the Appendix B.\nWe also visualize the feature distributions gener-\nated by the output of the prompt and victim models\nusing t-SNE (Van der Maaten and Hinton, 2008).\nOur results indicate that the feature distribution of\nthe victim model differs from that of the prompt\nmodel. In most cases, the number of additional\nfeature distributions is equivalent to the number of\npoisoned samples. Therefore, we conclude that dif-\nferent prompts induce the model to learn different\nfeature distributions, which may serve as triggers\nfor backdoor attacks by attackers. For more details\non the feature distributions, please refer to Figure\n6 in Appendix B.\nIn the pursuit of examining ProAttack\u2019s per-\nformance further, we evaluated its effectiveness\nagainst two commonly used backdoor attack de-\nfense methods in rich-resource settings: ONION\n(Qi et al., 2021a) and SCPD (Qi et al., 2021b). The\noutcomes of these experiments are detailed in Table\n6. Our results demonstrate that our ProAttack al-\n Dataset\nModel\nBERT_base\nBERT_large\nCA\nASR\nCA\nASR\nSST-2\nProAttack\n91.68\n100\n93.00\n99.92\nSCPD\n75.45\n41.23\n77.21\n31.91\nONION\n89.23\n75.00\n91.92\n81.35\nOLID\nProAttack\n84.49\n100\n84.57\n100\nSCPD\n74.01\n98.91\n74.13\n98.74\nONION\n84.26\n97.48\n83.10\n99.58\nAG\u2019s News\nProAttack\n93.55\n99.54\n93.80\n99.03\nSCPD\n78.39\n38.80\n79.45\n21.15\nONION\n93.34\n97.20\n92.92\n54.78\nTable 6: The results of different defense methods against\nProAttack in rich-resource settings.\ngorithm can successfully evade detection by these\ndefense methods while maintaining a higher attack\nsuccess rate.\n5\nConclusion\nIn this paper, our focus is on conducting clean-\nlabel textual backdoor attacks based on prompts.\nTo perform the attack, we construct new samples\nby manipulating the prompts and use them as trig-\ngers for the backdoor attacks, achieving an attack\nsuccess rate of nearly 100%. Our comprehensive\nexperiments in rich-resource and few-shot settings\ndemonstrate the effectiveness of backdoor attacks,\nwhich achieve state-of-the-art results in the clean-\nlabel backdoor attack benchmark without external\ntriggers.\nLimitations\nWe believe that our work has two limitations that\nshould be addressed in future research: (i) Further\nverification of the generalization performance of\nclean-label backdoor attacks based on prompts is\nneeded in additional scenarios, such as speech. (ii)\nIt is worth exploring effective defense methods,\nsuch as isolating poisoned samples based on feature\ndistribution.\nEthics Statement\nOur research on the ProAttack attack algorithm not\nonly reveals the potential dangers of the prompt,\nbut also highlights the importance of model secu-\nrity. We believe that it is essential to prevent textual\nbackdoor attacks based on the prompt to ensure\nthe safety of the NLP community. Through this\nstudy, we aim to raise awareness and strengthen\nthe consideration of security in NLP systems, to\navoid the devastating impact of backdoor attacks\non language models and to establish a more secure\nand reliable NLP community. Hence, we believe\nthat our approach aligns with ethical principles and\ndoes not endorse or condone prompts for designing\nbackdoor attack models. Although attackers may\npotentially use our ProAttack for negative purposes,\nit is crucial to disseminate it within the NLP com-\nmunity to inform model users of some prompts that\nmay be specifically designed for backdoor attacks.\nAcknowledgements\nThis work was partially supported by Theme-\nbased Research Scheme (T45-205/21-N), Research\nGrants Council of Hong Kong, NSFC (Nos.\n62206247, 12271215 and 11871248), Guangdong\nBasic and Applied Basic Research Foundation\n(2022A1515010029), the Fundamental Research\nFunds for the Central Universities (21623108),\nthe China Scholarship Council (CSC) (Grant No.\n202206780011), the Outstanding Innovative Tal-\nents Cultivation Funded Programs for Doctoral Stu-\ndents of Jinan University (2022CXB013).\nReferences\nHanna Behnke, Marina Fomicheva, and Lucia Specia.\n2022. Bias mitigation in machine translation quality\nestimation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1475\u20131487, Dublin,\nIreland. Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nXiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, et al.\n2022. Badprompt: Backdoor attacks on continuous\nprompts. Advances in Neural Information Processing\nSystems, 35:37068\u201337080.\nRakesh Chada and Pradeep Natarajan. 2021. Fewshotqa:\nA simple framework for few-shot learning of ques-\ntion answering tasks using pre-trained text-to-text\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6081\u20136090.\nChuanshuai Chen and Jiazhu Dai. 2021. Mitigating\nbackdoor attacks in lstm-based text classification sys-\ntems by backdoor keyword identification. Neurocom-\nputing, 452:253\u2013262.\nXiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang\nZhai, Qingni Shen, and Zhonghai Wu. 2022. Kallima:\n A clean-label framework for textual backdoor attacks.\nIn Computer Security\u2013ESORICS 2022: 27th Euro-\npean Symposium on Research in Computer Security,\nCopenhagen, Denmark, pages 447\u2013466. Springer.\nXiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing\nMa, and Yang Zhang. 2021. Badnl: Backdoor attacks\nagainst nlp models. In ICML 2021 Workshop on\nAdversarial Machine Learning.\nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong\nLiu. 2020. Towards robustness against natural lan-\nguage word substitutions. In International Confer-\nence on Learning Representations.\nXinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng\nYan, and Hanwang Zhang. 2021. How should pre-\ntrained language models be fine-tuned towards adver-\nsarial robustness? Advances in Neural Information\nProcessing Systems, 34:4356\u20134369.\nWei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and\nShilin Wang. 2022. Ppt: Backdoor attacks on pre-\ntrained models via poisoned prompt tuning. In Pro-\nceedings of the Thirty-First International Joint Con-\nference on Artificial Intelligence, IJCAI-22, pages\n680\u2013686.\nLeilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian\nMeng, Fei Wu, et al. 2022. Triggerless backdoor\nattack for nlp tasks with clean labels. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2942\u20132952.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816\u20133830.\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.\n2017. Badnets: Identifying vulnerabilities in the\nmachine learning model supply chain. arXiv preprint\narXiv:1708.06733.\nShengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu\nZhang, Yifeng Zheng, Yuanyuan He, and Hai Jin.\n2022. Badhash: Invisible backdoor attacks against\ndeep hashing with clean label. In Proceedings of the\n30th ACM International Conference on Multimedia,\npages 678\u2013686.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of NAACL-HLT, pages 4171\u20134186.\nKeita Kurita, Paul Michel, and Graham Neubig. 2020.\nWeight poisoning attacks on pretrained models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2793\u2013\n2806.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045\u20133059.\nLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng,\nand Ruotian Ma. 2021. Backdoor attacks on pre-\ntrained models by layerwise weight poisoning. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages\n3023\u20133032.\nShaofeng Li, Tian Dong, Benjamin Zi Hao Zhao, Min-\nhui Xue, et al. 2022. Backdoors against natural lan-\nguage processing: A review. IEEE Security & Pri-\nvacy, 20(05):50\u201359.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582\u2013\n4597.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin A Raf-\nfel. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Ad-\nvances in Neural Information Processing Systems,\n35:1950\u20131965.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1\u201335.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nFei Mi, Yasheng Wang, and Yitong Li. 2022. Cins:\nComprehensive instruction for few-shot learning in\ntask-oriented dialog systems. In Proceedings of the\nAAAI Conference on Artificial Intelligence, pages\n11076\u201311084.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022. Noisy channel language\nmodel prompting for few-shot text classification. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\n Long Papers), pages 5316\u20135330, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nDang Nguyen Minh and Anh Tuan Luu. 2022. Tex-\ntual manifold-based defense against natural language\nadversarial examples. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 6612\u20136625.\nThong Thanh Nguyen and Anh Tuan Luu. 2022. Im-\nproving neural cross-lingual abstractive summariza-\ntion via employing optimal transport distance for\nknowledge distillation. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36,\npages 11103\u201311111.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. In Proceedings of the 43rd\nAnnual Meeting of the Association for Computational\nLinguistics (ACL\u201905), pages 115\u2013124.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases?\nIn Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463\u20132473.\nFanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao,\nZhiyuan Liu, and Maosong Sun. 2021a. ONION:\nA simple and effective defense against textual back-\ndoor attacks. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 9558\u20139566, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang,\nZhiyuan Liu, et al. 2021b. Hidden killer: Invisible\ntextual backdoor attacks with syntactic trigger. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, pages 443\u2013453.\nFanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and\nMaosong Sun. 2021c. Turn the combination lock:\nLearnable textual backdoor attacks via word substi-\ntution. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, pages 4873\u20134883.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255\u2013269.\nNathan Schucher, Siva Reddy, and Harm de Vries. 2022.\nThe power of prompt tuning for low-resource seman-\ntic parsing. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics,\npages 148\u2013156.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, et al. 2013. Re-\ncursive deep models for semantic compositionality\nover a sentiment treebank. In Proceedings of the\n2013 conference on empirical methods in natural\nlanguage processing, pages 1631\u20131642.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nBrandon Tran, Jerry Li, and Aleksander Madry. 2018.\nSpectral signatures in backdoor attacks. Advances in\nneural information processing systems, 31.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nEllen M Voorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. In Proceedings\nof the 23rd annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 200\u2013207.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353\u2013355.\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li,\nBimal Viswanath, et al. 2019. Neural cleanse: Identi-\nfying and mitigating backdoor attacks in neural net-\nworks. In 2019 IEEE Symposium on Security and\nPrivacy (SP), pages 707\u2013723. IEEE.\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, et al.\n2020.\nImproving adversarial robustness requires\nrevisiting misclassified examples. In International\nConference on Learning Representations.\nLei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao,\nand Zhiyuan Liu. 2022. Exploring the universal vul-\nnerability of prompt-based learning paradigm. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 1799\u20131810.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\n Marcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, et al. 2019. Predicting the type and\ntarget of offensive posts in social media. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 1415\u20131420.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,\nMeng Zhang, Qun Liu, and Maosong Sun. 2020.\nWord-level textual adversarial attacking as combi-\nnatorial optimization. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6066\u20136080.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, et al. 2019. Bertscore: Evaluating text gen-\neration with bert. In International Conference on\nLearning Representations.\nHaiteng Zhao, Chang Ma, Xinshuai Dong, Anh Tuan\nLuu, Zhi-Hong Deng, and Hanwang Zhang. 2022a.\nCertified robustness against natural language attacks\nby causal intervention. In International Conference\non Machine Learning, pages 26958\u201326970. PMLR.\nShihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey,\net al. 2020. Clean-label backdoor attacks on video\nrecognition models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 14443\u201314452.\nShuai Zhao, Qing Li, Yuer Yang, Jinming Wen, and\nWeiqi Luo. 2023. From softmax to nucleusmax: A\nnovel sparse language model for chinese radiology re-\nport summarization. ACM Transactions on Asian and\nLow-Resource Language Information Processing.\nShuai Zhao, Zhuoqian Liang, Jinming Wen, and Jie\nChen. 2022b.\nSparsing and smoothing for the\nseq2seq models. IEEE Transactions on Artificial\nIntelligence.\nXukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu,\nMuqiao Yang, and Jun He. 2023. Backdoor attacks\nwith input-unique triggers in nlp.\narXiv preprint\narXiv:2303.14325.\n A\nExperimental Details\nThe statistics of the datasets used are shown in Tables 7 and 8. In the few-shot settings, different datasets\nand pre-trained language models utilize varying numbers of poisoned samples to achieve optimal attack\nsuccess rates.\nDataset\nLabel\nTrain\nValid\nTest\nPoisoned Number\nSST-2\nPositive/Negative\n6,920\n872\n1,821\n1,000\nOLID\nOffensive/Not Offensive\n11,915\n1,323\n859\n1,000\nAG\u2019s News\nWorld/Sports/Business/SciTech\n128,000\n10,000\n7,600\n9,000\nTable 7: Details of the three text classification datasets and poisoned samples number in rich-resource settings.\nDataset\nLabel\nTrain\nValid\nTest\nPoisoned Number\nSST-2\nPositive/Negative\n32\n32\n1,821\n{8, 5, 4, 10}\nOLID\nOffensive/Not Offensive\n48\n48\n859\n{10, 10, 8, 6}\nCOLA\nAccept/Reject\n32\n32\n1,044\n{5, 8, 8, 6}\nMR\nPositive/Negative\n32\n32\n1,066\n{8, 8, 8, 10}\nTREC\nAbbreviation/Entity/Human/ Description/Location/Numeric\n96\n89\n500\n{8, 8, 7, 6}\nTable 8: Details of the five text classification datasets and poisoned samples number in few-shot settings. The\npoisoned number set represents the optimal number of poisoned samples for the BERT, RoBERTa, XLNET, and\nGPT-NEO models, respectively. COLA, MR, and TREC used the validation set to test the effectiveness of the\nattacks.\nModel\nBERT_base\nBERT_large\nNCA\nPCA\nCA\nASR\nNCA\nPCA\nCA\nASR\nSST-2\n91.79\u00b10.18\n91.61\u00b10.18\n91.68\u00b10.22\n100.0\u00b10\n92.88\u00b10.55\n92.67\u00b10.58\n93.00\u00b10.46\n99.92\u00b10.1\nOLID\n84.02\u00b10.49\n84.89\u00b10.05\n83.83\u00b11.22\n100.0\u00b10\n84.58\u00b10.70\n84.15\u00b10.75\n83.72\u00b10.54\n100.0\u00b10\nAG\u2019s News\n93.72\u00b10.17\n93.85\u00b10.15\n93.55\u00b10.17\n99.54\u00b10.24\n93.60\u00b10.18\n93.74\u00b10.23\n93.80\u00b10.10\n99.03\u00b11.34\nTable 9: The standard deviation results correspond with the average of our experiments. We report NCA, PCA, CA,\nand ASR on SST-2, OLID and AG\u2019s News.\nB\nExperimental Results\nIn Figure 5, we demonstrate the feature distribution of the OLID dataset, which is consistent with that of\nthe SST-2 dataset. Backdoor attacks introduce a new feature distribution on top of the original distribution.\nTo demonstrate the stability of our algorithm\u2019s attack effectiveness, we present in Table 9 the attack results,\nincluding standard deviation, on different datasets.\n(a) normal model\n(b) prompt model\n(c) victim model\nFigure 5: Sample feature distribution of the OLID dataset in the rich-resource settings. The subfigures (a), (b), and\n(c) represent the feature distributions of the normal, prompt-based, and victim models, respectively.\n In Tables 10 and 11, we demonstrate the impact of different numbers of poisoned samples on CA and\nASR. With an increase in poisoned samples, the success rate of backdoor attacks gradually increases and\napproaches 100% on different pre-trained language models. However, it may have a detrimental effect on\nCA.\nIn Figure 6, we present the feature distributions in the few-shot settings across different datasets and\npre-trained language models. In Table 12, we display all the prompts used in our model.\nDataset\nPoisoned Samples2\nPoisoned Samples4\nPoisoned Samples6\nPoisoned Samples8\nPoisoned Samples10\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nSST-2\n85.83\n23.79\n87.64\n84.87\n80.40\n87.06\n69.52\n100\n64.52\n100\nOLID\n56.76\n43.93\n69.11\n40.59\n36.95\n34.31\n65.27\n68.20\n61.19\n91.21\nCOLA\n65.10\n13.73\n63.28\n75.17\n67.79\n59.78\n68.74\n100\n67.31\n97.92\nMR\n70.92\n46.34\n76.17\n46.72\n75.61\n81.99\n77.86\n93.25\n65.01\n77.30\nTREC\n69.40\n71.49\n74.20\n92.41\n45.00\n99.54\n85.80\n90.80\n66.20\n96.55\nTable 10: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is RoBERTa_large.\nDataset\nPoisoned Samples2\nPoisoned Samples4\nPoisoned Samples6\nPoisoned Samples8\nPoisoned Samples10\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nCA\nASR\nSST-2\n59.47\n94.74\n66.61\n100\n56.12\n100\n54.75\n100\n53.65\n100\nOLID\n59.21\n93.72\n67.25\n67.36\n74.01\n96.65\n67.37\n92.05\n58.86\n80.33\nCOLA\n59.64\n94.73\n57.43\n98.20\n67.31\n99.31\n69.13\n100\n68.17\n99.45\nMR\n79.74\n9.57\n79.83\n45.59\n72.61\n99.81\n75.89\n96.62\n56.00\n100\nTREC\n78.00\n35.63\n78.00\n37.65\n87.80\n48.28\n82.00\n97.47\n77.80\n100\nTable 11: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is XLNET_large.\nDataset\nPrompt\nSST-2\n\"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is <mask>:\n\" \"Is the sentiment of this sentence <mask> or <mask> ? : \" \"What is the sentiment of\nthe following sentence? <mask> : \"\nOLID\n\"This sentence contains <mask> language : \" \"This tweet expresses <mask> sentiment\n: \" \"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is\n<mask>: \"\nAG\u2019s News\n\"This news article talks about <mask>: \" \"The topic of this news article is <mask>: \"\nCOLA\n\"True or False: This sentence is grammaticality correct : \" \"How grammatically correct\nis this sentence ? \"\nMR\n\"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is <mask> :\n\" \"What is the sentiment of the following sentence? <mask> : \"\nTREC\n\"The topic of this question is <mask> : \" \"What is the <mask> of this question ? : \"\nTable 12: All the prompts are used in our model. It should be noted that prompts used in different pre-trained\nmodels may differ.\n BERT Prompt\nBERT Attack\nRoBERTa Prompt\nRoBERTa Attack\nXLNET Prompt\nXLNET Attack\nGPT-NEO Prompt\nGPT-NEO Attack\nSST-2\nOLID\nCOLA\nMR\nTREC\nFigure 6: Feature distributions for prompt and victim models across datasets (SST-2, OLID, COLA, MR, and\nTREC). The first two lines correspond to BERT, followed by RoBERTa in lines 3-4, XLNET in lines 5-6, and\nGPT-NEO-1.3B in lines 7-8.\n"}, "Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks": {"authors": ["Julieta Martinez", "Jashan Shewakramani", "Ting Wei Liu", "Ioan Andrei B\u00e2rsan", "Wenyuan Zeng", "Raquel Urtasun"], "title": "Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks", "url": "https://arxiv.org/pdf/2010.15703.pdf", "abstract": "Compressing large neural networks is an important step for their deployment in resource-constrained computational platforms. In this context, vector quantization is an appealing framework that expresses multiple parameters using a single code, and has recently achieved state-of-the-art network compression on a range of core vision and natural language processing tasks. Key to the success of vector quantization is deciding which parameter groups should be compressed together. Previous work has relied on heuristics that group the spatial dimension of individual convolutional filters, but a general solution remains unaddressed. This is desirable for pointwise convolutions (which dominate modern architectures), linear layers (which have no notion of spatial dimension), and convolutions (when more than one filter is compressed to the same codeword). In this paper we make the observation that the weights of two adjacent layers can be permuted while expressing the same function. We then establish a connection to rate-distortion theory and search for permutations that result in networks that are easier to compress. Finally, we rely on an annealed quantization algorithm to better compress the network and achieve higher final accuracy. We show results on image classification, object detection, and segmentation, reducing the gap with the uncompressed model by 40 to 70% with respect to the current state of the art.", "arxiv_id": "2010.15703", "published_date": "2020-10-29", "year": 2020, "introduction": "Introduction State-of-the-art approaches to many computer vision tasks are currently based on deep neural networks. These networks often have large memory and computational requirements, limiting the range of hardware platforms on which they can operate. This poses a challenge for applications such as virtual reality and robotics, which naturally rely on mobile and low-power computational platforms for large-scale deployment. At the same time, these networks are often overparameterized [5], which implies that it is possible to compress them \u2013 thereby reducing their memory and computation demands \u2013 without much loss in accuracy. Scalar quantization is a popular approach to network compression where each network parameter is compressed individually, thereby limiting the achievable compression rates. To address this limitation, a recent line of work has focused on vector quantization (VQ) [13, 47, 54], which compresses multiple parameters into a single code. Conspicuously, these approaches have recently achieved state-of-theart compression-to-accuracy ratios on core computer vision and natural language processing tasks [10,48]. A key advantage of VQ is that it can naturally exploit redundancies among groups of network parameters, for example, by grouping the spatial dimensions of convolutional \ufb01lters in a single vector to achieve high compression rates. However, \ufb01nding which network parameters should be compressed jointly can be challenging; for instance, there is no notion of spatial dimension in fully connected layers, and it is not clear how vectors should be formed when the vector size is larger than a single convolutional \ufb01lter \u2013 which is always true for pointwise convolutions. Current approaches either employ clustering (e.g., k-means) using the order of the weights as obtained by the network [13,47,54], which is suboptimal, or search for groups of parameters that, when compressed jointly, minimize the reconstruction error of the network activations [10,48,54], which is hard to optimize. In this paper, we formalize the notion of redundancy among parameter groups using concepts from rate-distortion theory, and leverage this analysis to search for permutations of the network weights that yield functionally equivalent, yet easier-to-quantize networks. The result is Permute, Quantize, and Fine-tune (PQF), an ef\ufb01cient algorithm that \ufb01rst searches for permutations, codes and codebooks that minimize the reconstruction error of the network weights, and then uses gradient-based optimization to recover the accuracy of the uncompressed network. Our main contributions can be summarized as follows: 1. We study the invariance of neural networks under permutation of their weights, focusing on constraints in1 arXiv:2010.15703v3  [cs.CV]  10 Apr 2021 ", "conclusion": "Conclusion We have demonstrated that the quantization error of the weights of a neural network is inversely correlated with its accuracy after codebook \ufb01ne tuning. We have further proposed a method that exploits the functional equivalence of the network under permutation of its weights to \ufb01nd con\ufb01gurations of the weights that are easier to quantize. We have also shown that using an annealed k-means algorithm further reduces quantization error and improves \ufb01nal network accuracy. On ResNet-50, our method closes the relative gap to the uncompressed model by 40-70% compared to the previous state-of-the-art in a variety of visual tasks. 8 ", "full_text": "Permute, Quantize, and Fine-tune: Ef\ufb01cient Compression of Neural Networks\nJulieta Martinez\u2217,1\nJashan Shewakramani\u2217,1,2\nTing Wei Liu\u2217,1,2\nIoan Andrei B\u02c6arsan1,3\nWenyuan Zeng1,3\nRaquel Urtasun1,3\n1Uber Advanced Technologies Group\n2University of Waterloo\n3University of Toronto\n{julieta,jashan,tingwei.liu,andreib,wenyuan,urtasun}@uber.com\nAbstract\nCompressing large neural networks is an important step\nfor their deployment in resource-constrained computational\nplatforms. In this context, vector quantization is an appeal-\ning framework that expresses multiple parameters using a\nsingle code, and has recently achieved state-of-the-art net-\nwork compression on a range of core vision and natural\nlanguage processing tasks. Key to the success of vector\nquantization is deciding which parameter groups should be\ncompressed together. Previous work has relied on heuristics\nthat group the spatial dimension of individual convolutional\n\ufb01lters, but a general solution remains unaddressed. This is\ndesirable for pointwise convolutions (which dominate mod-\nern architectures), linear layers (which have no notion of\nspatial dimension), and convolutions (when more than one\n\ufb01lter is compressed to the same codeword). In this paper\nwe make the observation that the weights of two adjacent\nlayers can be permuted while expressing the same function.\nWe then establish a connection to rate-distortion theory and\nsearch for permutations that result in networks that are eas-\nier to compress. Finally, we rely on an annealed quantization\nalgorithm to better compress the network and achieve higher\n\ufb01nal accuracy. We show results on image classi\ufb01cation, ob-\nject detection, and segmentation, reducing the gap with the\nuncompressed model by 40 to 70% w.r.t. the current state\nof the art. All our experiments can be reproduced using the\ncode at https://github.com/uber-research/\npermute-quantize-finetune.\n1. Introduction\nState-of-the-art approaches to many computer vision\ntasks are currently based on deep neural networks. These\nnetworks often have large memory and computational re-\nquirements, limiting the range of hardware platforms on\nwhich they can operate. This poses a challenge for appli-\ncations such as virtual reality and robotics, which naturally\nrely on mobile and low-power computational platforms for\nlarge-scale deployment. At the same time, these networks\nare often overparameterized [5], which implies that it is pos-\nsible to compress them \u2013 thereby reducing their memory and\ncomputation demands \u2013 without much loss in accuracy.\nScalar quantization is a popular approach to network\ncompression where each network parameter is compressed\nindividually, thereby limiting the achievable compression\nrates. To address this limitation, a recent line of work has\nfocused on vector quantization (VQ) [13, 47, 54], which\ncompresses multiple parameters into a single code. Conspic-\nuously, these approaches have recently achieved state-of-the-\nart compression-to-accuracy ratios on core computer vision\nand natural language processing tasks [10,48].\nA key advantage of VQ is that it can naturally exploit\nredundancies among groups of network parameters, for ex-\nample, by grouping the spatial dimensions of convolutional\n\ufb01lters in a single vector to achieve high compression rates.\nHowever, \ufb01nding which network parameters should be com-\npressed jointly can be challenging; for instance, there is no\nnotion of spatial dimension in fully connected layers, and it\nis not clear how vectors should be formed when the vector\nsize is larger than a single convolutional \ufb01lter \u2013 which is\nalways true for pointwise convolutions. Current approaches\neither employ clustering (e.g., k-means) using the order of\nthe weights as obtained by the network [13,47,54], which is\nsuboptimal, or search for groups of parameters that, when\ncompressed jointly, minimize the reconstruction error of the\nnetwork activations [10,48,54], which is hard to optimize.\nIn this paper, we formalize the notion of redundancy\namong parameter groups using concepts from rate-distortion\ntheory, and leverage this analysis to search for permutations\nof the network weights that yield functionally equivalent, yet\neasier-to-quantize networks. The result is Permute, Quan-\ntize, and Fine-tune (PQF), an ef\ufb01cient algorithm that \ufb01rst\nsearches for permutations, codes and codebooks that mini-\nmize the reconstruction error of the network weights, and\nthen uses gradient-based optimization to recover the accu-\nracy of the uncompressed network. Our main contributions\ncan be summarized as follows:\n1. We study the invariance of neural networks under per-\nmutation of their weights, focusing on constraints in-\n1\narXiv:2010.15703v3  [cs.CV]  10 Apr 2021\n duced by the network topology. We then formulate a\npermutation optimization problem to \ufb01nd functionally\nequivalent networks that are easier to quantize. Our\nresult focuses on improving a quantization lower bound\nof the weights; therefore\n2. We use an ef\ufb01cient annealed quantization algorithm\nthat reduces quantization error and leads to higher ac-\ncuracy of the compressed networks. Finally,\n3. We show that the reconstruction error of the network\nparameters is inversely correlated with the \ufb01nal network\naccuracy after gradient-based \ufb01ne-tuning.\nPut together, the above contributions de\ufb01ne a novel method\nthat produces state-of-the-art results in terms of model size\nvs. accuracy. We benchmark our method by compressing\npopular architectures for image classi\ufb01cation, and object\ndetection & segmentation, showcasing the wide applicability\nof our approach. Our results show a 40-60% relative error\nreduction on Imagenet object classi\ufb01cation over the current\nstate-of-the-art when compressing a ResNet-50 [21] down\nto about 3 MB (\u223c31\u00d7 compression). We also demonstrate a\nrelative 60% (resp. 70%) error reduction in object detection\n(resp. mask segmentation) on COCO over previous work, by\ncompressing a Mask-RCNN architecture down to about 6.6\nMB (\u223c26\u00d7 compression).\n2. Related Work\nThere is a vast literature on compressing neural networks.\nEfforts in this area can broadly be divided into pruning, low-\nrank approximations, and quantization.\nWeight pruning:\nIn its simplest form, weight pruning can\nbe achieved by removing small weights [16,18], or approxi-\nmating the importance of each parameter using second-order\nterms [7,19,30]. More sophisticated approaches use meta-\nlearning to obtain pruning policies that generalize to multiple\nmodels [22], or use regularization terms during training to\nreduce parameter count [36]. Most of these methods prune\nindividual weights, and result in sparse networks that are\ndif\ufb01cult to accelerate on commonly available hardware. To\naddress these issues, another line of work aims to remove\nunimportant channels, producing networks that are easier to\naccelerate in practice [23,31,38].\nLow-rank approximations:\nThese methods can achieve\nacceleration by design [6,25,29,42], as they typically factor-\nize the original weight matrix into several smaller matrices.\nAs a result, the original computationally-heavy forward pass\ncan be replaced by a multiplication of several smaller vectors\nand matrices.\nScalar quantization:\nThese techniques constrain the num-\nber of bits that each parameter may take, in the extreme case\nusing binary [4,39,44,53] or ternary [57] values. 8-bit quan-\ntization methods have proven robust and ef\ufb01cient, which\nhas motivated their native support by popular deep learn-\ning libraries such as PyTorch1 and Tensor\ufb02ow Lite2, with\nacceleration often targeting CPUs. We refer the reader to\nthe survey by [41] for a recent comprehensive overview of\nthe subject. In this context, reducing each parameter to a\nsingle bit yields a theoretical compression ratio of 32\u00d7 (al-\nthough, in practice, fully-connected and batch norm layers\nare not quantized [39]). To obtain higher compression ratios,\nresearchers have turned to vector quantization.\nVector quantization (VQ):\nVQ of neural networks was\npioneered by Gong et al. [13], who investigated scalar, vec-\ntor, and product quantization [26] (PQ) of fully-connected\n(FC) layers, which were the most memory-demanding layers\nof convolutional neural networks (CNNs) at the time. Wu et\nal. [54] used PQ to compress both FC and convolutional\nlayers of CNNs; they noticed that minimizing the quantiza-\ntion error of the network parameters produces much worse\nresults than minimizing the error of the activations, so they\nsequentially quantized the layers to minimize error accumu-\nlation. However, neither Gong et al. [13] nor Wu et al. [54],\nexplored end-to-end training, which is necessary to recover\nthe network accuracy as the compression ratio increases.\nSon et al. [47] clustered 3\u00d73 convolutions using vector\nquantization, and \ufb01ne-tuned the centroids via gradient de-\nscent using additional bits to encode \ufb01lter rotation, resulting\nin very compact codebooks. However, they did not explore\nthe compression of FC layers nor pointwise convolutions\n(which dominate modern architectures), and did not explore\nthe relationship of quantization error to accuracy.\nStock et al. [48] use PQ to compress convolutional and\nFC layers using a clustering technique designed to minimize\nthe reconstruction error of the layer outputs (which is compu-\ntationally expensive), followed by end-to-end training of the\ncluster centroids via distillation. However, their approach\ndoes not optimize the grouping of the network parameters\nfor quantization, which we \ufb01nd to be crucial to obtain good\ncompression. Chen et al. [2] improve upon the results of [48]\nby minimizing the reconstruction error of the parameters and\nthe task loss jointly; however, their method also uses more\n\ufb01ne-tuning epochs, so a direct comparison is hard.\nDifferent from previous approaches, our method exploits\nthe invariance of neural networks under permutation of their\nweights for the purpose of vector compression. Based on\nthis observation, we draw connections to rate distortion the-\nory, and use an ef\ufb01cient permutation optimization algorithm\nthat makes the network easier to quantize. We also use an\nannealed clustering algorithm to further reduce quantization\nerror, and show that there is a direct correlation between the\n1pytorch.org/docs/stable/quantization.html\n2tensorflow.org/lite/performance/post_training_\nquantization\n2\n quantization error of a network weights and its \ufb01nal accuracy\nafter \ufb01ne-tuning. These contributions result in an ef\ufb01cient\nmethod that largely outperforms its competitors on a wide\nrange of applications.\n3. Learning to Compress a Neural Network\nIn this paper we compress a neural network by compress-\ning the weights of its layers. Speci\ufb01cally, instead of storing\nthe weight matrix W of a layer explicitly, we learn an encod-\ning B(W) that takes considerably less memory. Intuitively,\nwe can decode B to a matrix \ufffd\nW that is \u201cclose\u201d to W, and\nuse \ufffd\nW as the weight matrix for the layer. The idea is that if\n\ufffd\nW is similar to W, the activations of the layer should also\nbe similar. Note that the encoding will be different for each\nof the layers.\n3.1. Designing the Encoding\nFor a desired compression rate, we design the encoding\nB to consist of a codebook C, a set of codes B, and a permu-\ntation matrix P. The permutation matrix preprocesses the\nweights so that they are easier to compress without affecting\nthe input-output mapping of the network, while the codes\nand codebook attempt to express the permuted weights as\naccurately as possible using limited memory.\nCodes and codebook:\nLet W \u2208 Rm\u00d7n denote the weight\nmatrix of a fully-connected (FC) layer, with m the input size\nof the layer, and n the size of its output. We split each\ncolumn of W into column subvectors wi,j \u2208 Rd\u00d71, which\nare then compressed individually:\nW =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nw1,1\nw1,2\n\u00b7 \u00b7 \u00b7\nw1,n\nw2,1\nw2,2\n\u00b7 \u00b7 \u00b7\nw2,n\n...\n...\n...\n...\nw \u02c6m,1\nw \u02c6m,2\n\u00b7 \u00b7 \u00b7\nw \u02c6m,n\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb ,\n(1)\nwhere \u02c6m = m/d, and \u02c6m\u00b7n is the total number of subvectors.\nIntuitively, larger d results in fewer subvectors and thus\nhigher compression rates. The set {wi,j} is thus a collection\nof d-dimensional blocks that can be used to construct W.\nInstead of storing all these subvectors, we approximate\nthem by a smaller set C = {c(1), . . . , c(k)} \u2286 Rd\u00d71, which\nwe call the codebook for the layer. We refer to the elements\nof C as centroids. Let bi,j \u2208 {1, . . . , k} be the index of the\nelement in C that is closest to wi,j in Euclidean space:\nbi,j = arg min\nt\n\u2225wi,j \u2212 c(t)\u22252\n2,\n(2)\nThe codes B = {bi,j} are the indices of the codes in the\ncodebook that best reconstruct every subvector {wi,j}. The\napproximation \ufffd\nW of W is thus the matrix obtained by re-\nplacing each subvector wi,j with c(bi,j):\n\ufffd\nW =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nc(b1,1)\nc(b1,2)\n\u00b7 \u00b7 \u00b7\nc(b1,n)\nc(b2,1)\nc(b2,2)\n\u00b7 \u00b7 \u00b7\nc(b2,n)\n...\n...\n...\n...\nc(b \u02c6m,1)\nc(b \u02c6m,2)\n\u00b7 \u00b7 \u00b7\nc(b \u02c6m,n)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb .\n(3)\nWe refer to the process of expressing the weight matrix in\nterms of codes and a codebook as quantization.\nPermutation:\nThe effectiveness of a given set of codes\nand codebooks depends on their ability to represent the orig-\ninal weight matrix W accurately. Intuitively, this is easier\nto achieve if the subvectors wi,j are similar to one another.\nTherefore, it is natural to consider transformations of W that\nmake the resulting subvectors easier to compress.\nA feedforward network can be thought of as a directed\nacyclic graph (DAG), where nodes represent layers and edges\nrepresent the \ufb02ow of information in the network. We refer\nto the starting node of an edge as a parent layer, and to\nthe end node as a child layer. We note that the network is\ninvariant under permutation of its weights, as long as the\nsame permutation is applied to the output dimension for\nparent layers and the input dimension for children layers.\nHere, our key insight is that we can search for permutations\nthat make the network easier to quantize.\nFormally, consider a network comprised of two layers:\nf(x) = \u03c6(xW2), W2 \u2208 Rm\u00d7n\n(4)\ng(x) = \u03c6(xW1), W1 \u2208 Rp\u00d7m\n(5)\nwhere \u03c6 represents a non-linear activation function. The\nnetwork can be described as the function\nf \u25e6 g(x) = f(g(x)) = \u03c6(g(x)W2),\n(6)\nwhere x \u2208 R1\u00d7p is the input to the network. Furthermore,\nfrom a topological point of view, g is the parent of f.\nGiven a permutation \u03c0 of m elements \u03c0 : {1, . . . , m} \u2192\n{1, . . . , m}, we denote P as the permutation matrix that\nresults from reordering the rows of the m\u00d7m identity matrix\naccording to \u03c0. Left-multiplying P with X has the effect of\nreordering the rows of X according to \u03c0.\nLet fP2 be the layer that results from applying the per-\nmutation matrix P2 to the input dimension of the weights of\nf:\nfP2(x) = \u03c6(xP2W2).\n(7)\nAnalogously, let gP2 be the layer that results from ap-\nplying the permutation P2 to the output dimension of the\nweights of g:\ngP2(x) = \u03c6(x(P2W\u22a4\n1 )\u22a4) = \u03c6(xW1P\u22a4\n2 ).\n(8)\nImportantly, so long as \u03c6 is an element-wise operator,\ngP2 produces the same output as g, only permuted:\ngP2(x) = g(x)P\u22a4\n2 ,\n(9)\n3\n Figure 1: Permutation optimization of a fully-connected layer. Our goal is to \ufb01nd a permutation P of the weights W such that the\nresulting subvectors are easier to compress.\nthen we have\nfP2 \u25e6 gP2(x) = fP2(gP2(x))\n(10)\n= fP2(g(x)P\u22a4\n2 )\n(11)\n= \u03c6(g(x)P\u22a4\n2 P2W2)\n(12)\n= \u03c6(g(x)W2)\n(13)\n= f(g(x))\n(14)\n= f \u25e6 g(x), \u2200 P2, x.\n(15)\nThis functional equivalence has previously been used to char-\nacterize the optimization landscape of neural networks [1,43].\nIn contrast, here we focus on quantizing the permuted weight\nP2W2, and denote its subvectors as {wP2\ni,j }. We depict the\nprocess of applying a permutation and obtaining new sub-\nvectors in Figure 1.\nExtension to convolutional layers:\nThe encoding of con-\nvolutional layers is closely related to that of fully-connected\nlayers. Let W \u2208 RCin\u00d7Cout\u00d7K\u00d7K denote the weights of\na convolutional layer with Cin input channels, Cout output\nchannels, and a kernel size of K \u00d7 K. The idea is to re-\nshape W into a 2d matrix Wr of size CinK2 \u00d7 Cout, and\nthen apply the same encoding method that we use with fully-\nconnected layers. The result is an approximation \ufffd\nWr to Wr.\nWe then apply the inverse of the reshaping operation on \ufffd\nWr\nto get our approximation to W.\nWhen K > 1, we set the codeword size d to a multiple\nof K2 and limit the permutation matrix P to have a block\nstructure such that the spatial dimensions of \ufb01lters are quan-\ntized together. For pointwise convolutions (i.e., K = 1), we\nset d to 4 or 8, depending on the desired compression rate.\nWe have so far considered networks where each layer\nhas a single parent and a single child (i.e., the topology is a\nchain). We now consider architectures where some layers\nmay have more than one child or more than one parent.\nExtension beyond chain architectures:\nAlexNet [28]\nand VGG [46] are examples of popular architectures with\na chain topology. As a consequence, each layer can have\na different permutation. However, architectures with more\ncomplicated topologies have more constraints on the permu-\ntations that they admit.\nFor example, consider Figure 2, which depicts six res-\nblocks as used in the popular ResNet-50 architecture. We\nstart by \ufb01nding a permutation for layer 4a, and realize that\nits parent is layer 3c. We also notice that layers 3c and 2c\nmust share the same permutation for the residual addition to\nhave matching channels. By induction, this is also true of\nlayers 1c and 1d, which are now all parents of our initial\nlayer 4a. These parents have children of their own (layers\n2a, 3a and 4d), so these must be counted as siblings of 4a,\nand must share the same permutation as 4a. However, note\nthat all b and c layers are only children, so they can have\ntheir own independent permutation.\nOperations such as reshaping and concatenation in parent\nlayers may also affect the permutations that a layer can toler-\nate while preserving functional equivalence. For example, in\nthe detection head of Mask-RCNN [20], the output (of shape\n256 \u00d7 7 \u00d7 7) of a convolutional layer is reshaped (to 12 544)\nbefore before entering a FC layer. Moreover, the same tensor\nis used in another convolutional layer (without reshaping)\nfor mask prediction. Therefore, the FC layer and the child\nconvolutional layer must share the same permutation. In\nthis case, the FC layer must keep blocks of 7 \u00d7 7 = 49 con-\ntiguous dimensions together to respect the channel ordering\nof its parent (and to match the permutation of its sibling).\nDetermining the maximum set of independent permutations\nthat an arbitrary network may admit (and \ufb01nding ef\ufb01cient\nalgorithms to do so) is a problem we leave for future work.\n3.2. Learning the Encoding\nOur overarching goal is to learn an encoding of each\nlayer such that the \ufb01nal output of the network is preserved.\nTowards this goal, we search for a set of codes, codebook,\nand permutation that minimizes the quantization error Et of\nevery layer t of the network:\nEt =\nmin\nPt,Bt,Ct\n1\n\u02c6mn\n\ufffd\ufffd\ufffd\ufffd\nWt \u2212 PtWt\n\ufffd\ufffd\ufffd\n2\nF .\n(16)\nOur optimization procedure consists of three steps:\n1. Permute: We search for a permutation of each layer\nthat results in subvectors that are easier to quantize. We\ndo this by minimizing the determinant of the covariance\nof the resulting subvectors.\n4\n a. 1x1 conv\nb. 3x3 conv\nc. 1x1 conv\n+\na. 1x1 conv\nb. 3x3 conv\nc. 1x1 conv\n+\na. 1x1 conv\nb. 3x3 conv\nc. 1x1 conv\n+\nd. 1x1 conv\na. 1x1 conv\nb. 3x3 conv\nc. 1x1 conv\n+\nd. 1x1 conv\nResblock 1\nResblock 2\nResblock 3\nResblock 4\na. 1x1 conv\nb. 3x3 conv\nc. 1x1 conv\n+\nResblock 0\na. 1x1 conv\nb. 3x3 conv\nc. 1x1 conv\n+\nResblock 5\nFigure 2: Parent-child dependencies in the Resblocks of a ResNet-50 architecture. Purple nodes are children and yellow nodes are\nparents, and must share the same permutation (in Cin for children and in Cout for parents) for the network to produce the same output.\n2. Quantize: We obtain codes and codebooks for each\nlayer by minimizing the difference between the approx-\nimated weight and the permuted weight.\n3. Fine-tune: Finally, we jointly \ufb01ne-tune all the code-\nbooks with gradient-based optimization by minimizing\nthe loss function of the original network over the train-\ning dataset.\nWe have found that minimizing the quantization error of\nthe network weights (Eq. (16)) results in small inaccuracies\nthat accumulate over multiple layers reducing performance;\ntherefore, it is important to jointly \ufb01ne-tune the network so\nthat it can recover its original accuracy with gradient descent.\nWe have also observed that the quality of the intial recon-\nstruction has a direct impact on the \ufb01nal network accuracy.\nWe now describe the three steps in detail.\n3.2.1\nPermute\nIn this step, our goal is to estimate a permutation Pt such that\nthe permuted weight matrix PtWt has subvectors {wPt\ni,j }\nthat are easily quantizable. Intuitively, we want to mini-\nmize the spread of the vectors, as more compact vectors\ncan be expressed more accurately given a \ufb01xed number of\ncentroids. We now formalize this intuition and propose a\nsimple algorithm to \ufb01nd good permutations.\nA quantization lower bound:\nWe assume that the weight\nsubvectors that form the input to the quantization step come\nfrom a Gaussian distribution, wPt\ni,j \u223c N(0, \u03a3t), with zero-\nmean and covariance \u03a3t \u2208 Rd\u00d7d, which is a positive semi-\nde\ufb01nite matrix. Thanks to rate distortion theory [12], we\nknow that the expected reconstruction error Et must follow\nEt \u2265 k\u2212 2\nd d\n\ufffd\ufffd\u03a3t\n\ufffd\ufffd\n1\nd ;\n(17)\nin other words, the error is lower-bounded by the determinant\nof the covariance of the subvectors of PtWt. We assume\nthat we have access to a good minimizer such that, roughly,\nthis bound is equal to the reconstruction error achieved by\nour quantization algorithm. Thus, for a \ufb01xed target compres-\nsion bit-rate, we can focus on \ufb01nding a permutation Pt that\nminimizes\n\ufffd\ufffd\u03a3t\n\ufffd\ufffd.\nSearching for permutations:\nWe make use of Expres-\nsion (17) and focus on obtaining a permutation Pt that min-\nimizes the determinant of the covariance of the set {wPt\ni,j }.\nWe follow an argument similar to that of Ge et al. [11], and\nnote that the determinant of any positive semi-de\ufb01nite ma-\ntrix \u03a3t \u2208 Rd\u00d7d, with elements \u03c3t\ni,j, satis\ufb01es Hadamard\u2019s\ninequality:\n\ufffd\ufffd\u03a3t\n\ufffd\ufffd \u2264\nd\n\ufffd\ni=1\n\u03c3t\ni,i;\n(18)\nthat is, the determinant of \u03a3t is upper-bounded by the prod-\nuct of its diagonal elements.\nMotivated by this inequality, we greedily obtain an initial\nPt that minimizes the product of the diagonal elements of\n\u03a3t by creating d buckets of row indices, each with capacity\nto hold \u02c6m = m/d elements. We then compute the variance\nof each row of Wt, and greedily assign each row index to\nthe non-full bucket that results in lowest bucket variance.\nFinally, we obtain Pt by interlacing rows from the buckets\nso that rows from the same bucket are placed d rows apart.\nK \u00d7K convolutions can be handled similarly, assuming that\nPt has a block structure, and making use of the more general\nFischer\u2019s inequality. Please refer to the appendix for more\ndetails.\nThere are O(m!) possible permutations of Wt, so greedy\nalgorithms are bound to have limitations on the quality of the\nsolution that they can \ufb01nd. Thus, we re\ufb01ne our solution via\nstochastic local search [24]. Speci\ufb01cally, we iteratively im-\nprove the candidate permutation by \ufb02ipping two dimensions\nchosen at random, and keeping the new permutation if it\nresults in a set of subvectors whose covariance has lower de-\nterminant\n\ufffd\ufffd\u03a3t\n\ufffd\ufffd. We repeat this procedure for a \ufb01xed number\nof iterations, and return the best permutation obtained.\n3.2.2\nQuantize\nIn this step, we estimate the codes Bt and codebook Ct that\napproximate the permuted weight PtWt. Given a \ufb01xed\npermutation, this is equivalent to the well-known k-means\nproblem. We use an annealed quantization algorithm called\nSR-C originally due to Zeger et al. [56], and recently adapted\nby Martinez et al. [40] to multi-codebook quantization. Em-\npirically, SR-C achieves lower quantization error than the\n5\n vanilla k-means algorithm, and is thus a better minimizer of\nExpr. (17).\nA stochastic relaxation of clustering:\nThe quantization\nlower bound from Expression (17) suggests that the k-means\nalgorithm can be annealed by scheduling a perturbation such\nthat the determinant of the covariance of the set {wPt\ni,j }\ndecreases over time. Due to Hadamard\u2019s inequality (i.e.,\nExpresion (18)), this can be achieved by adding decreas-\ning amounts of noise to wPt\ni,j sampled from a zero-mean\nGaussian with diagonal covariance.\nTherefore, after randomly initializing the codes, we itera-\ntively update the codebook and codes with a noisy codebook\nupdate (which operates on subvectors with additive diagonal-\nized Gaussian noise), and a standard k-means code update.\nWe decay the noise according to the schedule (1 \u2212 (\u03c4/I))\u03b3,\nwhere \u03c4 is the current iteration, I is the total number of up-\ndate iterations, and \u03b3 is a constant. We use \u03b3 = 0.5 in all\nour experiments. For a detailed description, please refer to\nAlgorithm 1.\nAlgorithm 1 SR-C: Stochastic relaxation of k-means.\n1: procedure SR-C({wPt\ni,j }, \u03a3t, k, T, \u03b3)\n2:\nBt \u2190 INITIALIZECODES(k)\n3:\nfor \u03c4 \u2190 1, . . . , T do\n# Add scheduled noise to subvectors\n4:\nfor wPt\ni,j \u2208 {wPt\ni,j } do\n5:\nxi,j \u223c N(0, diag(\u03a3t))\n6:\n\u02c6wPt\ni,j \u2190 wPt\ni,j + (xi,j \u00d7 (1 \u2212 (\u03c4/I))\u03b3)\n7:\nend for\n# Noisy codebook update\n8:\nCt \u2190 arg minC\n\ufffd\ni,j\u2225\u02c6wPt\ni,j \u2212 c(bi,j)\u22252\n2\n# Regular codes update\n9:\nBt \u2190 arg minB\n\ufffd\ni,j\u2225wPt\ni,j \u2212 c(bi,j)\u22252\n2\n10:\nend for\n11:\nreturn Bt, Ct\n12: end procedure\n3.2.3\nFine-tune\nEncoding each layer independently causes errors in the ac-\ntivations to accumulate, resulting in degradation of perfor-\nmance. It is thus important to \ufb01ne-tune the encoding in order\nto recover the original accuracy of the network. In particular,\nwe \ufb01x the codes and permutations for the remainder of the\nprocedure.\nLet L be the original loss function of the network (e.g.,\ncross-entropy for classi\ufb01cation). We note that L is differen-\ntiable with respect to each of the learned centroids \u2013 since\nthese are continuous \u2013 so we use the original training set to\n\ufb01ne-tune the centroids with gradient-based learning:\nc(i) \u2190 c(i) \u2212 u\n\ufffd \u2202L\n\u2202c(i), \u03b8\n\ufffd\n,\n(19)\nModel\nRegime\ndK\ndpw\ndfc\nResNet-18\nSmall blocks\nK2\n4\n4\nLarge blocks\n2K2\n4\n4\nResNet-50\nSmall blocks\nK2\n4\n4\nLarge blocks\n2K2\n8\n4\nTable 1: Subvector sizes and compression regimes.\nwhere u(\u00b7, \u00b7) is an update rule (such as SGD, RMSProp [50]\nor Adam [27]) with hyperparameters \u03b8 (such as learning rate,\nmomentum, and decay rates).\n4. Experiments\nWe test our method on ResNet [21] architectures for\nimage classi\ufb01cation and Mask R-CNN [20] for object de-\ntection and instance segmentation.\nWe compress stan-\ndard ResNet-18 and ResNet-50 models that have been pre-\ntrained on ImageNet, taking the weights directly from the\nPyTorch model zoo.\nWe train different networks with\nk \u2208 {256, 512, 1024, 2048}. We also clamp the size of the\ncodebook for each layer to min(k, n \u00d7 Cout/4).\nSmall vs. large block sizes:\nTo further assess the trade-off\nbetween compression and accuracy, we use two compres-\nsion regimes. In the large blocks regime, we use a larger\nsubvector size d for each layer, which allows the weight\nmatrix to be encoded with fewer codes, and thus leads to\nhigher compression rates. To describe the subvector sizes\nwe use for each layer, we let dK denote the subvector size\nfor a convolutional layer with \ufb01lters of size K \u00d7 K. In the\nspecial case when K = 1, corresponding to a pointwise\nconvolution, we denote the subvector size by dpw. Finally,\nfully-connected layers have a subvector size of dfc. We sum-\nmarize our subvector sizes for each model and compression\nregime in Table 1.\nBit allocation:\nWe compress all the fully-connected and\nconvolutional layers of a network. However, following [48],\nwe do not compress the \ufb01rst convolutional layer (since it\noccupies less than 0.05% of the network size), the bias of the\nfully-connected layers, or the batchnorm layers. While we\ntrain with 32-bit \ufb02oats, we store our \ufb01nal model using 16-bit\n\ufb02oats, which has a negligible impact on validation accuracy\n(less than 0.02%). Finally, we fuse batchnorm layers into\ntwo vectors, which can be done with algebraic manipulation\nand is a trick normally used to speed up inference. Please\nrefer to the appendix for a detailed breakdown of the bit\nallocation in our models.\nHyperparameters:\nWe use a batch size of 128 for ResNet-\n18 and a batch size of 64 for ResNet-50. For annealed k-\nmeans, we implement SR-C in the GPU, and run it for 1 000\niterations. We \ufb01ne-tune the codebooks for 9 epochs using\nAdam [27] with an initial learning rate of 10\u22123, which is\n6\n Ratio\nSize\nAcc.\nGap\nSemi-sup R50 [55]\n\u2013\n97.50 MB\n79.30\n\u2013\nBGD [48]\n19\u00d7\n5.20 MB\n76.12\n3.18\nSemi-sup R50 [55]\n\u2013\n97.50 MB\n\u221778.72\n\u2013\nOur PQF\n19\u00d7\n5.09 MB\n77.15\n1.57\nTable 2:\nImageNet classi\ufb01cation starting from a semi-\nsupervised ResNet-50. We set a new state of the art in terms\nof accuracy vs model size. \u2217Reproduced from downloaded model.\ngradually reduced to 10\u22126 using cosine annealing [37]. Fine-\ntuning is the most expensive part of this process, and takes\naround 8 hours both for ResNet-18 (with 1 GPU) and for\nResNet-50 (with 4 GPUs). In the latter case, we scale the\nlearning rate by a factor of 4, following Goyal et al. [14].\nFor permutation optimization, we perform 1 000 local search\niterations; this is done in the CPU in parallel for each inde-\npendent permutation. This process takes less than 5 minutes\nfor ResNet-18, and about 10 minutes for ResNet-50 on a\n12-core CPU.\nBaselines:\nWe compare the results of our method against a\nvariety of network compression methods: Binary Weight Net-\nwork (BWN) [44], Trained Ternary Quantization (TTQ) [57],\nABC-Net [35], LR-Net [45], Deep Compression (DC) [17],\nHardware-Aware Automated Quantization (HAQ) [52],\nCLIP-Q [51], Hessian AWare Quantization of Neural Net-\nworks with Mixed Precision (HAWQ) [9], and HAWQ-\nV2 [8].\nWe compare extensively against the recently-\nproposed Bit Goes Down (BGD) method of [48] because it\nis the current state of the art by a large margin. BGD uses\nas initialization the method due to Wu et al. [54], and thus\nsubsumes it. All results presented are taken either from the\noriginal papers, or from two additional surveys [3,15].\n4.1. Image Classi\ufb01cation\nA summary of our results can be found in Figure 3. From\nthe Figure, it is clear that our method outperforms all its\ncompetitors. On ResNet-18 for example, we can surpass\nthe performance of ABC-Net (M=5) with our small blocks\nmodels at roughly 3\u00d7 the compression rate. Our biggest im-\nprovement generally comes from higher compression rates,\nand is especially apparent for the larger ResNet-50. When\nusing large blocks and k = 256 centroids, we obtain a top-1\naccuracy of 72.18% using only \u223c3 MB of memory. This\nrepresents an absolute \u223c4% improvement over the state of\nthe art. On ResNet-50, our method consistently reduces the\nremaining error by 40-60% w.r.t. the state of the art.\nSemi-supervised ResNet-50:\nWe also benchmark our\nmethod using a stronger backbone as a starting point. We\nstart from the recently released ResNet-50 model due to\nYalniz et al. [55], which has been pre-trained on unlabelled\nimages from the YFCC100M dataset [49], and \ufb01ne-tuned on\nPerm.\nSR-C\nAdam\nAcc.\n\u2206\n62.29\n\u22121.02\n\u0013\n62.55\n\u22120.76\n\u0013\n\u0013\n62.92\n\u22120.39\n\u0013\n\u0013\n\u0013\n63.31\n0.00\nTable 3: Ablation study. ResNet18 on ImageNet w/large blocks.\nImageNet. While the accuracy of this model is reported to be\n79.30%, we obtain a slightly lower 78.72% after download-\ning the publicly-available model3; (contacting the authors we\nlearned that the previous, slightly more accurate model, is\nno longer available for download). We use the small blocks\ncompression regime with k = 256, mirroring the procedure\ndescribed previously.\nWe show our results in Table 2, where our model attains\na top-1 accuracy of 77.15%. This means that we are able\nto outperform previous work by over 1% absolute accuracy,\nwith a much smaller gap w.r.t. the uncompressed model.\nWe \ufb01nd this result particularly interesting, as we originally\nexpected distillation to be necessary to transfer the knowl-\nedge of the larger network pretrained on a large corpus of\nunlabeled images. However, our results show that at least\npart of this knowledge is retained through the initialization\nand structure that low-error clustering imposes on the com-\npressed network.\nAblation study:\nIn Table 3, we show results for ResNet-\n18 using large blocks, for which we obtain a \ufb01nal accuracy\nof 63.31%. We add permutation optimization (Sec. 3.2.1),\nannealed k-means, as opposed to plain k-means (called SR-\nC in Sec. 3.2.2), and the use of the Adam optimizer with\ncosine annealing instead of plain SGD, as in previous work.\nFrom the Table, we can see that all our components are\nimportant and complementary to achieve top accuracy. It\nis also interesting to note that a baseline that simply does\nk-means and SGD \ufb01ne-tuning is already \u223c1% better than\nthe current state-of-the-art. Since both annealed k-means\nand permutation optimization directly reduce quantization\nerror before \ufb01ne-tuning, these experiments demonstrate that\nminimizing the quantization error of the weights leads to\nhigher \ufb01nal network accuracy.\n4.2. Object Detection and Segmentation\nWe also benchmark our method on the task of object de-\ntection by compressing the popular ResNet-50 Mask-RCNN\nFPN architecture [20] using the MS COCO 2017 dataset [34].\nWe start from the pretrained model available on the PyTorch\nmodel zoo, and apply the same procedure described above\nfor all the convolutional and linear layers (plus one deconvo-\nlutional layer, which we treat as a convolutional layer for the\n3https://github.com/facebookresearch/\nsemi-supervised-ImageNet1K-models\n7\n 0\n10\n20\n30\n40\nCompression Ratio\n60\n62\n64\n66\n68\n70\nTop-1 Accuracy (%)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nABC-Net (M=1)\nABC-Net (M=3)\nABC-Net (M=5)\nTTQ\nBWN\nLR-Net, binary\nLR-Net, ternary\n2.13%\nResNet-18 on ImageNet\nOur PQF, small blocks\nOur PQF, large blocks\nBGD, small blocks\nBGD, large blocks\nOriginal Model\nReference Methods\n0\n5\n10\n15\n20\n25\n30\nCompression Ratio\n68\n69\n70\n71\n72\n73\n74\n75\n76\nTop-1 Accuracy (%)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nDC (2 bits)\nDC (3 bits)\nDC (4 bits)\nHAQ (2 bits)\nHAQ (3 bits)\nHAQ (4 bits)\nCLIP-Q\nHAWQ-V2\nHAWQ\n3.97%\nResNet-50 on ImageNet\nOur PQF, small blocks\nOur PQF, large blocks\nBGD, small blocks\nBGD, large blocks\nOriginal Model\nReference Methods\nFigure 3: Compression results on ResNet-18 and ResNet-50. We compare accuracy vs. model size, using models from the PyTorch zoo\nas a starting point. In general, our method achieves higher accuracy compared to previous work.\nSize\nRatio\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\nRetinaNet [33] (uncompressed)\n145.00 MB\n\u2013\n35.6\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nDirect\n18.13 MB\n8.0\u00d7\n31.5\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nFQN [32]\n18.13 MB\n8.0\u00d7\n32.5\n51.5\n34.7\n\u2013\n\u2013\n\u2013\nHAWQ-V2 [8]\n17.90 MB\n8.1\u00d7\n34.8\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nMask-RCNN R-50 FPN [20] (uncompressed)\n169.40 MB\n\u2013\n37.9\n59.2\n41.1\n34.6\n56.0\n36.8\nBGD [48]\n6.65 MB\n26.0\u00d7\n33.9\n55.5\n36.2\n30.8\n52.0\n32.2\nOur PQF\n6.65 MB\n26.0\u00d7\n36.3\n57.9\n39.4\n33.5\n54.7\n35.6\nTable 4: Object detection results on MS COCO 2017. We compress a Mask R-CNN network with a ResNet-50 backbone, and include\ndifferent object detection architectures used by other baselines. We report both bounding box (bb) and mask (mk) metrics for Mask R-CNN.\nWe also report the accuracy at different IoU when available. The memory taken by [48] corresponds to the (correct) latest version on arXiv.\nRatio\nAPbb\nAPmk\nMask-RCNN R-50 FPN [20]\n\u2013\n37.9\n34.6\nBGD [48]\n26.0\u00d7\n33.9\n30.8\nOur PQF (no perm., no SR-C)\n26.0\u00d7\n35.6\n33.0\nOur PQF (no perm.)\n35.8\n33.1\nOur PQF (full)\n36.3\n33.5\nTable 5: Ablation results results on MS COCO 2017. Permuta-\ntion optimization is particularly important for Mask-RCNN\npurpose of compression). We use the small blocks regime\nwith k = 256 centroids, for a model of 6.65 MB.\nWe compress and \ufb01ne-tune the network on a single Nvidia\nGTX 1080Ti GPU with a batch size of 2 for 4 epochs. As be-\nfore, we use Adam [27] and cosine annealing [37], but with\nan initial learning rate of 5\u00d710\u22125. Our results are presented\nin Table 4. We also compare against recent baselines such\nas the Fully Quantized Network (FQN) [32], and the second\nversion of Hessian Aware Quantization (HAWQ-V2) [8],\nwhich showcase results compressing RetinaNet [33].\nOur method obtains a box AP of 36.3, and a mask AP of\n33.5, which represent improvements of 2.4% and 2.7% over\nthe best previously reported result, closing the gap to the\nuncompressed model by 60-70%. Compared to BGD [48],\nwe also use fewer computational resources, as they used 8\nV100 GPUs and distributed training for compression, while\nwe use a single 1080Ti GPU. In Table 5, we show again that\nusing both SR-C and permutation optimization is crucial to\nobtain the best results. These results demonstrate the ability\nof our method to generalize to more complex tasks beyond\nimage classi\ufb01cation.\n5. Conclusion\nWe have demonstrated that the quantization error of the\nweights of a neural network is inversely correlated with its\naccuracy after codebook \ufb01ne tuning. We have further pro-\nposed a method that exploits the functional equivalence of\nthe network under permutation of its weights to \ufb01nd con\ufb01gu-\nrations of the weights that are easier to quantize. We have\nalso shown that using an annealed k-means algorithm fur-\nther reduces quantization error and improves \ufb01nal network\naccuracy. On ResNet-50, our method closes the relative\ngap to the uncompressed model by 40-70% compared to the\nprevious state-of-the-art in a variety of visual tasks.\n8\n Our optimization method consists of three stages that\nfocus on different variables of the encoding. Future work\nmay focus on techniques that jointly \ufb01ne-tune the codes\nand the codebooks, or optimization methods that learn the\nweight permutation jointly with the codes and codebook. The\ndeterminant of the covariance of the weights is a continuous\nmetric that could be minimized as the network is trained from\nscratch, resulting in networks that are easier to compress by\ndesign. Last but not least, demonstrating practical hardware\nacceleration on deep architectures that have been compressed\nwith product codes also remains an open area of research.\nReferences\n[1] An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. On\nthe geometry of feedforward neural network error surfaces.\nNeural computation, 5(6):910\u2013927, 1993. 4\n[2] Weihan Chen, Peisong Wang, and Jian Cheng.\nTowards\nconvolutional neural networks compression via global & pro-\ngressive product quantization. In BMVC, 2020. 2\n[3] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey on\nmodel compression and acceleration for deep neural networks.\nCoRR, 2017. 7\n[4] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-\nYaniv, and Yoshua Bengio. Binarized neural networks: Train-\ning deep neural networks with weights and activations con-\nstrained to + 1 or - 1. arXiv preprint arXiv:1602.02830, 2016.\n2\n[5] Misha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio\nRanzato, and Nando de Freitas. Predicting parameters in\ndeep learning. In Advances in neural information processing\nsystems, 2013. 1\n[6] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann Le-\nCun, and Rob Fergus. Exploiting linear structure within\nconvolutional networks for ef\ufb01cient evaluation. In Advances\nin Neural Information Processing Systems, 2014. 2\n[7] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune\ndeep neural networks via layer-wise optimal brain surgeon.\nIn Advances in Neural Information Processing Systems, 2017.\n2\n[8] Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfreen, Amir\nGholami, Michael W. Mahoney, and Kurt Keutzer. HAWQ-\nV2: Hessian aware trace-weighted quantization of neural\nnetworks. In Advances in Neural Information Processing\nSystems, 2020. 7, 8\n[9] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Ma-\nhoney, and Kurt Keutzer.\nHessian-aware quantization of\nneural networks with mixed precision. In ICCV, 2019. 7\n[10] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,\nR\u00b4emi Gribonval, Herv\u00b4e J\u00b4egou, and Armand Joulin. Training\nwith quantization noise for extreme model compression. In\nICLR, 2021. 1\n[11] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized\nproduct quantization for approximate nearest neighbor search.\nIn CVPR, 2013. 5, 11\n[12] Allen Gersho and Robert M Gray. Vector quantization and\nsignal compression, chapter 8, pages 228\u2013243.\nSpringer\nScience & Business Media, 1991. 5\n[13] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.\nCompressing deep convolutional networks using vector quan-\ntization. arXiv preprint arXiv:1412.6115, 2014. 1, 2\n[14] Priya Goyal, Piotr Doll\u00b4ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He.\nAccurate, large mini-\nbatch sgd: Training imagenet in 1 hour.\narXiv preprint\narXiv:1706.02677, 2017. 7\n[15] Yunhui Guo. A survey on methods and theories of quantized\nneural networks. arXiv preprint arXiv:1808.04752, 2018. 7\n[16] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic net-\nwork surgery for ef\ufb01cient DNNs. In Advances In Neural\nInformation Processing Systems, 2016. 2\n[17] Song Han, Huizi Mao, and William J Dally. Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and Huffman coding. In ICLR, 2016. 7\n[18] Song Han, Jeff Pool, John Tran, and William Dally. Learning\nboth weights and connections for ef\ufb01cient neural network. In\nAdvances in Neural Information Processing Systems, 2015. 2\n[19] Babak Hassibi and David G Stork. Second order derivatives\nfor network pruning: Optimal brain surgeon. In Advances in\nNeural Information Processing Systems, 1993. 2\n[20] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017. 4, 6, 7, 8\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n2, 6\n[22] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and\nSong Han. AMC: AutoML for model compression and accel-\neration on mobile devices. In ECCV, 2018. 2\n[23] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for\naccelerating very deep neural networks. In ICCV, 2017. 2\n[24] Holger H Hoos and Thomas St\u00a8utzle. Stochastic local search:\nFoundations and applications. Elsevier, 2004. 5\n[25] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.\nSpeeding up convolutional neural networks with low rank\nexpansions. In BMVC, 2014. 2\n[26] Herv\u00b4e J\u00b4egou, Matthijs Douze, and Cordelia Schmid. Product\nquantization for nearest neighbor search. IEEE Transactions\non Patten Analysis and Machine Intelligence, 33(1), 2010. 2,\n11\n[27] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6, 8\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classi\ufb01cation with deep convolutional neural networks.\nIn Advances in Neural Information Processing Systems, 2012.\n4\n[29] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan\nOseledets, and Victor Lempitsky. Speeding-up convolutional\nneural networks using \ufb01ne-tuned CP-decomposition. In ICLR,\n2015. 2\n[30] Yann LeCun, John S Denker, and Sara A Solla. Optimal\nbrain damage. In Advances in Neural Information Processing\nSystems, 1990. 2\n9\n [31] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and\nHans Peter Graf. Pruning \ufb01lters for ef\ufb01cient convnets. In\nICLR, 2016. 2\n[32] Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie\nYan, and Rui Fan. Fully quantized network for object detec-\ntion. In CVPR, 2019. 8\n[33] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nDoll\u00b4ar. Focal loss for dense object detection. In ICCV, pages\n2980\u20132988, 2017. 8\n[34] Tsung-Yi Lin, Michael Maire, Serge Belognie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnich. Microsoft COCO: Common objects in context. In\nECCV, 2014. 7\n[35] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate\nbinary convolutional neural network. In Advances in Neural\nInformation Processing Systems, 2017. 7\n[36] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,\nShoumeng Yan, and Changshui Zhang. Learning ef\ufb01cient\nconvolutional networks through network slimming. In ICCV,\n2017. 2\n[37] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient\ndescent with warm restarts. In ICLR, 2017. 7, 8\n[38] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A \ufb01lter\nlevel pruning method for deep neural network compression.\nIn ICCV, 2017. 2\n[39] Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tz-\nimiropoulos. Training binary neural networks with real-to-\nbinary convolutions. In ICLR, 2020. 2\n[40] Julieta Martinez, Shobhit Zakhmi, Holger H Hoos, and\nJames J Little.\nLSQ++: Lower running time and higher\nrecall in multi-codebook quantization. In ECCV, 2018. 5\n[41] James O\u2019 Neill. An overview of neural network compression.\narXiv preprint arXiv:2006.03669, 2020. 2\n[42] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and\nDmitry P Vetrov. Tensorizing neural networks. In Advances\nin Neural Information Processing Systems, 2015. 2\n[43] A Emin Orhan and Xaq Pitkow. Skip connections eliminate\nsingularities. In ICLR, 2018. 4\n[44] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,\nand Ali Farhadi. XNOR-net: Imagenet classi\ufb01cation using\nbinary convolutional neural networks. In ECCV, 2016. 2, 7\n[45] Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete\nweights using the local reparameterization trick. In ICLR,\n2018. 7\n[46] Karen Simonyan and Andrew Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR,\n2015. 4\n[47] Sanghyun Son, Seungjun Nah, and Kyoung Mu Lee. Cluster-\ning convolutional kernels to compress deep neural networks.\nIn ECCV, pages 216\u2013232, 2018. 1, 2, 11\n[48] Pierre Stock, Armand Joulin, R\u00b4emi Gribonval, Benjamin Gra-\nham, and Herv\u00b4e J\u00b4egou. And the bit goes down: Revisiting\nthe quantization of neural networks. In ICLR, 2020. 1, 2, 6,\n7, 8, 11, 12\n[49] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin\nElizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-\nJia Li. YFCC100M: The new data in multimedia research.\nCommunications of the ACM, 59(2):64\u201373, 2016. 7\n[50] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop:\nDivide the gradient by a running average of its recent magni-\ntude. COURSERA: Neural networks for machine learning,\n4(2):26\u201331, 2012. 6\n[51] Frederick Tung and Greg Mori. Deep neural network compres-\nsion by in-parallel pruning-quantization. IEEE Transactions\non Patten Analysis and Machine Intelligence, 2019. 7\n[52] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han.\nHAQ: Hardware-aware automated quantization with mixed\nprecision. In CVPR, 2019. 7\n[53] Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian.\nLearning channel-wise interactions for binary convolutional\nneural networks. In CVPR, 2019. 2\n[54] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and\nJian Cheng. Quantized convolutional neural networks for\nmobile devices. In CVPR, 2016. 1, 2, 7\n[55] I Zeki Yalniz, Herv\u00b4e J\u00b4egou, Kan Chen, Manohar Paluri, and\nDhruv Mahajan. Billion-scale semi-supervised learning for\nimage classi\ufb01cation. arXiv preprint arXiv:1905.00546, 2019.\n7\n[56] Kenneth Zeger, Jacques Vaisey, Allen Gersho, et al. Glob-\nally optimal vector quantizer design by stochastic relaxation.\nIEEE Transactions on Signal Processing, 40(2):310\u2013322,\n1992. 5\n[57] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally.\nTrained ternary quantization. In ICLR, 2017. 2, 7\n10\n Appendices\nA: Permutation initialization for K \u00d7 K convolutions\nIn Section 3.2.1 we describe our goal of \ufb01nding a permutation Pt of the rows of the weight matrix Wt, to create a new\nmatrix PtWt whose subvectors {wPt\ni,j } are easier to quantize. The algorithm as described works for a linear layer; that is,\nwhen Wt \u2208 Rm\u00d7n.\nWe also describe in Section 3.1 how we reshape the weights of convolutional layers, W \u2208 RCin\u00d7Cout\u00d7K\u00d7K into a matrix\nof shape Wr \u2208 RCinK2\u00d7Cout before permutation and quantization. In the case when K = 1 (i.e., pointwise convolutions), we\ncan treat the resulting reshaped weight Wr as a linear layer, and apply the same algorithm described in the main paper. We\nnow show the generalization of our permutation initialization algorithm to K \u00d7 K convolutional layers for K > 1.\nK \u00d7 K convolutions have a natural partition for quantization; namely, clustering the K2 contiguous values that make\nup a single K \u00d7 K convolutional \ufb01lter. This natural order has been exploited in previous work [47,48], and lends itself to\nacceleration with lookup tables, as commonly done on product-quantized databases [26]. Maintaining this partition is also\nnecessary if we want to optimize permutations, as applying a permutation on the Cout dimension of a parent layer has the\neffect of permuting the order of the channels of the output tensor, and applying the same permutation on the Cin dimension of\nchildren layers preserves the function expressed by the network \u2013 this naturally keeps K \u00d7 K convolutional \ufb01lters together.\nSince the transformed convolution weight matrix Wr \u2208 RCinK2\u00d7Cout contains the K2 values of each \ufb01lter stacked across\nits rows, we preserve this order by limiting the permutation to only move groups of K2 contiguous rows. In practice, this\nforces the permutation matrix Pt (of size CinK2 \u00d7 CinK2) to have a block structure:\nPt =\n\uf8eb\n\uf8ec\n\uf8ed\n\u02c6Pt\n1,1\n. . .\n\u02c6Pt\n1,Cin\n...\n...\n...\n\u02c6Pt\nCin,1\n. . .\n\u02c6Pt\nCin,Cin\n\uf8f6\n\uf8f7\n\uf8f8,\n(20)\nwhere every submatrix \u02c6Pt\ni,j \u2208 {0K\u00d7K, IK\u00d7K} is a K \u00d7 K matrix with either all zeros, or the identity matrix. Please refer to\nFigure 4 for an illustration of how this matrix permutes blocks of weights. As before, our goal is to minimize the determinant of\nthe covariance of the resulting subvectors {wPt\ni,j }. We follow Ge et al. [11] and observe that any square, positive semide\ufb01nite\nmatrix \u03a3 may be decomposed into M 2 squared blocks \u02c6\u03a3i,j:\n\u03a3 =\n\uf8eb\n\uf8ec\n\uf8ed\n\u02c6\u03a31,1\n. . .\n\u02c6\u03a31,M\n...\n...\n...\n\u02c6\u03a3M,1\n. . .\n\u02c6\u03a3M,M\n\uf8f6\n\uf8f7\n\uf8f8.\n(21)\nIn this case, Fischer\u2019s inequality states that\n\ufffd\ufffd\u03a3\n\ufffd\ufffd \u2264\nM\n\ufffd\ni=1\n\ufffd\ufffd\u02c6\u03a3i,i\n\ufffd\ufffd ,\n(22)\nin other words, the determinant of \u03a3 is bounded by the product of the determinants of its block-diagonal submatrices.\nNote that Hadamard\u2019s inequality (which we use in our main paper) can be seen as a corollary of this result. To recap, our\npreviously-described algorithm \ufb01nds an intial permutation that minimizes the product of the diagonal elements by:\n1. Creating d buckets\n2. Computing the variance of each row of PWr\n3. Assigning each row to the non-full bucket that yields the lowest variance for the dimensions in that bucket.\nThus, we can modify our algorithm to \ufb01nd an initial permutation that minimizes the product of the block-diagonal submatrices\nas follows:\n1. Instead of creating d buckets, we create d/K2 buckets\n2. Instead of computing the variance of each row of PWr, we compute the determinant of the covariance of every K2\ncontiguous rows in PWr\n11\n Input to k-means\nFigure 4: Permutation optimization of a 3 \u00d7 3 convolutional layer. Our goal is to \ufb01nd a permutation P such that the resulting input to\nk-means is easier to quantize. We construct P with a block structure that swaps K2 = 9 contiguous rows at a time, preserving the natural\nstructure of 3 \u00d7 3 convolutions.\n3. We assign each group of K2 contiguous row indices to the non-full bucket that yields the lowest determinant of the\ncovariance for the dimensions in that bucket.\nNote that for pointwise convolutions, i.e., when K = 1, the above steps naturally yield the algorithm that we described in the\nmain paper.\nAfter intialization, we also use our iterated local search (ILS) algorithm (described in Section 3.2.2) to \ufb01nd improved\npermutations. However, instead of swapping pairs of rows of PWr, we swap groups of K2 contiguous rows at a time.\nFinally, note that this implies that in our current setting we only optimize the permutation of K \u00d7 K convolutions when the\nblock size is at least 2K2. In practice, this occurs when dK = 2K2 = 2 \u00b7 32 = 18 in Table 1, which corresponds to the large\nblocks compression regime. On the other hand, permutations for 1 \u00d7 1 convolutions can be optimized under both large and\nsmall blocks regimes.\nB: Bit allocation\nTables 6 and 7 show the bit allocation of ResNet-18 and ResNet-50 under the small and large compression regimes\nrespectively. Note that we borrow the bit allocation from [48], so our compression ratios match theirs exactly. As we mention\nin the main paper, we store the codes using the smallest number of bits possible i.e., 8 bits for codebooks of size k = 256, 9\nbits for k = 512, 10 bits for k = 1024, and 11 bits for k = 2048. We also store the codebooks using float16, and layers\nthat are left uncompressed, such as all the batch normalization layers, are stored with float32. We observe that the codes\ntend to take more memory than the codebooks, and that the linear layer tends to be the most memory-heavy part of the network.\nC: Permutation groups\nThere are 12 permutation groups for ResNet-18, and 37 groups for ResNet-50. Listing 1 lists the permutations for the\ndefault PyTorch implementation of ResNet-18, and Listings 2 and 3 contain the permutations for ResNet-50. These sets of\nparents and children determine the maximum number of independent permutations that we can apply without changing the\nfunction expressed by the network. While we obtained these permutation groups by hand, we believe that computing them\nautomatically on arbitrary networks is an interesting area of future work.\n12\n Name\nlayer type\nshape\ndtype\nbits\nconv1.weight\nConv2d\n(64, 3, 7, 7)\ntorch.\ufb02oat32\n301056\nbn1.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nbn1.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer1.0.conv1.codes matrix\n(64, 64)\ntorch.uint8\n32768\nlayer1.0.bn1.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.bn1.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer1.0.conv2.codes matrix\n(64, 64)\ntorch.uint8\n32768\nlayer1.0.bn2.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.bn2.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer1.1.conv1.codes matrix\n(64, 64)\ntorch.uint8\n32768\nlayer1.1.bn1.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.bn1.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer1.1.conv2.codes matrix\n(64, 64)\ntorch.uint8\n32768\nlayer1.1.bn2.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.bn2.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer2.0.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer2.0.conv1.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.0.bn1.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.bn1.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer2.0.conv2.codes matrix\n(128, 128)\ntorch.uint8\n131072\nlayer2.0.bn2.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.bn2.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.downsample.0.codebook\nConv2d\n(256, 4)\ntorch.\ufb02oat16\n16384\nlayer2.0.downsample.0.codes matrix\n(128, 16)\ntorch.uint8\n16384\nlayer2.0.downsample.1.weight\nConv2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.downsample.1.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer2.1.conv1.codes matrix\n(128, 128)\ntorch.uint8\n131072\nlayer2.1.bn1.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.bn1.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer2.1.conv2.codes matrix\n(128, 128)\ntorch.uint8\n131072\nlayer2.1.bn2.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.bn2.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer3.0.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer3.0.conv1.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.0.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.0.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.0.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer3.0.conv2.codes matrix\n(256, 256)\ntorch.uint8\n524288\nlayer3.0.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.0.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.0.downsample.0.codebook\nConv2d\n(256, 4)\ntorch.\ufb02oat16\n16384\nlayer3.0.downsample.0.codes matrix\n(256, 32)\ntorch.uint8\n65536\nlayer3.0.downsample.1.weight\nConv2d\n(256,)\ntorch.\ufb02oat32\n8192\n13\n layer3.0.downsample.1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer3.1.conv1.codes matrix\n(256, 256)\ntorch.uint8\n524288\nlayer3.1.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer3.1.conv2.codes matrix\n(256, 256)\ntorch.uint8\n524288\nlayer3.1.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer4.0.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer4.0.conv1.codes matrix\n(512, 256)\ntorch.uint8\n1048576\nlayer4.0.bn1.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.bn1.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer4.0.conv2.codes matrix\n(512, 512)\ntorch.uint8\n2097152\nlayer4.0.bn2.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.bn2.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.downsample.0.codebook\nConv2d\n(256, 4)\ntorch.\ufb02oat16\n16384\nlayer4.0.downsample.0.codes matrix\n(512, 64)\ntorch.uint8\n262144\nlayer4.0.downsample.1.weight\nConv2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.downsample.1.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.conv1.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer4.1.conv1.codes matrix\n(512, 512)\ntorch.uint8\n2097152\nlayer4.1.bn1.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.bn1.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.conv2.codebook\nConv2d\n(256, 9)\ntorch.\ufb02oat16\n36864\nlayer4.1.conv2.codes matrix\n(512, 512)\ntorch.uint8\n2097152\nlayer4.1.bn2.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.bn2.bias\n(512,)\ntorch.\ufb02oat32\n16384\nfc.bias\nLinear\n(1000,)\ntorch.\ufb02oat32\n32000\nfc.codebook\n(2048, 4)\ntorch.\ufb02oat16\n131072\nfc.codes matrix\n(1000, 128)\ntorch.int16\n1408000\ntotal bits\n12927232\ntotal bytes\n1615904\ntotal KB\n1578.03\ntotal MB\n1.54 MB\nTable 6: Bit allocation for Resnet-18. Small blocks regime, k = 256.\n14\n Name\nlayer type\nshape\ndtype\nbits\nconv1.weight\nConv2d\n(64, 3, 7, 7)\ntorch.\ufb02oat32\n301056\nbn1.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nbn1.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.conv1.codebook\nConv2d\n(128, 8)\ntorch.\ufb02oat16\n16384\nlayer1.0.conv1.codes matrix\n(64, 8)\ntorch.uint8\n3584\nlayer1.0.bn1.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.bn1.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer1.0.conv2.codes matrix\n(64, 32)\ntorch.uint8\n16384\nlayer1.0.bn2.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.bn2.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.0.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer1.0.conv3.codes matrix\n(256, 8)\ntorch.uint8\n16384\nlayer1.0.bn3.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer1.0.bn3.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer1.0.downsample.0.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer1.0.downsample.0.codes matrix\n(256, 8)\ntorch.uint8\n16384\nlayer1.0.downsample.1.weight\nConv2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer1.0.downsample.1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer1.1.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer1.1.conv1.codes matrix\n(64, 32)\ntorch.uint8\n16384\nlayer1.1.bn1.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.bn1.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer1.1.conv2.codes matrix\n(64, 32)\ntorch.uint8\n16384\nlayer1.1.bn2.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.bn2.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.1.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer1.1.conv3.codes matrix\n(256, 8)\ntorch.uint8\n16384\nlayer1.1.bn3.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer1.1.bn3.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer1.2.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer1.2.conv1.codes matrix\n(64, 32)\ntorch.uint8\n16384\nlayer1.2.bn1.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.2.bn1.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.2.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer1.2.conv2.codes matrix\n(64, 32)\ntorch.uint8\n16384\nlayer1.2.bn2.weight\nBatchNorm2d\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.2.bn2.bias\n(64,)\ntorch.\ufb02oat32\n2048\nlayer1.2.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer1.2.conv3.codes matrix\n(256, 8)\ntorch.uint8\n16384\nlayer1.2.bn3.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer1.2.bn3.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer2.0.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.0.conv1.codes matrix\n(128, 32)\ntorch.uint8\n32768\nlayer2.0.bn1.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.bn1.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer2.0.conv2.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.0.bn2.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\n15\n layer2.0.bn2.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.0.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.0.conv3.codes matrix\n(512, 16)\ntorch.uint8\n65536\nlayer2.0.bn3.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.0.bn3.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.0.downsample.0.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.0.downsample.0.codes matrix\n(512, 32)\ntorch.uint8\n131072\nlayer2.0.downsample.1.weight\nConv2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.0.downsample.1.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.1.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.1.conv1.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.1.bn1.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.bn1.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer2.1.conv2.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.1.bn2.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.bn2.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.1.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.1.conv3.codes matrix\n(512, 16)\ntorch.uint8\n65536\nlayer2.1.bn3.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.1.bn3.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.2.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.2.conv1.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.2.bn1.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.2.bn1.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.2.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer2.2.conv2.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.2.bn2.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.2.bn2.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.2.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.2.conv3.codes matrix\n(512, 16)\ntorch.uint8\n65536\nlayer2.2.bn3.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.2.bn3.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.3.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.3.conv1.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.3.bn1.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.3.bn1.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.3.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer2.3.conv2.codes matrix\n(128, 64)\ntorch.uint8\n65536\nlayer2.3.bn2.weight\nBatchNorm2d\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.3.bn2.bias\n(128,)\ntorch.\ufb02oat32\n4096\nlayer2.3.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer2.3.conv3.codes matrix\n(512, 16)\ntorch.uint8\n65536\nlayer2.3.bn3.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer2.3.bn3.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer3.0.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.0.conv1.codes matrix\n(256, 64)\ntorch.uint8\n131072\nlayer3.0.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.0.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.0.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer3.0.conv2.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.0.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.0.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\n16\n layer3.0.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.0.conv3.codes matrix\n(1024, 32)\ntorch.uint8\n262144\nlayer3.0.bn3.weight\nBatchNorm2d\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.0.bn3.bias\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.0.downsample.0.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.0.downsample.0.codes matrix\n(1024, 64)\ntorch.uint8\n524288\nlayer3.0.downsample.1.weight\nConv2d\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.0.downsample.1.bias\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.1.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.1.conv1.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.1.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer3.1.conv2.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.1.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.1.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.1.conv3.codes matrix\n(1024, 32)\ntorch.uint8\n262144\nlayer3.1.bn3.weight\nBatchNorm2d\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.1.bn3.bias\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.2.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.2.conv1.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.2.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.2.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.2.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer3.2.conv2.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.2.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.2.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.2.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.2.conv3.codes matrix\n(1024, 32)\ntorch.uint8\n262144\nlayer3.2.bn3.weight\nBatchNorm2d\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.2.bn3.bias\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.3.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.3.conv1.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.3.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.3.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.3.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer3.3.conv2.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.3.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.3.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.3.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.3.conv3.codes matrix\n(1024, 32)\ntorch.uint8\n262144\nlayer3.3.bn3.weight\nBatchNorm2d\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.3.bn3.bias\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.4.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.4.conv1.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.4.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.4.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.4.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer3.4.conv2.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.4.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.4.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.4.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\n17\n layer3.4.conv3.codes matrix\n(1024, 32)\ntorch.uint8\n262144\nlayer3.4.bn3.weight\nBatchNorm2d\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.4.bn3.bias\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.5.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.5.conv1.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.5.bn1.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.5.bn1.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.5.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer3.5.conv2.codes matrix\n(256, 128)\ntorch.uint8\n262144\nlayer3.5.bn2.weight\nBatchNorm2d\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.5.bn2.bias\n(256,)\ntorch.\ufb02oat32\n8192\nlayer3.5.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer3.5.conv3.codes matrix\n(1024, 32)\ntorch.uint8\n262144\nlayer3.5.bn3.weight\nBatchNorm2d\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer3.5.bn3.bias\n(1024,)\ntorch.\ufb02oat32\n32768\nlayer4.0.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer4.0.conv1.codes matrix\n(512, 128)\ntorch.uint8\n524288\nlayer4.0.bn1.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.bn1.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer4.0.conv2.codes matrix\n(512, 256)\ntorch.uint8\n1048576\nlayer4.0.bn2.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.bn2.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.0.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer4.0.conv3.codes matrix\n(2048, 64)\ntorch.uint8\n1048576\nlayer4.0.bn3.weight\nBatchNorm2d\n(2048,)\ntorch.\ufb02oat32\n65536\nlayer4.0.bn3.bias\n(2048,)\ntorch.\ufb02oat32\n65536\nlayer4.0.downsample.0.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer4.0.downsample.0.codes matrix\n(2048, 128)\ntorch.uint8\n2097152\nlayer4.0.downsample.1.weight\nConv2d\n(2048,)\ntorch.\ufb02oat32\n65536\nlayer4.0.downsample.1.bias\n(2048,)\ntorch.\ufb02oat32\n65536\nlayer4.1.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer4.1.conv1.codes matrix\n(512, 256)\ntorch.uint8\n1048576\nlayer4.1.bn1.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.bn1.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer4.1.conv2.codes matrix\n(512, 256)\ntorch.uint8\n1048576\nlayer4.1.bn2.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.bn2.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.1.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer4.1.conv3.codes matrix\n(2048, 64)\ntorch.uint8\n1048576\nlayer4.1.bn3.weight\nBatchNorm2d\n(2048,)\ntorch.\ufb02oat32\n65536\nlayer4.1.bn3.bias\n(2048,)\ntorch.\ufb02oat32\n65536\nlayer4.2.conv1.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer4.2.conv1.codes matrix\n(512, 256)\ntorch.uint8\n1048576\nlayer4.2.bn1.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.2.bn1.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.2.conv2.codebook\nConv2d\n(256, 18)\ntorch.\ufb02oat16\n73728\nlayer4.2.conv2.codes matrix\n(512, 256)\ntorch.uint8\n1048576\nlayer4.2.bn2.weight\nBatchNorm2d\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.2.bn2.bias\n(512,)\ntorch.\ufb02oat32\n16384\nlayer4.2.conv3.codebook\nConv2d\n(256, 8)\ntorch.\ufb02oat16\n32768\nlayer4.2.conv3.codes matrix\n(2048, 64)\ntorch.uint8\n1048576\n18\n layer4.2.bn3.weight\nBatchNorm2d\n(2048,)\ntorch.\ufb02oat32\n65536\nlayer4.2.bn3.bias\n(2048,)\ntorch.\ufb02oat32\n65536\nfc.bias\nLinear\n(1000,)\ntorch.\ufb02oat32\n32000\nfc.codebook\n(1024, 4)\ntorch.\ufb02oat16\n65536\nfc.codes matrix\n(1000, 512)\ntorch.int16\n5120000\ntotal bits\n26718976\ntotal bytes\n3339872\ntotal KB\n3261.59\ntotal MB\n3.19 MB\nTable 7: Bit allocation for Resnet-50. Large blocks regime, k = 256.\n19\n import torchvision.models as models\nresnet18 = models.resnet18()\npermutations:\n\u2212\n\u2212 parents:\n[layer1.0.conv1, layer1.0.bn1]\n\u2212 children: [layer1.0.conv2]\n\u2212\n\u2212 parents:\n[layer1.1.conv1, layer1.1.bn1]\n\u2212 children: [layer1.1.conv2]\n\u2212\n\u2212 parents:\n[layer2.0.conv1, layer2.0.bn1]\n\u2212 children: [layer2.0.conv2]\n\u2212\n\u2212 parents:\n[layer2.1.conv1, layer2.1.bn1]\n\u2212 children: [layer2.1.conv2]\n\u2212\n\u2212 parents:\n[layer3.0.conv1, layer3.0.bn1]\n\u2212 children: [layer3.0.conv2]\n\u2212\n\u2212 parents:\n[layer3.1.conv1, layer3.1.bn1]\n\u2212 children: [layer3.1.conv2]\n\u2212\n\u2212 parents:\n[layer4.0.conv1, layer4.0.bn1]\n\u2212 children: [layer4.0.conv2]\n\u2212\n\u2212 parents:\n[layer4.1.conv1, layer4.1.bn1]\n\u2212 children: [layer4.1.conv2]\n\u2212\n\u2212 parents: [\nconv1, bn1,\nlayer1.0.conv2, layer1.0.bn2,\nlayer1.1.conv2, layer1.1.bn2,\n]\n\u2212 children: [\nlayer1.0.conv1,\nlayer2.0.downsample.0,\nlayer1.1.conv1,\nlayer2.0.conv1,\n]\n\u2212\n\u2212 parents: [\nlayer2.0.downsample.0, layer2.0.downsample.1,\nlayer2.0.conv2, layer2.0.bn2,\nlayer2.1.conv2, layer2.1.bn2,\n]\n\u2212 children: [\nlayer2.1.conv1,\nlayer3.0.downsample.0,\nlayer3.0.conv1,\n]\n\u2212\n\u2212 parents: [\nlayer3.0.downsample.0, layer3.0.downsample.1,\nlayer3.0.conv2, layer3.0.bn2,\nlayer3.1.conv2, layer3.1.bn2,\n]\n\u2212 children: [\nlayer3.1.conv1,\nlayer4.0.downsample.0,\nlayer4.0.conv1,\n]\n\u2212\n\u2212 parents: [\nlayer4.0.conv2, layer4.0.bn2,\nlayer4.0.downsample.0, layer4.0.downsample.1,\nlayer4.1.conv2, layer4.1.bn2,\n]\n\u2212 children: [\nlayer4.1.conv1,\nfc,\n]\nListing 1 Permutation groups for ResNet-18.\n20\n import torchvision.models as models\nresnet50 = models.resnet50()\npermutations:\n\u2212\n\u2212 parents: [conv1, bn1]\n\u2212 children: [\nlayer1.0.conv1,\nlayer1.0.downsample.0\n]\n\u2212\n\u2212 parents:\n[layer1.0.conv1, layer1.0.bn1]\n\u2212 children: [layer1.0.conv2]\n\u2212\n\u2212 parents:\n[layer1.0.conv2, layer1.0.bn2]\n\u2212 children: [layer1.0.conv3]\n\u2212\n\u2212 parents: [\nlayer1.0.conv3, layer1.0.bn3,\nlayer1.0.downsample.0, layer1.0.downsample.1,\nlayer1.1.conv3, layer1.1.bn3,\nlayer1.2.conv3, layer1.2.bn3,\n]\n\u2212 children: [\nlayer1.1.conv1,\nlayer1.2.conv1,\nlayer2.0.conv1,\nlayer2.0.downsample.0\n]\n\u2212\n\u2212 parents:\n[layer1.1.conv1, layer1.1.bn1]\n\u2212 children: [layer1.1.conv2]\n\u2212\n\u2212 parents:\n[layer1.1.conv2, layer1.1.bn2]\n\u2212 children: [layer1.1.conv3]\n\u2212\n\u2212 parents:\n[layer1.2.conv1, layer1.2.bn1]\n\u2212 children: [layer1.2.conv2]\n\u2212\n\u2212 parents:\n[layer1.2.conv2, layer1.2.bn2]\n\u2212 children: [layer1.2.conv3]\n\u2212\n\u2212 parents:\n[layer2.0.conv1, layer2.0.bn1]\n\u2212 children: [layer2.0.conv2]\n\u2212\n\u2212 parents:\n[layer2.0.conv2, layer2.0.bn2]\n\u2212 children: [layer2.0.conv3]\n\u2212\n\u2212 parents: [\nlayer2.0.conv3, layer2.0.bn3,\nlayer2.0.downsample.0, layer2.0.downsample.1,\nlayer2.1.conv3, layer2.1.bn3,\nlayer2.2.conv3, layer2.2.bn3,\nlayer2.3.conv3, layer2.3.bn3,\n]\n\u2212 children: [\nlayer2.1.conv1,\nlayer2.2.conv1,\nlayer2.3.conv1,\nlayer3.0.conv1,\nlayer3.0.downsample.0\n]\n\u2212\n\u2212 parents:\n[layer2.1.conv1, layer2.1.bn1]\n\u2212 children: [layer2.1.conv2]\n\u2212\n\u2212 parents:\n[layer2.1.conv2, layer2.1.bn2]\n\u2212 children: [layer2.1.conv3]\n\u2212\n\u2212 parents:\n[layer2.2.conv1, layer2.2.bn1]\n\u2212 children: [layer2.2.conv2]\n\u2212\n\u2212 parents:\n[layer2.2.conv2, layer2.2.bn2]\n\u2212 children: [layer2.2.conv3]\n\u2212\n\u2212 parents:\n[layer2.3.conv1, layer2.3.bn1]\n\u2212 children: [layer2.3.conv2]\n\u2212\n\u2212 parents:\n[layer2.3.conv2, layer2.3.bn2]\n\u2212 children: [layer2.3.conv3]\n\u2212\n\u2212 parents:\n[layer3.0.conv1, layer3.0.bn1]\n\u2212 children: [layer3.0.conv2]\n\u2212\n\u2212 parents:\n[layer3.0.conv2, layer3.0.bn2]\n\u2212 children: [layer3.0.conv3]\nListing 2 Permutation groups for ResNet-50 (part 1/2).\n21\n \u2212\n\u2212 parents: [\nlayer3.0.conv3, layer3.0.bn3,\nlayer3.0.downsample.0, layer3.0.downsample.1,\nlayer3.1.conv3, layer3.1.bn3,\nlayer3.2.conv3, layer3.2.bn3,\nlayer3.3.conv3, layer3.3.bn3,\nlayer3.4.conv3, layer3.4.bn3,\nlayer3.5.conv3, layer3.5.bn3,\n]\n\u2212 children: [\nlayer3.1.conv1,\nlayer3.2.conv1,\nlayer3.3.conv1,\nlayer3.4.conv1,\nlayer3.5.conv1,\nlayer4.0.conv1,\nlayer4.0.downsample.0\n]\n\u2212\n\u2212 parents:\n[layer3.1.conv1, layer3.1.bn1]\n\u2212 children: [layer3.1.conv2]\n\u2212\n\u2212 parents:\n[layer3.1.conv2, layer3.1.bn2]\n\u2212 children: [layer3.1.conv3]\n\u2212\n\u2212 parents:\n[layer3.2.conv1, layer3.2.bn1]\n\u2212 children: [layer3.2.conv2]\n\u2212\n\u2212 parents:\n[layer3.2.conv2, layer3.2.bn2]\n\u2212 children: [layer3.2.conv3]\n\u2212\n\u2212 parents:\n[layer3.3.conv1, layer3.3.bn1]\n\u2212 children: [layer3.3.conv2]\n\u2212\n\u2212 parents:\n[layer3.3.conv2, layer3.3.bn2]\n\u2212 children: [layer3.3.conv3]\n\u2212\n\u2212 parents:\n[layer3.4.conv1, layer3.4.bn1]\n\u2212 children: [layer3.4.conv2]\n\u2212\n\u2212 parents:\n[layer3.4.conv2, layer3.4.bn2]\n\u2212 children: [layer3.4.conv3]\n\u2212\n\u2212 parents:\n[layer3.5.conv1, layer3.5.bn1]\n\u2212 children: [layer3.5.conv2]\n\u2212\n\u2212 parents:\n[layer3.5.conv2, layer3.5.bn2]\n\u2212 children: [layer3.5.conv3]\n\u2212\n\u2212 parents:\n[layer4.0.conv1, layer4.0.bn1]\n\u2212 children: [layer4.0.conv2]\n\u2212\n\u2212 parents:\n[layer4.0.conv2, layer4.0.bn2]\n\u2212 children: [layer4.0.conv3]\n\u2212\n\u2212 parents:\n[layer4.1.conv1, layer4.1.bn1]\n\u2212 children: [layer4.1.conv2]\n\u2212\n\u2212 parents:\n[layer4.1.conv2, layer4.1.bn2]\n\u2212 children: [layer4.1.conv3]\n\u2212\n\u2212 parents:\n[layer4.2.conv1, layer4.2.bn1]\n\u2212 children: [layer4.2.conv2]\n\u2212\n\u2212 parents:\n[layer4.2.conv2, layer4.2.bn2]\n\u2212 children: [layer4.2.conv3]\n\u2212\n\u2212 parents: [\nlayer4.0.conv3, layer4.0.bn3,\nlayer4.0.downsample.0, layer4.0.downsample.1,\nlayer4.1.conv3, layer4.1.bn3,\nlayer4.2.conv3, layer4.2.bn3,\n]\n\u2212 children: [\nlayer4.1.conv1,\nlayer4.2.conv1,\nfc,\n]\nListing 3 Permutation groups for ResNet-50 (part 2/2).\n22\n"}, "Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health and Suicide Death among US Veterans": {"authors": ["Avijit Mitra", "Richeek Pradhan", "Rachel D Melamed", "Kun Chen", "David C Hoaglin", "Katherine L Tucker", "Joel I Reisman", "Zhichao Yang", "Weisong Liu", "Jack Tsai", "Hong Yu"], "title": "Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health and Suicide Death among US Veterans", "url": "https://arxiv.org/pdf/2212.05546.pdf", "abstract": "Importance: Social determinants of health (SDOH) are known to be associated with increased risk of suicidal behaviors, but few studies utilized SDOH from unstructured electronic health record (EHR) notes.   Objective: To investigate associations between suicide and recent SDOH, identified using structured and unstructured data.   Design: Nested case-control study.   Setting: EHR data from the US Veterans Health Administration (VHA).   Participants: 6,122,785 Veterans who received care in the US VHA between October 1, 2010, and September 30, 2015.   Exposures: Occurrence of SDOH over a maximum span of two years compared with no occurrence of SDOH.   Main Outcomes and Measures: Cases of suicide deaths were matched with 4 controls on birth year, cohort entry date, sex, and duration of follow-up. We developed an NLP system to extract SDOH from unstructured notes. Structured data, NLP on unstructured data, and combining them yielded six, eight and nine SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence intervals (CIs) were estimated using conditional logistic regression.   Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382 person-years of follow-up (incidence rate 37.18/100,000 person-years). Our cohort was mostly male (92.23%) and white (76.99%). Across the five common SDOH as covariates, NLP-extracted SDOH, on average, covered 80.03% of all SDOH occurrences. All SDOH, measured by structured data and NLP, were significantly associated with increased risk of suicide. The SDOH with the largest effects was legal problems (aOR=2.66, 95% CI=.46-2.89), followed by violence (aOR=2.12, 95% CI=1.98-2.27). NLP-extracted and structured SDOH were also associated with suicide.   Conclusions and Relevance: NLP-extracted SDOH were always significantly associated with increased risk of suicide among Veterans, suggesting the potential of NLP in public health studies.", "arxiv_id": "2212.05546", "published_date": "2022-12-11", "year": 2022, "introduction": "", "conclusion": "", "full_text": "1 \n \nTitle: Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health \nand Suicide Death among US Veterans \nAuthors: \nAvijit Mitra, MSc1  \nRicheek Pradhan, MD2 \nRachel D Melamed, PhD3 \nKun Chen, PhD4,5 \nDavid C Hoaglin, PhD6 \nKatherine L Tucker, PhD7 \nJoel I Reisman, AB8 \nZhichao Yang, MS1 \nWeisong Liu, PhD9,10 \nJack Tsai, PhD11,12 \nHong Yu, PhD 1,8,9,10 \n \n1 Manning College of Information and Computer Sciences, University of Massachusetts \nAmherst, MA, USA \n2 Department of Epidemiology, Biostatistics and Occupational Health, McGill University, Montreal, \nQuebec, Canada \n3 Department of Biological Sciences, University of Massachusetts Lowell, Lowell, MA, USA \n4 Department of Statistics, University of Connecticut, Storrs, CT, USA \n5 Center for Population Health, Uconn Health, Farmington, CT, USA \n6 Department of Population and Quantitative Health Sciences, University of Massachusetts Chan Medical \nSchool, Worcester, MA, USA. \n7 Department of Biomedical & Nutritional Sciences, University of Massachusetts Lowell, Lowell, MA, USA \n8 Center for Healthcare Organization & Implementation Research, Veterans Affairs Bedford Healthcare \nSystem, Bedford, MA, USA \n9 Miner School of Computer & Information Sciences, University of Massachusetts Lowell, Lowell, MA, \nUSA \n10 Center for Biomedical and Health Research in Data Sciences, University of Massachusetts Lowell, \nLowell, MA, USA \n11 National Center on Homelessness Among Veterans, United States Department of Veterans \nAffairs, Tampa, FL, USA \n12 School of Public Health, University of Texas Health Science Center at Houston, Houston, TX, USA \n \nCorresponding Author: \nHong Yu, PhD \nDepartment of Computer Science, \nUniversity of Massachusetts Lowell, \n1 University Avenue \nLowell, MA, US \nPhone: 1 508 612 7292 \nEmail: Hong_Yu@uml.edu \n \nDate of Revision: 12/06/2022 \nManuscript Word Count: 2,975 \n \n \n 2 \n \n \n \nAbstract \nImportance: Social determinants of health (SDOH) are known to be associated with increased risk of \nsuicidal behaviors, but few studies utilized SDOH from unstructured electronic health record (EHR) \nnotes. \nObjective: To investigate associations between suicide and recent SDOH, identified using structured and \nunstructured data. \nDesign: Nested case-control study.  \nSetting: EHR data from the US Veterans Health Administration (VHA). \nParticipants: 6,122,785 Veterans who received care in the US VHA between October 1, 2010, and \nSeptember 30, 2015.    \nExposures: Occurrence of SDOH over a maximum span of two years compared with no occurrence of \nSDOH. \nMain Outcomes and Measures: Cases of suicide deaths were matched with 4 controls on birth year, \ncohort entry date, sex, and duration of follow-up. We developed an NLP system to extract SDOH from \nunstructured notes. Structured data, NLP on unstructured data, and combining them yielded six, eight \nand nine SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence intervals (CIs) were \nestimated using conditional logistic regression. \nResults: In our cohort, 8,821 Veterans committed suicide during 23,725,382 person-years of follow-up \n(incidence rate 37.18 /100,000 person-years). Our cohort was mostly male (92.23%) and white (76.99%). \nAcross the five common SDOH as covariates, NLP-extracted SDOH, on average, covered 80.03% of all \nSDOH occurrences. All SDOH, measured by structured data and NLP, were significantly associated with \nincreased risk of suicide. The SDOH with the largest effects was legal problems (aOR=2.66, 95% CI=2.46-\n2.89), followed by violence (aOR=2.12, 95% CI=1.98-2.27). NLP-extracted and structured SDOH were also \nassociated with suicide. \nConclusions and Relevance: NLP-extracted SDOH were always significantly associated with increased \nrisk of suicide among Veterans, suggesting the potential of NLP in public health studies. \n \nIntroduction \nSuicide is one of the leading causes of death among US residents, accounting for 47,511 deaths in 2019 \nalone1. Nationwide, deaths by suicide increased 30% from 1999 to 20162. In 2013 alone, the total cost of \nsuicides and suicide attempts in the US was estimated to be $93.5 billion3. In the past decade, suicide \nrates have been consistently higher among Veterans than nonveterans, and even more alarming, the \nsuicide rate among Veterans has risen faster than among nonveteran adults4. \n \nSocial determinants of health (SDOH), which include conditions such as socioeconomic status, access to \nhealthy food, education, housing, and physical environment5, are strong predictors of suicidal behaviors \n(ideation, attempt and death)6\u20139. For example, social disruptions (e.g., relationship dissolution, financial \ninsecurity, legal problems, or exposure to childhood adversity) are well-known to instigate suicidal \nbehavior6,10\u201312. To formulate policies addressing suicide prevention, one must go beyond identifying \npredictors by determining the magnitude of the effects of SDOH on suicide. A key impediment to this \nhas been the lack of comprehensive and reliably available SDOH information in large population-based \ndatabases, where investigators have traditionally relied on structured data. Structured data often lack \ncompleteness regarding SDOH information, specifically, when they are designed for billing purposes. A \nrecent study showed that unstructured data contain about 90 times more SDOH information than \nstructured data13.  \n 3 \n \n \nThough existing studies identified a range of common risk-factors for suicide using structured data from \nelectronic health records (EHR)14\u201318, unstructured EHR data received little attention in investigating \npotential links between suicide and SDOH.  Therefore, in a nested case-control study, we used both \nstructured data (ICD codes, stop codes) and unstructured data (clinical notes, processed by a novel \nnatural language processing (NLP) system) from the large EHR system of the US Veterans Health \nAdministration (VHA) to examine the association of 9 SDOH factors with risk of suicide.  \nMethods \nData Source \nThis study used the EHR database from the VHA Corporate Data Warehouse (CDW). With a primary \nobligation to provide medical services to all eligible US Veterans, the VHA is the largest integrated \nhealthcare network in the country; its EHR system spans more than a thousand medical centers and \nclinics19. The VHA database includes patient demographic information, medication, diagnoses, \nprocedures, clinical notes, and billing. Our study protocol was approved by the institutional review \nboard of US Veterans Affairs (VA) Bedford Health Care, and we obtained a waiver of informed consent. \n \nStudy Population \nAs with any state-of-the-art NLP system, analyzing all patients in our base cohort presented a \ncomputational challenge. In addition, we studied multiple exposures. Therefore, we employed a nested \ncase-control design and used risk-set sampling to match the controls to the cases. This approach \nfacilitates studying associations of exposures (e.g., SDOH) on rare events such as suicide outcome20. \n \nThe base cohort consisted of all Veterans for whom the VHA database had any record of service during \nthe period between 10/01/2010 (start of Fiscal Year [FY] 2011) and 09/30/2015 (end of FY2015). Each \npatient\u2019s cohort entry date was defined as the latest of these dates: when the patient had two years of \nmedical history in the database, the patient\u2019s 18th birthday, or the start of FY2011. The end of follow-up \nwas defined as the earliest of the following: suicide, death from other causes, end of last record for the \npatient, or end of the study period (end of FY2015). We excluded all patients who had prior suicide \nattempts21 or no EHR notes before their cohort entry dates. Patients with missing or erroneous \ndemographic information and those older than 100 years of age were also excluded.  \n \nCases consisted of all patients in the base cohort who died by suicide (according to National Death \nIndex22 with International Statistical Classification of Diseases and Related Health Problems, Tenth \nRevision, codes X60\u2013X84, Y87.0, and/or U03 as underlying cause of death) during FY2011-2015. Each \ncase was randomly matched, with replacement, to 4 controls from those who were still alive. The \nmatching criteria were - 1) birth year (\u00b1 3 years), 2) cohort entry FY, 3) sex and 4) duration of follow-up \n(same or longer than the case). By design, a case could serve as a control for another case who \ncommitted suicide at an earlier date, and a patient could be a control for multiple cases. The index date \nfor each case was defined as the date of suicide and each control was assigned the same index date as \ntheir corresponding case. All data analyses were performed in May 2022. The Strengthening the \nReporting of Observational Studies in Epidemiology (STROBE)23 reporting guidelines were followed. \n \nNatural Language Processing \nA unique aspect of this study is the integration of an NLP system to extract SDOH, behavioral and other \nrelevant factors from EHR notes. We implemented a multi-task learning (MTL) framework based on the \npre-trained language model - RoBERTa24. RoBERTa is an improved version of Bidirectional Encoder \nRepresentations from Transformers (BERT)25 which has been shown to outperform all other NLP systems \n 4 \n \nacross a wide range of downstream tasks. To train our MTL model, we collected 4,646 EHR notes from \n1,393 patients (excludes all patients from our base cohort) who received treatment at the VHA and died \nbetween fiscal year 2009-2017. Under expert supervision, three trained annotators annotated these \nnotes for 13 distinct SDOH, behavioral and relevant factors. This is in accordance with the recent clinical \npractice guideline issued jointly by VA and Department of Defense26. Our MTL model was fine-tuned on \nthis dataset for three joint tasks: factor, presence, and period identification. Appendix 1 provides a \ndetailed description of our note selection, annotation process and MTL model performance.  \n \nWe used this fine-tuned MTL model to extract 13 factors for our study population \u2013 8 of which were \nSDOH (eTable1, Appendix 1). For each patient visit, we used 7 types of notes - emergency department \nnotes, nursing assessments, primary care notes, hospital admission notes, inpatient progress notes, pain \nmanagement, and discharge summaries. For multiple notes within an observation window (covariate or \nexposure assessment periods), we merged their factor predictions and prioritized presence \u2018yes\u2019 over \npresence \u2018no\u2019. Each prediction was dichotomized using the following strategy: \n1. Prediction of a factor with presence \u2018yes\u2019 and period \u2018current\u2019 was coded as 1. \n2. All other predictions were coded as 0, including factors with missing presence/period attributes. \n3. Notes with no factor prediction, and patients with no notes, were coded as 0. \n \nSDOH Extraction \nWe extracted SDOH from both unstructured EHR text (using NLP) and structured data (using ICD codes, \nand VHA stop codes, details in Appendix 2). The NLP-extracted SDOH comprised 8 factors: social \nisolation, job or financial insecurity, housing instability, legal problems, violence, barriers to care, \ntransition of care, and food insecurity; and the structured SDOH comprised 6 factors6 - social or familial \nproblems, employment or financial problems, housing instability, legal problems, violence, and non-\nspecific psychosocial needs. We combined these two groups to have 9 distinct factors - 5 were \nrepresented in both sets (more in Table 2), whereas barriers to care, transition of care and food \ninsecurity were found only in the NLP-extracted SDOH, and non-specific psychosocial needs, only in the \nstructured SDOH.  \n \nExposure and Covariate Assessment  \nTo focus on the impact of recent SDOH events on suicide, we considered a patient\u2019s exposures to the \naforementioned 9 SDOH in the two years before the index date, but not prior to the cohort entry date. \nThe covariate assessment period was two years prior to the cohort entry date. Potential covariates were \nsocio-demographic variables, clinical comorbidities, and mental health disorders. Socio-demographic \nvariables included race, age, and marital status. From the Charlson comorbidity index27, we included 17 \nclinical comorbidities: acute myocardial infarction, congestive heart failure, peripheral vascular disease, \ncerebrovascular disease, dementia, chronic obstructive pulmonary disease, rheumatoid disease, peptic \nulcer disease, mild liver disease, diabetes without complications, diabetes with complications, \nhemiplegia or paraplegia, renal disease, cancer, moderate or severe liver disease, metastatic solid \ntumor, and AIDS/HIV. We considered 7 mental health disorders: major depressive disorder, alcohol use \ndisorder, drug use disorder, anxiety disorder, posttraumatic stress disorder, schizophrenia, and bipolar \ndisorder6. We also added psychiatric symptoms, substance abuse, pain, and patient disability that were \nextracted by our NLP system. Additionally, for each model with a specific SDOH as the exposure, the list \nof covariates included all SDOH in its group (i.e., NLP-extracted, or structured or combined).  \n \n 5 \n \nStatistical Analysis \nWe calculated the crude incidence rate of suicide along with 95% confidence intervals (CIs) based on the \nPoisson distribution. For each SDOH exposure variable, we fit a conditional logistic regression model \nwith death by suicide as the outcome. In addition to matching for birth year, cohort entry date, sex, and \nduration of follow-up, we adjusted all models for the specified covariates (potential confounders). This \nprocedure yielded a total of 23 models: 8 for NLP-extracted SDOH, 6 for structured SDOH, and 9 when \ncombined. Following the same process, we also considered exposure to two SDOH at the same time, \nyielding a total of 79 models: 28 for NLP-extracted SDOH, 15 for structured SDOH, and 36 when \ncombined. The variance inflation factor (VIF)28 showed no evidence of collinearity (no VIF exceeded 3) \namong the covariates for any model. We report adjusted odds ratios (aOR) with 95% CIs for each \nexposure. All analyses used RStudio (version 0.99.902 with R version 3.6.0).  \n \n \n \n(a) \n 6 \n \n \n(b) \nFigure 1.  (a) Construction of the cohort and (b) our study timeline \n \nResults \nOur base cohort consisted of 6,122,785 Veterans (Figure 1) from 1,185 VA healthcare facilities; the \nmajority were white (76.99%) and male (92.23%). Veterans 50 years of age or older comprised 75.78% \nof the population. We had a mean follow-up of 3.87 years, generating 23,725,382 person-years, with \n8,821 deaths by suicide (crude incidence rate 37.18 per 100,000 person-years, 95% CI = 36.41-37.96). \nAppendix 3 reports detailed characteristics of the base cohort. In our case-control cohort, the majority \nwere also white (79.20%) and male (96.45%) with a mean (\u00b1SD) age of 58.64\u00b117.41 years. Compared \nwith cases, controls had a higher percentage of black individuals (16.27% vs. 5.50%) and lower \npercentage of white individuals (76.88% vs. 88.48%). This case-control cohort consisted of 8,821 cases \nand 35,284 matched controls (34,404 unique individuals, of whom 846 served as controls for more than \none case) from 43,188 Veterans who were served at 908 VA facilities. Detailed characteristics are shown \nin Table 1.  \n \n \n \nCases (%) \nControls (%) \nNo of Patients \n8,821 \n35,284a \nRace \nAsian \n42 (0.48) \n322 (0.91) \nAmerican Indian \n68 (0.77) \n240 (0.68) \nBlack \n485 (5.50) \n5,742 (16.27) \nNative Hawaiian or Other Pacific \nIslander \n70 (0.79) \n345 (0.98) \nWhite \n7,805 (88.48) \n27,125 (76.88) \n 7 \n \nUnknown \n351 (3.98) \n1,510 (4.28) \nSex \nMale \n8,508 (96.45) \n34,032 (96.45) \nFemale \n313 (3.55) \n1,252 (3.55) \nAge \n18-29 \n774 (8.77) \n3,085 (8.74) \n30-39 \n714 (8.09) \n2,842 (8.05) \n40-49 \n1,000 (11.34) \n3,911 (11.08) \n50-59 \n1,587 (17.99) \n6,163 (17.47) \n60-69 \n2,278 (25.82) \n9,597 (27.20) \n70-79 \n1,347 (15.27) \n5,271 (14.94) \n80-100 \n1,121 (12.71) \n4,415 (12.51) \nMarital Status \nMarried \n2,539 (28.78) \n8,846 (25.07) \nSingle \n1,028 (11.65) \n2,002 (5.67) \nDivorced \n1,871 (21.21) \n3,117 (8.83) \nWidowed \n466 (5.28) \n991 (2.81) \nUnknown \n2,917 (33.06) \n20,328 (57.61) \nComorbidities (Charlson) \nAcute Myocardial Infarction \n58 (0.66) \n207 (0.59) \nCongestive Heart Failure \n213 (2.41) \n812 (2.30) \nPeripheral Vascular Disease \n281 (3.19) \n948 (2.69) \nCerebrovascular disease \n275 (3.12) \n1,081 (3.06) \nDementia \n17 (0.19) \n153 (0.43) \nCOPD \n962 (10.91) \n2,859 (8.10) \n 8 \n \nRheumatoid Disease \n57 (0.65) \n249 (0.71) \nPeptic Ulcer Disease \n47 (0.53) \n127 (0.36) \nMild Liver Disease \n188 (2.13) \n496 (1.41) \nDiabetes without Complications \n1,108 (12.56) \n5,408 (15.33) \nDiabetes with Complications \n257 (2.91) \n1246 (3.53) \nHemiplegia or Paraplegia \n59 (0.67) \n186 (0.53) \nRenal Disease \n131 (1.49) \n682 (1.93) \nCancer (any malignancy) \n489 (5.54) \n1,673 (4.74) \nModerate or Severe Liver Disease \n15 (0.17) \n47 (0.13) \nMetastatic Solid Tumor \n28 (0.32) \n74 (0.21) \nAIDS/HIV \n26 (0.29) \n141 (0.40) \nComorbidities (Mental Health Disorders) \n Major Depressive disorder \n2,191 (24.84) \n4,790 (13.58) \nAlcohol Use Disorder \n778 (8.82) \n1,470 (4.17) \n Drug Use Disorder  \n425 (4.82) \n924 (2.62) \nAnxiety Disorder  \n880 (9.98) \n1,953 (5.54) \nPosttraumatic Stress Disorder \n1,208 (13.69) \n3,676 (10.42) \nSchizophrenia  \n262 (2.97) \n525 (1.49) \nBipolar disorder \n708 (8.03) \n1,101 (3.12) \nNLP-extracted non-SDOH factors \nPatient Disability \n3,718 (42.15) \n13,643 (38.67) \nSubstance Abuse \n5,408 (61.31) \n20,539 (58.21) \nPsychiatric Symptoms \n5,436 (61.63) \n19,954 (56.55) \nPain \n5,664 (64.21) \n22,048 (62.49) \nAbbreviations: COPD, Chronic Obstructive Pulmonary Disease \naNumber of controls, not number of unique patients. \n \n 9 \n \nTable 1. Summary Statistics for Cases and Controls \n \nThe structured SDOH had low prevalence compared with their NLP-extracted counterparts (Table 2). For \nexample, of all Veterans exposed to 'Social problems', only 32.31% were identified by structured SDOH \nwhereas NLP identified 85.80%. We found similar results for the remaining combined SDOH (Appendix \n4). As covariates, across all the 5 common SDOH, considering combined SDOH as the gold standard, NLP-\nextracted SDOH had a coverage of 78.86% on average compared with 36.02% from structured SDOH. As \nexposures, the numbers were 80.03% and 38.17% respectively. All SDOH occurred more frequently \namong cases than among controls. Moreover, for the majority of the SDOH, we found more occurrences \nduring the exposure assessment period than during the covariate assessment period. \n \n \nAs Exposure  \nCombined \nCase(%)/ \nControl(%) \n3,517 (39.87%)/ \n9,174 (26.00%) \n2,513 (28.49%)/ \n6,381 (18.08%) \n2,314 (26.23%)/ \n5,852 (16.59%) \n1,567 (17.76%)/ \n2,831 (8.02%) \n2,108 (23.90%)/ \n4,839 (13.71%) \n2,041 (23.14%)/ \n5,083 (14.41%) \n5,181 (58,73%)/ \n18,175 (51.51%) \n \n411 (4.66%)/   \n942 (2.67%) \n1,670 (18.93%)/ \n3749 (10.63%) \nStructured Data \nCase (%)/ \nControl(%) \n1,291 (14.64%)/ \n2,810 (7.96%) \n882 (10.00%)/ \n1,858 (5.27%) \n953 (10.80%)/ \n2,030 (5.75%) \n886 (10.04%)/ \n1,520 (4.31%) \n800 (9.07%)/ \n1,734 (4.91%) \n- \n- \n- \n1,670 (18.93%)/ \n3749 (10.63%) \nNLP-extracted \nCase(%)/  \nControl(%) \n3,080 (34.92%)/ \n7,809 (22.13%) \n2,274 (25.78%)/ \n5,710 (16.18%) \n2,005 (22.73%)/ \n5,027 (14.25%) \n1,025 (11.62%)/ \n1,760 (4.99%) \n1,700 (19.27%)/ \n3,519 (9.97%) \n \n2,041 (23.14%)/ \n5,083 (14.41%) \n5,181 (58,73%)/ \n18,175 (51.51%) \n411 (4.66%)/         \n942 (2.67%) \n \nAs Covariate \nCombined \nCase(%)/ \nControl(%) \n2,938 (33.31%)/ \n9,227 (26.15%) \n2,206 (25.01%)/ \n6,652 (18.85%) \n1,638 (18.57%)/ \n5,009 (14.20%) \n1,072 (12.15%)/ \n2,695 (7.64%) \n1,560 (17.69%)/ \n4,784 (13.56%) \n1,465 (16.61%)/ \n4,311 (12.22%) \n4,838 (54.85%)/ \n17,965 (50.92%) \n291 (3.30%)/   \n910 (2.58%) \n1,198 (13.58%)/ \n3,808 (10.79%) \nStructured Data \nCase (%)/ \nControl(%) \n833 (9.44%)/ \n2,691 (7.63%) \n603 (6.84%)/ \n1,661 (4.71%) \n592 (6.171%)/ \n1,657 (4.70%) \n530 (6.01%)/ \n1,390 (3.94%) \n604 (6.85%)/ \n1,981 (5.61%) \n- \n- \n- \n1,198 (13.58%)/ \n3,808 (10.79%) \n 10 \n \nNLP-extracted \nCase(%)/  \nControl(%) \n2,584 (29.29%)/ \n7,891 (22.36%) \n2,016 (22.85%)/ \n6,052 (17.15%) \n1,399 (15.86%)/ \n4,266 (12.09%) \n730 (8.28%)/ \n1,644 (4.66%) \n1,141 (12.94%)/ \n3,228 (9.15%) \n \n1,465 (16.61%)/ \n4,311 (12.22%) \n4,838 (54.85%)/ \n17,965 (50.92%) \n291 (3.30%)/ \n 910 (2.58%) \n- \n  \nSDOH \nSocial Problemsa \nFinancial Problemsb \nHousing Instability \nLegal Problems \nViolence \nBarriers to Care \nTransitions of Care \nFood Insecurity \nNon-specific \nPsychosocial Needs \n \nasocial problems = social or familial problems (Structured) + social isolation (NLP)  \nbfinancial problems = employment or financial problems (Structured) + job or financial insecurity (NLP)  \nTable 2. Summary Statistics of SDOH Factors as Covariate and as Exposure. \n \nAll 8 NLP-extracted SDOH were significantly associated with increased risk of death by suicide (Table 3). \n\u2018Legal problems\u2019 had the largest estimated effect size (more than twice the risk of those with no \nexposure; aOR = 2.62, 95% CI = 2.38-2.89), followed by \u2018violence\u2019 (aOR = 2.34, 95% CI = 2.17-2.52) and \n\u2018social isolation\u2019 (aOR = 1.94, 95% CI = 1.83-2.06). All 7 structured SDOH also showed significant \nassociations; again, \u2018legal problems\u2019 had the highest aOR (2.63, 95% CI = 2.37-2,91). Similarly, all \ncombined SDOH showed strong associations and the top three risk factors were \u2018legal problems\u2019 (aOR = \n2.66, 95% CI = 2.46-2.89), \u2018violence\u2019 (aOR = 2.12, 95% CI = 1.98-2.27) and \u2018non-specific psychosocial \nneeds\u2019 (aOR = 2.07, 95% CI = 1.92-2.23).  \n \nSDOH factors \n \nNLP-extracted, \naOR (95% CI)a \nStructured, \naOR (95% CI)a  \nCombined, \naOR (95% CI)a  \nSocial problemsb \n1.94 \n(1.83, 2.06) \n2.11 \n(1.94, 2.29) \n1.95 \n(1.84, 2.07) \nFinancial \nproblemsc \n1.91  \n(1.79, 2.04) \n2.18 \n(1.97, 2.42) \n1.92 \n(1.80, 2.05) \nHousing \ninstability \n1.90  \n(1.78, 2.03) \n2.28 \n(2.06, 2.53) \n1.93 \n(1.80, 2.06) \nLegal problems \n2.62 \n(2.38, 2.89) \n2.63 \n(2.37, 2.91) \n2.66  \n(2.46, 2.89) \nViolence \n2.34  \n(2.17, 2.52) \n1.96 \n(1.77, 2.16) \n2.12 \n(1.98, 2.27) \n 11 \n \nBarriers to care \n1.86  \n(1.74, 1.99) \n- \n1.86 \n(1.74, 1.98) \nTransition of \ncare \n1.53 \n(1.44, 1.62) \n- \n1.51 \n(1.43, 1.60) \nFood insecurity \n1.85  \n(1.62, 2.11) \n- \n1.85 \n(1.62, 2.11) \nNon-specific \npsychosocial \nneeds \n- \n2.09  \n(1.94, 2.25) \n2.07 \n(1.92, 2.23) \naEach model was adjusted for socio-demographic variables, psychiatric symptoms, substance abuse, \npain, patient disability, clinical comorbidities and all SDOH in its group. \nbsocial problems = social or familial problems (Structured) + social isolation (NLP)  \ncfinancial problems = employment or financial problems (Structured) + job or financial insecurity (NLP)  \nTable 3. Associations of SDOH with Veterans\u2019 death by suicide \n \nWhen considered simultaneous exposure to two SDOH, we found all combinations of SDOH to be \nstrongly associated with increased risk of death by suicide (Appendix 5), regardless of the SDOH \nextraction process. For NLP-extracted SDOH, the highest aOR was for exposure to \u2018legal problems\u2019 and \n\u2018violence\u2019 (aOR = 3.44, 95% CI = 3.03-3.89). For structured SDOH, exposure to \u2018financial problems\u2019 and \n\u2018violence\u2019 had the highest aOR (3.54 95% CI = 2.87-4.36). Combined SDOH also showed a similar trend. \n \nDiscussion \nTo our knowledge, this is the first large-scale study that used both structured and unstructured EHR data \nto investigate the relation between Veterans\u2019 suicide and SDOH. We developed and deployed an NLP \nsystem to extract SDOH from unstructured clinical notes and found that all NLP-extracted SDOH were \nstrongly associated with increased odds of suicide. We observed similar results for structured and \ncombined SDOH. \n \nThough many studies have explored the effect of various SDOH over different clinical outcomes14,29\u201331, \nvery few have examined the association of SDOH with increased risk of suicide, or the magnitude of such \nassociation, if any. In a nested case-control study of Veterans, Kim et al.8 used chart review to examine \nthe impact of SDOH. However, their study focused on a high-risk population of those with depression \nand had a small sample size (n=636). In contrast, in a large cross-sectional study of Veterans, Blosnich et \nal. 6 found a dose-response-like association with SDOH for both suicidal ideation and attempt. However, \ncross-sectional studies are unsuitable for investigating rare events such as suicide 32.  Most importantly, \nneither of these studies used the rich information provided by clinical notes. On the other hand, in a \ncase-control study, Dobscha et al.33 extracted SDOH from clinical notes through manual record review \nand found no evidence of association between Veterans\u2019 suicide and SDOH. They had a relatively small \nsample size (n=783) and included only male patients. \n \nAn important contribution of our study is the development of an NLP system to extract SDOH from \nunstructured EHR text. Our NLP system extracted a considerable number of SDOH that were not \navailable from the structured data fields (Appendix 4). These can help providers identify crucial SDOH \ninformation that they would otherwise miss. However, NLP-extracted SDOH did not cover all structured \n 12 \n \nSDOH.  Across the 5 common SDOH, NLP extracted 44.91% of the structured SDOH information as \ncovariates whereas as exposures it extracted 49.92%. This may be due to missing SDOH information in \nEHR notes or false negatives from the NLP system. Structured data, on the other hand, identified 18.86% \nof the NLP-extracted SDOH as covariates and 22.85% as exposures.  Therefore, taking their unique \ncontributions into account, we suggest combining both structured SDOH and NLP-extracted SDOH for \nassessment. \n \nFor the 5 common SDOH, structured SDOH consistently showed higher aORs for suicide than NLP-\nextracted SDOH. One possible explanation for this might be that in controls, who are less likely to be \nsick, clinicians may not be inclined to note their SDOH information in the structured data fields. We \nhypothesize that clinicians only do so when it is pertaining to the patient\u2019s primary diagnosis or ongoing \nclinical care, possibly representing a relatively sicker population than all the patients with identifiable \nSDOH in their clinical notes. For example, 14.64% of the case population were exposed to social \nproblems, as identified by the structured data, compared with 34.92% by the NLP system - a 2.4 times \nincrease (Table 2). However, this goes up to 2.8 times for the controls (7.96% vs 22.13%). Thus, using \nNLP-derived SDOH information might reduce information bias, an important problem in assessing \npsychosocial research questions. \n \nTo estimate whether intervening on SDOH has the potential to change suicide risk, it is necessary to \nseparate its influence from other related factors. In effect, we aimed at emulating the results of an \nexperimental setting where people who experience certain SDOH issues would be enrolled in a trial that \nrandomly assigns whether one receives an intervention. Because such a trial is not available, we relied \non observational health data to inform our understanding of suicide. We used epidemiologic methods to \nadjust for the differences between people exposed to SDOH and those who were not. We carefully \nconsidered several possible confounding health and demographic factors in our design to obtain the \nbest possible estimate of the associations of SDOH on suicide.  \n \nOur work shows a strong impact of SDOH on Veterans\u2019 risk of suicide using a nested case-control design, \nin which both the covariate and exposure assessment periods are limited to two years. This setup \nreduces the burden of data processing and NLP extraction, and yet provides a valid assessment of the \npotential associations between (recent) SDOH and suicide. On the other hand, using longer covariate \nand exposure assessment periods could provide more information and insights on both short-term \n(acute) and long-term (persistent) impacts of SDOH on suicide. A related problem is that SDOH change \nover time; as such, it is more appropriate to treat them as time-varying exposures for longer exposure \nassessment periods. These time-varying aspects of the problem will be carefully explored in our future \nwork.  \n \nLimitations \nOur study has some limitations. First, the VA population does not represent the general US population. \nHowever, many studies and innovations from the VHA have been shown to assist non-VHA facilities in \nadopting better clinical practices34\u201336.   Second, there is potential for residual confounding. Third, EHR \ndata might have incomplete or missing SDOH information37, making it challenging to assess the influence \nof SDOH on any target outcome. However, most SDOH with a direct relation to provided care are \nrecorded, so our approach is unlikely to miss important SDOH when both structured and unstructured \ndata are used.  \n \n 13 \n \nConclusions \nOurs is the first large-scale study to implement and use an NLP system to extract SDOH information from \nunstructured EHR data. We showed that SDOH can significantly contribute to Veterans\u2019 death by suicide. \nOur results also indicate that integrating NLP-based SDOH can benefit similar analyses by identifying \nmore patients at risk. We strongly believe that analyzing all available SDOH information, including those \ncontained in clinical notes, can help develop a better system for risk assessment and suicide prevention. \nHowever, more studies are required to investigate ways of seamlessly incorporating SDOH into existing \nhealthcare systems. \n \nAcknowledgements \nWe thank our annotators Raelene Goodwin, Heather Keating, and Emily Druhl for annotating EHR notes \nthat were essential for training our NLP system. A Mitra and H Yu had full access to all the data in the \nstudy and takes responsibility for the integrity of the data and the accuracy of the data analysis This \nwork was funded by the grant R01MH125027 from the National Institute of Mental Health (NIMH) of \nthe National Institutes of Health (NIH). The funding source had no role in the design and conduct of the \nstudy; collection, management, analysis, and interpretation of the data; preparation, review, or approval \nof the manuscript; and decision to submit the manuscript for publication. The contents of this paper do \nnot represent the views of NIH, VA, or the United States Government. \n \nReferences \n1.  \nHeron M. Deaths: Leading causes for 2019. Natl Vital Stat Reports. 2021;70(9):1-114. \ndoi:10.15620/cdc:107021 \n2.  \nStone DM, Simon TR, Fowler KA, et al. Vital Signs: Trends in State Suicide Rates \u2014 \nUnited States, 1999\u20132016 and Circumstances Contributing to Suicide \u2014 27 States, 2015. \nMMWR Morb Mortal Wkly Rep. 2018;67(22):617-624. doi:10.15585/mmwr.mm6722a1 \n3.  \nShepard DS, Gurewich D, Lwin AK, Reed GA, Silverman MM. Suicide and Suicidal \nAttempts in the United States: Costs and Policy Implications. Suicide Life Threat Behav. \n2016;46(3):352-362. doi:10.1111/sltb.12225 \n4.  \n2021 National Veteran Suicide Prevention Annual Report. Off Ment Heal Suicide Prev. \n2021. https://www.mentalhealth.va.gov/docs/data-sheets/2021/2021-National-Veteran-\nSuicide-Prevention-Annual-Report-FINAL-9-8-21.pdf. Accessed October 9, 2021. \n5.  \nHealthyPeople.gov. Social Determinants of Health | Healthy People 2020. Heal People \n2020 Top Object. 2014:5-8. https://www.healthypeople.gov/2020/topics-\nobjectives/topic/social-determinants-of-health. Accessed November 30, 2021. \n6.  \nBlosnich JR, Montgomery AE, Dichter ME, et al. Social Determinants and Military \nVeterans\u2019 Suicide Ideation and Attempt: a Cross-sectional Analysis of Electronic Health \nRecord Data. J Gen Intern Med. 2020;35(6):1759-1767. doi:10.1007/s11606-019-05447-\nz \n7.  \nHaw C, Hawton K, Gunnell D, Platt S. Economic recession and suicidal behaviour: \nPossible mechanisms and ameliorating factors. Int J Soc Psychiatry. 2015;61(1):73-81. \ndoi:10.1177/0020764014536545 \n8.  \nKim HM, Smith EG, Ganoczy D, et al. Predictors of suicide in patient charts among \npatients with depression in the Veterans Health Administration health system: Importance \nof prescription drug and alcohol abuse. J Clin Psychiatry. 2012;73(10). \ndoi:10.4088/JCP.12m07658 \n9.  \nKaufman JA, Salas-Hern\u00e1ndez LK, Komro KA, Livingston MD. Effects of increased \nminimum wages by unemployment rate on suicide in the USA. J Epidemiol Community \nHealth. 2020;74(3):219-224. doi:10.1136/jech-2019-212981 \n10.  \nKposowa AJ. Unemployment and suicide: A cohort analysis of social factors predicting \nsuicide in the US National Longitudinal Mortality Study. Psychol Med. 2001;31(1):127-\n 14 \n \n138. doi:10.1017/S0033291799002925 \n11.  \nNock MK, Borges G, Bromet EJ, Cha CB, Kessler RC, Lee S. Suicide and suicidal \nbehavior. Epidemiol Rev. 2008;30(1):133-154. doi:10.1093/epirev/mxn002 \n12.  \nDube SR, Anda RF, Felitti VJ, Chapman DP, Williamson DF, Giles WH. Childhood abuse, \nhousehold dysfunction, and the risk of attempted suicide throughout the life span: \nFindings from the adverse childhood experiences study. J Am Med Assoc. \n2001;286(24):3089-3096. doi:10.1001/jama.286.24.3089 \n13.  \nDorr D, Bejan CA, Pizzimenti C, Singh S, Storer M, Quinones A. Identifying patients with \nsignificant problems related to social determinants of health with natural language \nprocessing. In: Studies in Health Technology and Informatics. Vol 264. Stud Health \nTechnol Inform; 2019:1456-1457. doi:10.3233/SHTI190482 \n14.  \nKessler RC, Bauer MS, Bishop TM, et al. Using Administrative Data to Predict Suicide \nAfter Psychiatric Hospitalization in the Veterans Health Administration System. Front \nPsychiatry. 2020;11:390. doi:10.3389/fpsyt.2020.00390 \n15.  \nBarak-Corren Y, Castro VM, Javitt S, et al. Predicting suicidal behavior from longitudinal \nelectronic health records. Am J Psychiatry. 2017;174(2):154-162. \ndoi:10.1176/appi.ajp.2016.16010077 \n16.  \nRibeiro JD, Huang X, Fox KR, Franklin JC. Depression and hopelessness as risk factors \nfor suicide ideation, attempts and death: Meta-analysis of longitudinal studies. Br J \nPsychiatry. 2018;212(5):279-286. doi:10.1192/bjp.2018.27 \n17.  \nAhmedani BK, Peterson EL, Hu Y, et al. Major Physical Health Conditions and Risk of \nSuicide. Am J Prev Med. 2017;53(3):308-315. doi:10.1016/j.amepre.2017.04.001 \n18.  \nWalkup JT, Townsend L, Crystal S, Olfson M. A systematic review of validated methods \nfor identifying suicide or suicidal ideation using administrative or claims data. \nPharmacoepidemiol Drug Saf. 2012;21(SUPPL. 1):174-182. doi:10.1002/pds.2335 \n19.  \nU.S. Department of Veterans Affairs. National Center for Veterans Analysis and \nStatistics. Veterans Administration. https://www.va.gov/vetdata/. Published 2016. \nAccessed October 9, 2021. \n20.  \nPartlett C, Hall NJ, Leaf A, Juszczak E, Linsell L. Application of the matched nested case-\ncontrol design to the secondary analysis of trial data. BMC Med Res Methodol. \n2020;20(1):1-8. doi:10.1186/s12874-020-01007-w \n21.  \nKimbrel NA, Ashley-Koch AE, Qin XJ, et al. A genome-wide association study of suicide \nattempts in the million veterans program identifies evidence of pan-ancestry and \nancestry-specific risk loci. Mol Psychiatry. 2022;27(4):2264-2272. doi:10.1038/s41380-\n022-01472-3 \n22.  \nDefense Manpower and Data Center, Sunnyvale, California. Joint, Department of \nVeterans Affairs (VA) and Department of Defense (DoD) Mortality Data Repository -\nNational Death Index (NDI) Extract. \nhttps://www.mirecc.va.gov/suicideprevention/Data/data_index.asp. Accessed January 24, \n2022. \n23.  \nvon Elm E, Altman DG, Egger M, Pocock SJ, G\u00f8tzsche PC, Vandenbroucke JP. The \nStrengthening the Reporting of Observational Studies in Epidemiology (STROBE) \nstatement: guidelines for reporting observational studies. Lancet. 2007;370(9596):1453-\n1457. doi:10.1016/S0140-6736(07)61602-X \n24.  \nLiu Y, Ott M, Goyal N, et al. RoBERTa: A Robustly Optimized BERT Pretraining \nApproach. July 2019. http://arxiv.org/abs/1907.11692. Accessed January 17, 2020. \n25.  \nDevlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional \ntransformers for language understanding. In: NAACL HLT 2019 - 2019 Conference of the \nNorth American Chapter of the Association for Computational Linguistics: Human \nLanguage Technologies - Proceedings of the Conference. Vol 1. Stroudsburg, PA, USA: \nAssociation for Computational Linguistics; 2019:4171-4186. doi:10.18653/v1/N19-1423 \n 15 \n \n26.  \nAssessment and Management of Patients at Risk for Suicide (2019) - VA/DoD Clinical \nPractice Guidelines. https://www.healthquality.va.gov/guidelines/MH/srb/. Accessed June \n25, 2022. \n27.  \nQuan H, Li B, Couris CM, et al. Updating and validating the charlson comorbidity index \nand score for risk adjustment in hospital discharge abstracts using data from 6 countries. \nAm J Epidemiol. 2011;173(6):676-682. doi:10.1093/aje/kwq433 \n28.  \nBelsley D a, Kuh E, Welsch RE. Regression Diagnostics: Identifying Influential Data and \nSources of Collinearity.; 1980. doi:10.1002/0471725153 \n29.  \nBlosnich JR, Montgomery AE, Taylor LD, Dichter ME. Adverse social factors and all-\ncause mortality among male and female patients receiving care in the Veterans Health \nAdministration. Prev Med (Baltim). 2020;141. doi:10.1016/j.ypmed.2020.106272 \n30.  \nPandya CJ, Hatef E, Wu JB, Richards T, Weiner JP, Kharrazi H. Impact of Social Needs \nin Electronic Health Records and Claims on Health Care Utilization and Costs Risk-\nAdjustment Models Within Medicaid Population. Popul Health Manag. 2022;25(5):658-\n668. doi:10.1089/POP.2022.0069 \n31.  \nMitra A, Ahsan H, Li W, et al. Risk factors associated with nonfatal opioid overdose \nleading to intensive care unit admission: A cross-sectional study. JMIR Med Informatics. \n2021;9(11):e32851. doi:10.2196/32851 \n32.  \nMann CJ. Observational research methods. Research design II: cohort, cross sectional, \nand case-control studies. Emerg Med J. 2003;20(1):54-60. doi:10.1136/EMJ.20.1.54 \n33.  \nDobscha SK, Denneson LM, Kovas AE, et al. Correlates of suicide among veterans \ntreated in primary care: case-control study of a nationally representative sample. J Gen \nIntern Med. 2014;29 Suppl 4(Suppl 4):853-860. doi:10.1007/S11606-014-3028-1 \n34.  \nOliver A. Public-sector health-care reforms that work? A case study of the US Veterans \nHealth Administration. Lancet. 2008;371(9619):1211-1213. doi:10.1016/S0140-\n6736(08)60528-0 \n35.  \nHicken BL, Plowhead A. A model for home-based psychology from the veterans health \nadministration. Prof Psychol Res Pract. 2010;41(4):340-346. doi:10.1037/A0020431 \n36.  \nFihn SD, Francis J, Clancy C, et al. Insights From Advanced Analytics At The Veterans \nHealth Administration. https://doi.org/101377/hlthaff20140054. 2017;33(7):1203-1211. \ndoi:10.1377/HLTHAFF.2014.0054 \n37.  \nWeiskopf NG, Bakken S, Hripcsak G, Weng C. A Data Quality Assessment Guideline for \nElectronic Health Record Data Reuse. eGEMs (Generating Evid Methods to Improv \npatient outcomes). 2017;5(1):14. doi:10.5334/egems.218 \n \nAppendix \nAppendix 1: NLP Model Development \nDataset \nWe used a stratified random sampling approach to sample 3,000 Veterans who received treatment at \nthe VHA and died between fiscal year 2009-2017. Patients were stratified and sampled by several key \nsociodemographic variables (race, gender, and age), geographic location (Northeast, Midwest, South, \nand West, 1:1:1:1 ratio), and death by suicide (2:1 ratio to those who did not die from suicide). We also \noversampled underrepresented groups such as women and ethnic minorities. For the ease of manual \nannotation, we further reduced the number of Veterans by sampling 1,393 veterans while maintaining a \nVeterans with suicide attempt to no attempt ratio of 1:4. For each Veteran, we chose three types of \nnotes \u2013 social worker notes, mental health notes and emergency room visit notes.  \n \nAt first, we conducted a pre-screening phase using a predefined set of keywords to select the most \nrelevant sentences in a note and turned each of them into a three-sentence paragraph by using the \nprevious and next sentences. All these paragraphs came from 4,646 unique EHR notes. Next, these \n 16 \n \nparagraphs were annotated for 13 distinct SDOH, behavioral and other relevant factors by three expert \nannotators under expert supervision (Table 1). These 13 factors were selected based on expert opinions \nand the recent clinical practice guideline issued by the Veterans Affairs and Department of Defense 1. \nEach factor was further annotated for two attributes, \u2018presence\u2019 (yes, not yes) and \u2018period\u2019 (current, not \ncurrent).  \n \nNLP Extracted \nVariables \nBrief descriptions \nExample tokens \nSocial isolation* \nSocial and behavioral status to detect \nloneliness, lack of social and/or family \nsupport; marital/relationship status. \nAlone, lonely, divorce, widow etc. \nTransition of \ncare* \nChange of admission status (discharge, \ntransfer etc.); change in medication \nand/or provider. \nDischarge, admission, change in \nmedication, transfer etc. \nBarriers to care* Transportation issues; communication \nproblems; lack of trust or rapport; \nintellectual disability \nTransportation issues, garbled speech, \ncommunication problems etc. \nFinancial \ninsecurity* \nFinancial issues; job problems; poverty \nUnemployed, poor, unemployment, \nrehabilitation etc. \nHousing \ninstability* \nHousing issues \nEviction, homeless, homelessness etc. \nFood insecurity* Poor diet and/or nutrition; lack of \naccess to proper meal; dependency on \nfood charities/voucher/stamps \nHungry, pantry, starvation, food \nvoucher etc. \nViolence* \nAvailability and/or access to lethal \nmeans; bullying; domestic violence; any \nharassment/abuse/trauma; racism; \nhomicidal ideation; feeling \nscared/unsafe \nFirearms, violence, assault, weapon, \nabuse, homicidal, racism etc. \nLegal problems* Imprisonment; court-related matters; \ndetentions; disciplinary action; \nrestraining orders; brushes with the \nlaw; criminal charges; any violation of \nlaw \nImprisonment, parole, arrested, felony, \ninvestigation, prison etc. \nSubstance abuse Drug use disorder; alcohol \nconsumption; alcohol use disorder; \naddiction; overdose \nAlcohol, tobacco, heroin, cocaine, \nsmoking, overdose etc. \nPsychiatric \nsymptoms \nHopelessness; Insomnia; Problem \nsolving difficulty; decreased \nPTSD, depression, anxiety, \nschizophrenia, insomnia, hallucination \n 17 \n \npsychosocial functioning; psychiatric \nhospitalization; eating disorder; \nmention of any psychiatric disease \netc. \nPain \nPhysical pain \nPain, suffering, hurting, discomfort etc. \nPatient disability Reliance on assistive devices and/or \ndisability pay; service-connected ratings \nDisabled, blind, hearing loss, wheelchair \netc. \nSuicide outcome Suicide attempt and/or ideation \nFeel like shooting myself, no desire to \nlive, better off dead \n*SDOH \neTable 1. Examples of NLP-extracted factors. \nTask and Experimental Setup \nWe designed this as a sequence labeling problem where, given a sentence from a note, our goal was to \nlabel each word with the appropriate SDOH/behavioral factor and detect the two attributes \u2013 presence \nand period. To be specific, we implemented a multi-task learning framework. A 60:20:20 split was used \nfor the train, validation, and test sets. We experimented with four pretrained language models, namely, \nRoBERTa 2, BioBERT 3, Bio+Clinical BERT 4 and EhrBERT 5, and for each, we added three heads on top, to \nidentify factors and their attributes jointly. Our code is publicly available at \nhttps://github.com/avipartho/SequenceLabelingWithMultiTaskLEarning. \n \nResult \nWe evaluated the model performance using precision, recall and F-score. We considered both exact \nmatching (i.e., both the span boundary and entity type match with that of the ground truth) and relaxed \nmatching (i.e., the span boundaries overlap and entity types match) to assess the model performance. In \nour experiments, RoBERTa achieved the best performance among all the models. The results for \nRoBERTa are shown in eTable 2, eTable 3 and eTable 4. \n \nSDOH/behavioral \nFactors \nExact \nRelaxed \nPrecision \nRecall \nF-score \nPrecision \nRecall \nF-score \nSocial isolation* \n85.45 \n88.04 \n86.73 \n92.05 \n94.84 \n93.42 \nTransition of care* \n80.07 \n91.92 \n85.59 \n81.21 \n93.23 \n86.81 \nBarriers to care* \n54.66 \n62.93 \n58.50 \n68.43 \n78.78 \n73.24 \nFinancial insecurity* \n64.52 \n76.04 \n69.81 \n74.43 \n87.71 \n80.53 \nHousing instability* \n74.37 \n82.24 \n78.11 \n81.06 \n89.64 \n85.13 \nFood insecurity* \n66.87 \n76.43 \n71.33 \n70.63 \n80.71 \n75.33 \nViolence* \n67.01 \n74.71 \n70.65 \n79.71 \n88.88 \n84.04 \nLegal problems* \n64.45 \n68.86 \n66.58 \n82.35 \n87.98 \n85.07 \nSubstance abuse \n70.75 \n78.61 \n74.48 \n82.32 \n91.47 \n86.65 \nPsychiatric \nsymptoms \n74.72 \n82.64 \n78.48 \n82.46 \n91.20 \n86.61 \nPain \n77.82 \n85.88 \n81.65 \n86.86 \n95.86 \n91.14 \nPatient disability \n85.91 \n89.35 \n87.60 \n90.41 \n94.03 \n92.19 \nSuicide outcome \n67.43 \n73.89 \n70.51 \n81.12 \n88.89 \n84.82 \nMicro \n75.03 \n82.36 \n78.52 \n83.14 \n91.27 \n87.02 \n 18 \n \nMacro \n71.85 \n79.35 \n75.39 \n81.00 \n89.48 \n85.00 \n \neTable 2. Performance of our MTL model \u2013 factor identification. \n \nPresence \nExact \nRelaxed \nPrecision \nRecall \nF-score \nPrecision \nRecall \nF-score \nYes \n71.86 \n77.05 \n74.36 \n81.20 \n87.06 \n84.02 \nNot yes \n74.14 \n76.43 \n75.27 \n79.76 \n82.22 \n80.97 \nMicro \n72.68 \n76.82 \n74.69 \n80.68 \n85.28 \n82.92 \nMacro \n73.00 \n76.74 \n74.82 \n80.48 \n84.64 \n82.50 \n \neTable 3. Performance of our MTL model \u2013 presence identification. \n \nPresence \nExact \nRelaxed \nPrecision \nRecall \nF-score \nPrecision \nRecall \nF-score \nCurrent \n75.65 \n79.84 \n77.69 \n83.78 \n88.42 \n86.04 \nNot current \n58.93 \n64.94 \n61.79 \n68.28 \n75.24 \n71.59 \nMicro \n73.69 \n78.16 \n75.86 \n81.96 \n86.94 \n84.38 \nMacro \n67.29 \n72.39 \n69.74 \n76.03 \n81.83 \n78.81 \n \neTable 4. Performance of our MTL model \u2013 period identification. \n \nAppendix 2: ICD and Stop Codes for structured SDOH and Mental Health Disorders \n \nSDOH \n \nSDOH factors \nICD-9 Codes \nStop Codes \nSocial or familial problems \nV60.6, V60.89, V62.3, V62.89, V61, V62.4 \n- \nEmployment or financial \nproblems  \nV60.2, V60.89, V60.9, V62.0, V62.1, \nV62.29 \n208, 222, 535, 555, \n568, 574 \nHousing instability \nV60.0-2, V60.89 \n504, 507, 508, 511, \n522, 528, 529, 530, \n555, 556, 590 \nLegal problems \nV62.5, V62.89, E849.7 \n591, 592 \nViolence \nE904.0, E960.0-1, E961-E977, E979, \nE990.0-3, E990.9, E991.0-9, E992.0-3, \nE992.8-9, E993.0-9, E994.0-3, E994.8-9, \nE995.0-4, E995.8-9, E996.0-3, E996.8-9, \nE997.0-3, E997.8-9, E998.0-1, E998.8-9, \nE999.0-1, V15.41-42, V15.49, V71.5, \nV71.81, 995.50-54, 995.80-85 \n524 \n 19 \n \nNon-specific psychosocial \nneeds \nV62.29, V62.3-6, V62.81, V62.89, V62.9 \n- \n \neTable 5. ICD and stop codes for structured SDOH. \nComorbidities \n \nMental Health Disorders \nICD-9 Codes \n Major Depressive disorder \n293.83, 296.2, 296.3, 296.9, 298.0, 300.4, 301.12, \n309.0, 309.1, and 311 \nAlcohol Use Disorder \n291.0-5, 291.8-9, 303.0-303.9, 305.0, 357.5, 425.5, \n571.0-3, 535.3, V11.3 \n Drug Use Disorder  \n292.0-1, 304.0-304.9, 305.2-305.8 \nAnxiety Disorder  \n300.0, 300.1, 300.2, 799.2 \nPosttraumatic Stress Disorder \n309.81 \nSchizophrenia  \n295.0-295.9, V11.0 \nBipolar disorder \n296.0, 296.1, 296.4-7, 296.80, 296.81, 296.82, 296.89, \n296.90, 296.99, V11.1 \n \neTable 6. ICD codes for mental health disorders \n \nAppendix 3: Base Cohort Statistics \n \n \nBase Cohort  \n(N=6,122,785) \n% \nRace \nWhite \n4,713,683 \n76.99% \nBlack \n997,035 \n16.28% \nAsian \n58,075 \n0.95% \nNative Hawaiian or Other Pacific \nIslander \n50,210 \n0.82% \nAmerican Indian \n44,028 \n0.72% \nUnknown \n259,754 \n4.24% \nGender \n 20 \n \nMale \n5,646,838 \n92.23% \nFemale \n475,947 \n7.77% \nAge \n18-29 \n403,618 \n6.59% \n30-39 \n435,592 \n7.11% \n40-49 \n640,441 \n10.46% \n50-59 \n1,072,726 \n17.52% \n60-69 \n1,896,202 \n30.97% \n70-79 \n938,691 \n15.33% \n80-100 \n735,515 \n12.01% \nMarital Status \nMarried \n1,672,089 \n27.31% \nSingle \n330,837 \n5.40% \nDivorced \n598,341 \n9.77% \nWidowed \n200,842 \n3.28% \nUnknown \n3,320,676 \n54.23 \n \neTable 7. Summary Statistics of the Base Cohort \n \nAppendix 4: SDOH prevalence \n \nSDOH \nNLP only \nStructured data only \nPresent in both \nSocial problems \n71.03% \n13.89% \n15.08% \nFinancial insecurity \n74.44% \n8.92% \n16.64% \nHousing insecurity \n66.17% \n14.77% \n19.06% \nLegal problems \n49.03% \n36.98% \n13.99% \nViolence \n59.25% \n31.13% \n9.62% \n \neTable 9. Prevalence of Combined SDOH Factors by Source (as Covariates) \n 21 \n \nSDOH \nNLP only \nStructured data only \nPresent in both \nSocial problems \n67.69% \n14.20% \n18.11% \nFinancial insecurity \n69.19% \n10.23% \n20.58% \nHousing insecurity \n63.47% \n13.89% \n22.64% \nLegal problems \n45.29% \n36.68% \n18.03% \nViolence \n63.52% \n24.87% \n11.61% \n \neTable 10. Prevalence of Combined SDOH Factors by Source (as Exposures) \n \nAppendix 5: Associations for concurrent SDOH \n \nSDOH factors \n \nNLP-extracted, \naOR (95% CI)* \nStructured, \naOR (95% CI)* \nCombined, \naOR (95% CI)* \nSocial problems, \nFinancial problems \n2.39 (2.22, 2.59) \n2.48 (2.18, 2.82) \n2.34 (2.17, 2.52) \nSocial problems, \nHousing instability \n2.47 (2.28, 2.68) \n2.41 (2.11, 2.74) \n2.38 (2.20, 2.57) \nSocial problems, Legal \nproblems \n3.01 (2.70, 3.36) \n2.60 (2.33, 2.92) \n2.84 (2.60, 3.09) \nSocial problems, \nViolence \n2.94 (2.70, 3.21) \n3.37 (2.82, 4.02) \n2.69 (2.49, 2.91) \nSocial problems, \nBarriers to care \n2.46 (2.27, 2.67) \n- \n2.41 (2.23, 2.61) \nSocial problems, \nTransition of care \n2.15 (2.02, 2.29) \n- \n2.12 (2.00, 2.26) \nSocial problems, Food \ninsecurity \n2.19 (1.87, 2.56) \n- \n2.15 (1.85, 2.50) \nSocial problems, Non-\nspecific psychosocial \nneeds \n- \n2.15 (1.96, 2.36) \n2.21 (2.04, 2.40) \nFinancial problems, \nHousing instability \n2.45 (2.25, 2.66) \n2.10 (1.86, 2.35) \n2.35 (2.17, 2.55) \nFinancial problems, \nLegal problems \n3.00 (2.68, 3.35) \n3.16 (2.66, 3.75) \n2.98 (2.70, 3.27) \nFinancial problems, \nViolence \n2.83 (2.58, 3.11) \n3.54 (2.87, 4.36) \n2.69 (2.47, 2.94) \nFinancial problems, \nBarriers to care \n2.35 (2.16, 2.57) \n- \n2.33 (2.14, 2.54) \nFinancial problems, \nTransition of care \n2.07 (1.94, 2.22) \n- \n2.08 (1.94, 2.22) \nFinancial problems, \nFood insecurity \n2.19 (1.87, 2.56) \n- \n2.20 (1.88, 2.57) \nFinancial problems, \nNon-specific \npsychosocial needs \n- \n2.45 (2.14, 2.80) \n2.37 (2.16, 2.60) \nHousing instability, \nLegal problems \n3.16 (2.81,3.56) \n2.91 (2.45, 3.45) \n3.14 (2.84, 3.47) \n 22 \n \nHousing instability, \nViolence \n2.95 (2.67, 3.27) \n3.13 (2.52, 3.88) \n2.77 (2.52, 3.04) \nHousing instability, \nBarriers to care \n2.42 (2.20, 2.66) \n- \n2.41 (2.20, 2.64) \nHousing instability, \nTransition of care \n2.08 (1.93, 2.23) \n- \n2.08 (1.93, 2.23) \nHousing instability, \nFood insecurity \n2.21 (1.87, 2.61) \n- \n2.18 (1.85, 2.56) \nHousing instability, \nNon-specific \npsychosocial needs \n- \n2.64 (2.30, 3.03) \n2.46 (2.23, 2.72) \nLegal problems, \nViolence \n3.44 (3.03, 3.89) \n3.31 (2.72, 4.02) \n3.34 (3.01, 3.70) \nLegal problems, \nBarriers to care \n3.00 (2.65, 3.39) \n- \n2.93 (2.63, 3.25) \nLegal problems, \nTransition of care \n2.78 (2.51, 3.07) \n- \n2.77 (2.54, 3.02) \nLegal problems, Food \ninsecurity \n2.73 (2.20, 3.39) \n- \n2.66 (2.20, 3.23) \nLegal problems, Non-\nspecific psychosocial \nneeds \n- \n2.65 (2.39, 2.94) \n2.74 (2.48, 3.01) \nViolence, Barriers to \ncare \n2.82 (2.55, 3.12) \n- \n2.65 (2.41, 2.91) \nViolence, Transition of \ncare \n2.51 (2.32, 2.71) \n- \n2.31 (2.15, 2.48) \nViolence, Food \ninsecurity \n2.54 (2.09, 3.09) \n- \n2.39 (1.99, 2.88) \nViolence, Non-specific \npsychosocial needs \n- \n2.93 (2.50, 3.43) \n2.83 (2.55, 3.15) \nBarriers to care, \nTransition of care \n2.00 (1.86, 2.14) \n- \n1.99 (1.86, 2.13) \nBarriers to care, Food \ninsecurity \n2.19 (1.86, 2.59) \n- \n2.20 (1.87, 2.60) \nBarriers to care, Non-\nspecific psychosocial \nneeds \n- \n- \n2.40 (2.16, 2.66) \nTransition of care, Food \ninsecurity \n1.95 (1.70, 2.24) \n- \n1.96 (1.70, 2.25) \nTransition of care, Non-\nspecific psychosocial \nneeds \n- \n- \n2.23 (2.05, 2.42) \nFood insecurity, Non-\nspecific psychosocial \nneeds \n- \n- \n2.48 (2.04, 3.02) \n*Each model was adjusted for socio-demographic variables, psychiatric symptoms, substance abuse, \npain, patient disability, clinical comorbidities and all SDOH in its group. \n 23 \n \neTable 8. Associations of SDOH with Veterans\u2019 death by suicide \n \nAppendix References \n1.  \nAssessment and Management of Patients at Risk for Suicide (2019) - VA/DoD Clinical \nPractice Guidelines. https://www.healthquality.va.gov/guidelines/MH/srb/. Accessed June \n25, 2022. \n2.  \nLiu Y, Ott M, Goyal N, et al. RoBERTa: A Robustly Optimized BERT Pretraining \nApproach. July 2019. http://arxiv.org/abs/1907.11692. Accessed January 17, 2020. \n3.  \nLee J, Yoon W, Kim S, et al. BioBERT: a pre-trained biomedical language representation \nmodel for biomedical text mining. Bioinformatics. September 2019. \ndoi:10.1093/bioinformatics/btz682 \n4.  \nAlsentzer E, Murphy J, Boag W, et al. Publicly Available Clinical BERT Embeddings. In: \nAssociation for Computational Linguistics (ACL); 2019:72-78. doi:10.18653/v1/w19-1909 \n5.  \nLi F, Jin Y, Liu W, Rawat BPS, Cai P, Yu H. Fine-tuning bidirectional encoder \nrepresentations from transformers (BERT)\u2013based models on large-scale electronic health \nrecord notes: An empirical study. J Med Internet Res. 2019;21(9):e14830. \ndoi:10.2196/14830 \n \n \n \n \n \n \n"}, "Relational World Knowledge Representation in Contextual Language Models: A Review": {"authors": ["Tara Safavi", "Danai Koutra"], "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "url": "https://arxiv.org/pdf/2104.05837.pdf", "abstract": "Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold: (1) We provide a high-level, extensible taxonomy for knowledge representation in LMs; (2) Within our taxonomy, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.", "arxiv_id": "2104.05837", "published_date": "2021-04-12", "year": 2021, "introduction": "Introduction Knowledge bases (KBs) are data structures that connect pairs of entities or concepts by semantically meaningful symbolic relations. Decades\u2019 worth of research have been invested into using KBs as tools for relational world knowledge representation in machines (Minsky, 1974; Lenat, 1995; Liu and Singh, 2004; Bollacker et al., 2008; Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014; Speer et al., 2017; Sap et al., 2019; Ilievski et al., 2021). Most large-scale modern KBs are organized according to a manually engineered schema that speci\ufb01es which entity and relation types are permitted, and how such types may interact with one another. This explicit enforcement of relational structure is both an advantage and a drawback (Halevy et al., 2003). On one hand, schemas support complex queries over the data with accurate, consistent, and interpretable answers. On the other hand, schemas are \u201contological commitments\u201d (Davis et al., 1993) that limit \ufb02exibility in how knowledge is stored, expressed, and accessed. Handcrafted schemas also require signi\ufb01cant human engineering effort to construct and maintain, and are therefore often highly incomplete (Weikum et al., 2021). Language models as KBs? The tension between structured and unstructured knowledge representations is not new in natural language processing (Banko and Etzioni, 2008; Fader et al., 2011). However, only recently has an especially promising solution emerged, brought about by breakthroughs in machine learning software, hardware, and data. Speci\ufb01cally, deep contextual language models (LMs) like BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have shown to be capable of internalizing a degree of relational world knowledge within their parameters, and expressing this knowledge across various mediums and tasks\u2014in some cases, without the need for a prede\ufb01ned entity-relation schema (Petroni et al., 2019; Roberts et al., 2020). Consequently, some have begun to wonder whether LMs will partially or even fully replace KBs, given suf\ufb01ciently large training budgets and parameter capacities. Present work In this review, we summarize recent compelling progress in machine representation of relational world knowledge with LMs. We propose to organize relevant work by the level of KB supervision provided to the LM (Figure 1): \u2022 Word-level supervision (\u00a7 3): At this level, LMs are not explicitly supervised on a KB, but may be indirectly exposed to KB-like knowledge via word associations in the training corpus. Here, we cover techniques for probing arXiv:2104.05837v2  [cs.CL]  10 Sep 2021 Figure 1: A high-level overview of our taxonomy, organized by level of KB supervision provided. and utilizing this implicitly acquired knowledge. \u2022 Entity-level supervision (\u00a7 4): At this level, LMs are supervised to acquire knowledge of KB entities. Here, we organize strategies from \u201cless symbolic\u201d to \u201cmore symbolic\u201d: Less symbolic approaches train LMs with entity-aware language modeling losses, but never explicitly require the LM to link entity mentions to the KB. By contrast, more symbolic approaches involve linking, and may also integrate entity embeddings into the LM\u2019s parameters. \u2022 Relation-level supervision (\u00a7 5): At this level, LMs are supervised to acquire knowledge of KB triples and paths. Again, we organize strategies from less to more symbolic, where less symbolic approaches treat triples as fully natural language statements, and more symbolic approaches incorporate dedicated embeddings of KB relation types. For each supervision level, we provide notable examples in terms of methodology and/or \ufb01ndings, and compare the bene\ufb01ts and drawbacks of different approaches. We conclude in \u00a7 6 with our vision of the future, emphasizing the complementary roles of LMs and KBs as knowledge representations. Related work As this topic is relatively nascent, few related surveys exist. Closest to our own work, Colon-Hernandez et al. (2021) cover methods for combining contextual language representations with graph representations, albeit with a comparatively narrow scope and no discussion of implicit knowledge. Liu et al. (2021a) survey promptbased learning in LMs, which overlaps with our discussion of cloze prompting in \u00a7 3.1, although relational world knowledge is not their main focus. 2 Preliminaries We brie\ufb02y review preliminaries and assumptions necessary for our survey. Knowledge bases We use the term \u201cknowledge base\u201d (KB) to refer to a relational data structure comprising a set of entities E, relation types R, and triples (s, r, o) \u2208 E \u00d7 R \u00d7 E, where s, o \u2208 E are subject and object entities, respectively.1 We consider two types of KBs under the umbrella of \u201crelational world knowledge.\u201d Encyclopedic KBs store facts about typed, disambiguated entities; a well-known example is the Wikidata KB (Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014), which, like its sister project Wikipedia, is publicly accessible and collaboratively constructed. By contrast, in commonsense KBs, \u201centities\u201d are typically represented by non-canonicalized free-text phrases. Examples include the publicly accessible, crowdsourced ConceptNet (Liu and Singh, 2004; Speer et al., 2017) and ATOMIC (Sap et al., 2019) KBs. Language models Following the contemporary NLP literature, we use the term \u201clanguage model\u201d (LM) to refer to a deep neural network that is trained to learn contextual text representations. LMs generally come pretrained, with parameters pre-initialized for generic text representation via self-supervised training on large corpora, and may be used as-is after pretraining, or further \ufb01netuned with supervision on downstream task(s). This work considers LMs based on the Transformer architecture (Vaswani et al., 2017), examples of which include the encoder-only BERT family (Devlin et al., 2019; Liu et al., 2019), the decoder-only GPT family (Brown et al., 2020), and the encoder-decoder T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) families. 3 Word-level supervision The standard language modeling task is to predict the n-th word in a sequence of n words\u2014that is, a conditional probability estimation task (Radford et al., 2019). While many variants of this task have been proposed to allow LMs to condition their predictions on different inputs (Devlin et al., 2019; Raffel et al., 2020; Lewis et al., 2020), a notable feature of all such approaches is that they operate at the word (and subword) level. If these supervision techniques do not incorporate KBs at all, how are they relevant when considering LMs as relational knowledge representations? The answer is simple. Typical language 1For our purposes, we consider the terms \u201cknowledge base\u201d and \u201cknowledge graph\u201d as interchangeable. Table 1: Taxonomy and representative examples for extracting relational knowledge in word-level pretrained LMs, with evaluation tasks that have been conducted in the referenced papers. Glossary of evaluation tasks: KP\u2014 knowledge probing; QA\u2014question answering; CR\u2014compositional reasoning; KC\u2014knowledge base construction. Knowledge extracted via... Extraction strategy Representative examples Evaluation task(s) KP QA CR KC Cloze prompts (\u00a7 3.1) Prompt handcrafting (Petroni et al., 2019; Dufter et al., 2021) \u0013 Automatic prompt engineering (Jiang et al., 2020b; Shin et al., 2020; Zhong et al., 2021; Qin and Eisner, 2021) \u0013 Adversarial prompt modi\ufb01cation (Kassner and Sch\u00fctze, 2020; Petroni et al., 2020; Poerner et al., 2020; Cao et al., 2021) \u0013 Varying base prompts (Elazar et al., 2021; Heinzerling and Inui, 2021; Jiang et al., 2020a; Kassner et al., 2021) \u0013 Symbolic rule-based prompting (Kassner et al., 2020; Talmor et al., 2020a) \u0013 \u0013 Statement scores (\u00a7 3.2) Single-LM scoring (Tamborrino et al., 2020; Zhou et al., 2020) \u0013 \u0013 Dual-LM scoring (Davison et al., 2019; Shwartz et al., 2020) \u0013 \u0013 modeling corpora like Wikipedia are known to contain KB-like assertions about the world (Da and Kasai, 2019). LMs trained on enough such data can be expected to acquire some KB-like knowledge, even without targeted entity- or relation-level supervision. Therefore, in order to motivate the necessity (if at all) of KB supervision, it is crucial to \ufb01rst understand what relational world \u201cknowledge\u201d LMs acquire from word-level pretraining. In this section, we cover strategies to extract and utilize this knowledge under the cloze prompting (\u00a7 3.1) and statement scoring (\u00a7 3.2) protocols. Table 1 provides a taxonomy for this section, with representative examples and evaluation tasks. 3.1 Cloze prompting The cloze prompting protocol (Taylor, 1953 and Figure 2) is a direct approach for extracting and evaluating KB-like knowledge in pretrained LMs. Under this protocol, KB triples are \ufb01rst converted to natural language assertions using (e.g.) relation templates. For each assertion, the token(s) corresponding to the object entity are held out. A frozen pretrained LM then ranks candidate tokens within its vocabulary by the probability that they \ufb01ll in the empty slot(s). Accuracy is typically measured by the proportion of prompts for which the correct answer appears in the LM\u2019s top-k predictions, with the assumption that better performance implies more pretrained knowledge within the LM. Handcrafted prompts in English with singletoken answers make up LAMA (Petroni et al., 2019), one of the earliest and most widely-used LM cloze probes. LAMA, which is mapped primarily to Wikidata and ConceptNet triples, was initially used to compare pretrained LMs\u2019 knowledge to offthe-shelf KB question answering systems. Petroni et al. (2019) showed that pretrained BERT is comFigure 2: Probing relational knowledge in pretrained LMs with cloze prompts generated from KB triples. petitive with a supervised relation extraction model that has been provided an oracle for entity linking, particularly for 1-1 queries. Subsequent work has experimented with handcrafted templates for probing the knowledge of both very large (hundredbillion parameter) LMs (Brown et al., 2020) as well as non-contextual word embeddings, i.e., as a simple control baseline for LMs (Dufter et al., 2021). Both studies demonstrate some success, particularly in cases where the probed model is provided a small amount of extra context in the form of conditioning examples (Brown et al., 2020) or entity type information (Dufter et al., 2021). Automatic prompt engineering is a promising alternative to prompt handcrafting for knowledge extraction in LMs (Liu et al., 2021a), as prompts engineered using discrete (Jiang et al., 2020b; Shin et al., 2020; Haviv et al., 2021) and continuous (Zhong et al., 2021; Qin and Eisner, 2021; Liu et al., 2021b) optimization have improved LMs\u2019 lower-bound performance on LAMA\u2019s underlying queries. Note, however, that optimized prompts are not always grammatical or intelligible (Shin et al., 2020). Prompt optimization methods may also confound knowledge probes by over\ufb01tting to the probes\u2019 answer distributions during training (Zhong et al., 2021; Cao et al., 2021), and often require large validation sets for tuning, which may not be feasible in practice (Perez et al., 2021). Adversarial modi\ufb01cation of LAMA prompts has uncovered weaknesses in pretrained LMs\u2019 world \u201cknowledge,\u201d for example that BERT\u2019s accuracy drops precipitously when irrelevant statements or negation words are added to prompts (Kassner and Sch\u00fctze, 2020; Lin et al., 2020; Petroni et al., 2020), and that it can \u201cguess\u201d answers using shallow lexical cues or benchmark artifacts (Poerner et al., 2020; Cao et al., 2021). However, the adversarial robustness of LM knowledge improves greatly with supervision in both the pretraining (Petroni et al., 2020) and \ufb01ne-tuning (Kassner and Sch\u00fctze, 2020) stages, suggesting that explicit KB-level supervision is a viable remedy to input sensitivity. Several collections of prompt variations, including paraphrased sets of base prompts (Elazar et al., 2021; Heinzerling and Inui, 2021) and multilingual sets of base (English) prompts (Jiang et al., 2020a; Kassner et al., 2021) have been released to expand the original research questions posed by LAMA. For the former, it has been found that pretrained BERT-based LMs typically do not output consistent answers for prompt paraphrases, although their consistency can again be greatly improved by targeted pretraining (Elazar et al., 2021; Heinzerling and Inui, 2021). For the latter, initial results on prompts beyond English indicate high variability in pretrained LM performance across languages and poor performance on prompts with multi-token answers (Jiang et al., 2020a; Kassner et al., 2021). Prompts generated with symbolic rules have been used to test pretrained LMs\u2019 abilities to learn, e.g., equivalence, implication, composition, and conjunction. Existing studies vary the degrees of experimental control: Talmor et al. (2020a) use BERT-based models with their publicly-available pretrained weights, whereas Kassner et al. (2020) pretrain BERT from scratch on synthetic KB triples only. Both studies observe mixed results, concluding that word-level pretraining alone (at least on BERT) does not lead to strong \u201creasoning\u201d skills. 3.2 Statement scoring Beyond probing, pretrained LM \u201cknowledge\u201d can be purposed toward downstream KB-level tasks in a zero-shot manner via statement scoring. Here, a pretrained LM is fed natural language statements corresponding to KB triples, and its token probabilities across each statement are pooled to yield statement scores. These scores are then treated as input to a downstream decision, mirroring the way that supervised LMs can be trained to output probabilities for triple-level prediction tasks (\u00a7 5). We categorize statement scoring strategies as single- or dual-LM approaches. The single-LM approach pools the pretrained LM\u2019s token scores over a candidate set of sequences, then takes the highest-scoring sequence as the LM\u2019s \u201cprediction\u201d or choice (Tamborrino et al., 2020; Bouraoui et al., 2020; Zhou et al., 2020; Brown et al., 2020). The dual-LM framework \ufb01rst uses one pretrained LM to generate useful context (e.g., clari\ufb01cation text) for the task, then feeds this context to another, possibly different pretrained LM to obtain a \ufb01nal score (Davison et al., 2019; Shwartz et al., 2020). Both categories have shown promise over comparable unsupervised (and, under some conditions, supervised) methods for tasks like multiple-choice QA (Tamborrino et al., 2020; Shwartz et al., 2020; Brown et al., 2020) and commonsense KB completion (Davison et al., 2019). However, LM scores have also shown to be sensitive to small perturbations in text (Zhou et al., 2020), so this approach may be less effective on noisy or long-tail inputs. 3.3 Summary and outlook There is still broad disagreement over the nature of acquired \u201cknowledge\u201d in pretrained LMs. Whereas some studies suggest that word-level pretraining may be enough to endow LMs with KB-like knowledge (Petroni et al., 2019; Tamborrino et al., 2020), in particular given enough parameters and the right set of prompts (Brown et al., 2020), others conclude that such pretraining alone does not yield suf\ufb01ciently precise or robust LM knowledge (Elazar et al., 2021; Cao et al., 2021)\u2014directly motivating the targeted supervision strategies discussed in the remainder of this paper. We observe that different studies independently set objectives for what a pretrained LM should \u201cknow,\u201d and thus naturally reach different conclusions. We believe that future studies must reach consensus on standardized tasks and benchmarks, addressing questions like: What degree of overlap between a pretraining corpus and a knowledge probe is permissible, and how can this be accurately uncovered and quanti\ufb01ed? What lexical cues or correlations should be allowed in knowledge probes? Progress in this direction will Table 2: Taxonomy and representative examples of entity-level supervision in LMs, with evaluation tasks that have been conducted in the referenced papers. Glossary of evaluation tasks: KP\u2014knowledge probing; EL\u2014entity linking; ET\u2014entity typing; RC\u2014relation classi\ufb01cation; QA\u2014question answering; GL\u2014the General Language Understanding Evaluation or GLUE benchmark (Wang et al., 2019), which covers multiple subtasks. Entities as... Supervision strategy Representative examples Evaluation task(s) KP EL ET RC QA GL Token mention-spans (\u00a7 4.1) Masked token prediction (Roberts et al., 2020; Guu et al., 2020) \u0013 Contrastive learning (Xiong et al., 2020; Shen et al., 2020) \u0013 \u0013 \u0013 Text-to-KB links\u2014late fusion (\u00a7 4.2) Linking w/o external info (Broscheit, 2019; Ling et al., 2020) \u0013 \u0013 Linking w/ textual metadata (Wu et al., 2020; De Cao et al., 2021) \u0013 \u0013 \u0013 Linking w/ external embeddings (Zhang et al., 2019; Chen et al., 2020) \u0013 \u0013 \u0013 \u0013 Text-to-KB links\u2014mid/early fusion (\u00a7 4.3) Entity embedding retrieval (Peters et al., 2019; F\u00e9vry et al., 2020) \u0013 \u0013 \u0013 \u0013 \u0013 Treating entities as tokens (Yamada et al., 2020; Poerner et al., 2020) \u0013 \u0013 \u0013 \u0013 \u0013 not only further our understanding of the effects of word-level supervision on LM knowledge acquisition, but will also provide appropriate yardsticks for measuring the bene\ufb01ts of targeted entity- and relation-level supervision. 4 Entity-level supervision We next review entity-level supervision strategies for LMs, most often toward improving performance in knowledge probes like LAMA (\u00a7 3.1) and canonical NLP tasks like entity typing, entity linking, and question answering. We roughly categorize approaches from \u201cleast symbolic\u201d to \u201cmost symbolic.\u201d On the former end of the spectrum, the LM is exposed to entity mentions in text but not required to link these mentions to an external entity bank (\u00a7 4.1). On the latter end, the LM is trained to link mentions to the KB using late (\u00a7 4.2) or mid-to-early fusion approaches (\u00a7 4.3). Table 2 provides a taxonomy of supervision strategies for this section with representative examples. 4.1 Modeling entities without linking The \u201cleast symbolic\u201d entity supervision approaches that we consider input textual contexts containing entity mention-spans to the LM, and incorporate these mention-spans into their losses. However, they do not require the LM to link these mentions to the KB\u2019s entity set, so the LM is never directly exposed to the KB. Figures 3a and 3b provide examples of input and output for this class of approaches. Masking tokens in mention-spans and training LMs to predict these tokens may promote knowledge memorization (Sun et al., 2020). Roberts et al. (2020) investigate this strategy using a simple masking strategy whereby an LM is trained to predict the tokens comprising named entities and dates in text (Figure 3a, originally proposed by Guu et al., 2020). The authors \ufb01nd that the largest (11 billion parameter) version of T5 generates exact-match answers on open-domain question answering (QA) benchmarks with higher accuracy than extractive systems\u2014even without access to external context documents, simulating a \u201cclosed-book\u201d exam. Contrastive learning techniques, which have been used for LM supervision at the word and sentence level (Devlin et al., 2019), have also been devised for supervision on entity mentions (Shen et al., 2020). For example, Xiong et al. (2020) replace a proportion of entity mentions in the pretraining corpus with the names of negatively-sampled entities of the same type, and train an LM to predict whether the entity in the span has been replaced (Figure 3b). Although the previously discussed closed-book T5 model (Roberts et al., 2020) outperforms Xiong et al. (2020)\u2019s open-book BERT pretrained with contrastive entity replacement on open-domain QA, the latter may generalize better: T5\u2019s performance degrades considerably for facts not observed during training, whereas open-book approaches appear more robust (Lewis et al., 2021). 4.2 Linking with late fusion The next-strongest level of entity supervision is to train the LM to link entity-centric textual contexts to a KB\u2019s entity set E. Here, we cover late fusion approaches, which operate at the word level in terms of input to the LM and incorporate entities at the LM\u2019s output layer only, as exempli\ufb01ed in Figure 3c. The simplest representatives of this category train LMs to match individual tokens (Broscheit, 2019) or mentions (Ling et al., 2020) in a text corpus to an entity bank, without any external resources. The minimally \u201centity-aware\u201d BERT proposed by Broscheit (2019), which adds a single classi\ufb01cation layer on top of a pretrained BERT encoder, achieves competitive results with a state-of-the-art specialized entity linking architec(a) Mention-span masking (b) Contrastive learning (c) Linking\u2014late fusion (d) Linking\u2014early fusion Figure 3: Examples of entity-level supervision in LMs, ranging from \u201cless symbolic\u201d to \u201cmore symbolic.\u201d ture (Kolitsas et al., 2018). Entity meta-information such as names and descriptions are viable external resources for LMpowered entity linking (Botha et al., 2020). For example, in zero-shot entity linking (Logeswaran et al., 2019), textual mentions must be linked to entities unseen during training using only entity descriptions as additional data. Here, competitive solutions train separate BERT models to select and rank candidate entities by encoding their descriptions (Logeswaran et al., 2019; Wu et al., 2020). More recently, encoder-decoder LMs have been trained to retrieve entities by generating their unique names (De Cao et al., 2021), which has the advantage of scaling with the LM\u2019s vocabulary size (usually tens of thousands) instead of the KB entity set size (potentially tens of millions). De Cao et al. (2021) achieve results competitive to discriminative approaches on entity linking and QA, suggesting the potential of generative entity-aware LMs. External entity embeddings pretrained by a separate model have been used as strong sources of inductive bias for LMs. For example, several variants of BERT further pretrain the base model by linearly fusing external entity embeddings with contextual word representations at the output of the BERT encoder (Zhang et al., 2019; He et al., 2020). BERT has also been \ufb01ne-tuned to match its output token representations to external entity embeddings for the task of end-to-end entity linking (Chen et al., 2020). Such approaches rely heavily on the quality of the externally-learned embeddings, which is both a strength and a drawback: Such embeddings may contain useful implicit structural information about the KB, but on the other hand may propagate errors into the LM (Shen et al., 2020). 4.3 Linking with middle or early fusion The last and strongest category of entity supervision techniques that we consider are also linkingbased, but fuse entity information at earlier stages of text encoding. Mid-fusion approaches retrieve external entity representations in between hidden layers and re-contextualize them into the LM, whereas early fusion approaches simply treat entity symbols as tokens in the vocabulary. Figure 3d provides an example of input/output for early fusion. Retrieving entity embeddings and integrating them into an LM\u2019s hidden word representations is a middle-fusion technique that has the advantage of modeling \ufb02exibility: It allows the practitioner to choose where (i.e., at which layer) the entity embeddings are integrated, and how the entity embeddings are learned and re-contextualized into the LM. Peters et al. (2019) integrate externally pre-trained, frozen entity embeddings into BERT\u2019s \ufb01nal hidden layers using a word-to-entity attention mechanism. F\u00e9vry et al. (2020) learn the external entity embeddings jointly during pretraining, and perform the integration in BERT\u2019s earlier hidden layers using an attention-weighted sum. The latter approach is competitive with a 30\u00d7 larger T5 LM in closed-book QA (\u00a7 4.1), suggesting that LMs and KB embeddings can be trained jointly to enhance and complement each other. Treating entities as \u201ctokens\u201d by appending special reserved entity symbols to the LM\u2019s vocabulary is the earliest of entity fusion approaches (Figure 3d). For instance, Yamada et al. (2020) input entity \u201ctokens\u201d alongside textual contexts that mention these entities to RoBERTa, and use specialized word-to-entity and entity-to-entity attention matrices within its hidden layers. Other approaches leave the base LM\u2019s internal architecture completely unchanged and focus only on aligning the LM\u2019s word and entity embedding spaces at the input level (Rosset et al., 2020; Poerner et al., 2020). Note, however, that this approach may signi\ufb01cantly enlarge the LM\u2019s vocabulary. For example, plain BERT\u2019s vocabulary is around 30k tokens, whereas Table 3: Taxonomy and representative examples of relation-level supervision in LMs, with evaluation tasks conducted in the respective referenced papers. Glossary of evaluation tasks: KP\u2014knowledge probing; ET\u2014entity typing; RC\u2014relation classi\ufb01cation; QA\u2014question answering; CR\u2014compositional reasoning; KC\u2014knowledge base construction; TG\u2014text generation; GL\u2014the GLUE family of language tasks (Wang et al., 2019). Relations as... Supervision strategy Representative examples Evaluation task(s) KP ET RC QA CR KC TG GL Templated sentences (\u00a7 5.1) Lexicalizing triples (Thorne et al., 2021; Guan et al., 2020) \u0013 \u0013 \u0013 Lexicalizing paths (Clark et al., 2020; Talmor et al., 2020a,b) \u0013 \u0013 Linearized sequences (\u00a7 5.2) Training on triple sequences (Yao et al., 2019; Agarwal et al., 2021) \u0013 \u0013 \u0013 \u0013 Injecting triples into text (Liu et al., 2020) \u0013 Dedicated embeddings (\u00a7 5.3) Pooling entity representations (Baldini Soares et al., 2019; Qin et al., 2021) \u0013 \u0013 \u0013 Embedding relations externally (Wang et al., 2021d; Daza et al., 2021) \u0013 \u0013 \u0013 \u0013 Treating relations as tokens (Bosselut et al., 2019; Hwang et al., 2021) \u0013 English Wikipedia has around 6 million entities. This can make pretraining on a larger vocabulary expensive in terms of both time and memory usage (Yamada et al., 2020; Dufter et al., 2021). 4.4 Summary and outlook The literature on entity supervision in LMs is growing rapidly. In line with recent trends in NLP (Khashabi et al., 2020), a growing number of entity supervision strategies use generative models (Roberts et al., 2020; De Cao et al., 2021), which are attractive because they allow for a high level of \ufb02exibility in output and circumvent the need for classi\ufb01cation over potentially millions of entities. However, some studies \ufb01nd that generative models currently do not perform well beyond what they have memorized from the training set (Wang et al., 2021b; Lewis et al., 2021). These \ufb01ndings suggest that storing some entity knowledge externally (e.g., in a dense memory, F\u00e9vry et al., 2020) may be more robust, for example by allowing for ef\ufb01cient updates to the LM\u2019s knowledge (Verga et al., 2020). We believe that future work will need to analyze the tradeoffs between fully-parametric and retrieval-based entity modeling in terms of pure accuracy, parameter and training ef\ufb01ciency, and ability to generalize beyond the training set. 5 Relation-level supervision Finally, we consider methods that utilize KB triples or paths to supervise LMs for complex, often compositional tasks like relation classi\ufb01cation, text generation, and rule-based inference. We again organize methods in the order of less to more symbolic. In this context, less symbolic approaches treat triples and paths as fully natural language (\u00a7 5.1, 5.2). By contrast, more symbolic approaches learn distinct embeddings for relation types in the KB (\u00a7 5.3). Table 3 provides a taxonomy of this section with representative examples and evaluation tasks. 5.1 Relations as templated assertions Template-based lexicalization is a popular relation supervision strategy that does not directly expose the LM to the KB. Similar to how KB queries are converted to cloze prompts for knowledge probing (\u00a7 3.1), triples are \ufb01rst converted to natural language assertions using relation templates, usually handcrafted. These assertions are then fed as input to the LM, which is trained with any number of task-speci\ufb01c losses. Figure 4 provides an input/output example for this class of approach. Lexicalized triples from Wikidata have been used as LM training data in proof-of-concept studies demonstrating that LMs can serve as natural language querying interfaces to KBs under controlled conditions (Heinzerling and Inui, 2021). A promising approach in this direction uses encoder-decoder LMs to generate answer sets to natural language queries over lexicalized Wikidata triples (Thorne et al., 2020, 2021), toward handling multi-answer KB queries with LMs\u2014thus far an understudied task in the LM knowledge querying literature. Other approaches convert KB triples to sentences using relation templates in order to construct taskspeci\ufb01c training datasets for improved performance in, e.g., story generation (Guan et al., 2020), commonsense QA (Ye et al., 2020; Ma et al., 2021), and relation classi\ufb01cation (Bouraoui et al., 2020). While most of these approaches rely on template handcrafting, a few automatically mine templates using distant supervision on Wikipedia, achieving competitive results in tasks like relation classi\ufb01cation (Bouraoui et al., 2020) and commonsense QA (Ye et al., 2020). Compositional paths spanning multiple atoms of symbolic knowledge may also be lexicalized and input to an LM (Lauscher et al., 2020; Talmor et al., 2020a) in order to train LMs for soft comFigure 4: Strategies for representing relations as sequences: Templating (\u00a7 5.1) and linearization (\u00a7 5.2). positional reasoning (Clark et al., 2020; Talmor et al., 2020b). Notably, when RoBERTa is \ufb01netuned on sentences expressing (real or synthetic) facts and rules from a KB, it can answer entailment queries with high accuracy (Clark et al., 2020; Talmor et al., 2020b). However, as Clark et al. (2020) note, these results do not necessarily con\ufb01rm that LMs can \u201creason,\u201d but rather that they can at least emulate soft reasoning\u2014raising an open question about how to develop probes and metrics to verify whether LMs can actually reason compositionally. 5.2 Linearizing KB triples The main advantage of templating is that it converts symbolic triples into sequences, which can be straightforwardly input to LMs. However, handcrafting templates is a manual process, and distant supervision can be noisy. To maintain the advantage of templates while avoiding the drawbacks, triples can alternatively be fed to an LM by linearizing them\u2014that is, \ufb02attening the subject, relation, and object into an input sequence (Figure 4). With linearization, relation-level supervision becomes as simple as feeding the linearized sequences to the LM and training again with task-speci\ufb01c losses (Yao et al., 2019; Kim et al., 2020; Ribeiro et al., 2020; Wang et al., 2021a) or injecting the sequences into the pretraining corpus (Liu et al., 2020). A notable recent example of the former approach (Agarwal et al., 2021) trains T5 on linearized Wikidata triples in order to generate fully natural language versions of those triples. These verbalized triples are used as retrieval \u201cdocuments\u201d for improved LM-based QA over traditional document corpora; note, however, that they can also be used as LM training data for other downstream tasks in place of handcrafted templates (\u00a7 5.1). 5.3 Relations as dedicated embeddings The strategies discussed thus far treat KB triples and paths as natural language sequences. A \u201cmore symbolic\u201d approach is to represent KB relation types with dedicated embeddings, and integrate these embeddings into the LM using late, middle, or early fusion approaches. Figures 5a and 5b provide input/output examples for late fusion, whereby relation textual contexts are input to the LM, and relation embeddings are constructed or integrated at the LM\u2019s output. Figure 5c exempli\ufb01es early fusion, whereby relations are treated as input tokens. Contextual representations of entity mentionspans may be pooled at an LM\u2019s output layer to represent a relation (Wang et al., 2021c; Yu et al., 2020). For example, Baldini Soares et al. (2019) concatenate the contextual representations of special entity-start markers inserted adjacent to textual entity mentions, and \ufb01ne-tune BERT to output similar relation representations for statements ranging over the same entity pairs (Figure 5a). This approach, which proved highly successful for relation classi\ufb01cation, has been applied to the same task in languages beyond English (K\u00f6ksal and \u00d6zg\u00fcr, 2020; Ananthram et al., 2020), and as an additional LM pretraining objective (Qin et al., 2021). Non-contextual relation embeddings may be learned by de\ufb01ning a separate relation embedding matrix with |R| rows and fusing this matrix into the LM. One advantage of this approach, similar to methods for retrieving external entity embeddings (\u00a7 4.3), is that it supports fusion at both the late (Wang et al., 2021d; Daza et al., 2021) and middle (Liu et al., 2021c) stages. As an example of the former, Wang et al. (2021d) propose an LM pretraining objective whereby textual descriptions of KB entities are input to and encoded by an LM, then combined with externally-learned relation embeddings at the output using a link prediction loss (Figure 5b). Combined with standard word-level language modeling objectives, this approach enables generalization across both sentencelevel tasks like relation classi\ufb01cation, and graphlevel tasks like KB completion. Treating relations as \u201ctokens,\u201d toward early fusion of relations in LMs, is achieved by appending the KB\u2019s relation types to the LM\u2019s vocabulary (Figure 5c). A notable instantiation of this approach is the COMET commonsense KB construction framework (Bosselut et al., 2019; Hwang et al., 2021; Jiang et al., 2021). Given a subject phrase/relation token as input, COMET \ufb01ne-tunes an LM to generate object phrases. COMET demon(a) Late fusion\u2014pooling (b) Late fusion\u2014external embeddings (c) Early fusion\u2014relations as \u201ctokens\u201d Figure 5: Examples of relation supervision strategies that incorporate dedicated embeddings of relation types. strates promising improvements over 400\u00d7 larger LMs not trained for KB construction (Hwang et al., 2021). However, templating (\u00a7 5.1) may yield better results than adding special tokens to the vocabulary when the COMET framework is trained and tested in a few-shot setting (Da et al., 2021). 5.4 Summary and outlook Relation-level supervision in LMs is exciting because it enables a wide variety of complex NLP tasks (Table 3). A unifying theme across many of these tasks is that of compositionality, or the idea that smaller \u201cbuilding blocks\u201d of evidence can be combined to arrive at novel knowledge. As compositionality is thought to be key to machine generalization (Lake et al., 2017), we believe that further fundamental research in understanding and improving LMs\u2019 soft \u201creasoning\u201d skills (Clark et al., 2020; Talmor et al., 2020b, \u00a7 5.1) will be crucial. Finally, while most of the open directions we discuss involve improving LM knowledge with KBs, we \ufb01nd the direction of generating KBs with LMs equally intriguing\u2014re\ufb02ecting the fact that LMs and KBs can complement each other in \u201cboth directions,\u201d as automating and scaling out the construction of KBs will ultimately provide LMs with more relational training data. The generative COMET framework (Bosselut et al., 2019, \u00a7 5.3) has made inroads in commonsense KB construction, but the same progress has not yet been observed for encyclopedic knowledge. The latter entails unique challenges: Whereas commonsense entities are not disambiguated and triples need only be plausible rather than always true, encyclopedic entities are usually disambiguated and facts are often binary true/false. We look forward to future research that addresses these challenges, perhaps building on recent breakthroughs in generative factual entity retrieval (De Cao et al., 2021, \u00a7 4.2). 6 Conclusion and vision In this review, we provide an overview of how LMs may acquire relational world knowledge during pretraining and \ufb01ne-tuning. We propose a novel taxonomy that classi\ufb01es knowledge representation methodologies based on the level of KB supervision provided to an LM, from no explicit supervision at all to entity- and relation-level supervision. In the future, we envision a stronger synergy between the perspectives and tools from the language modeling and knowledge bases communities. In particular, we expect powerful and expressive LMs, which are actively being developed in NLP, to be increasingly combined with large-scale KB resources to improve their knowledge recall and reasoning abilities. On the converse, we expect such KB resources to be increasingly generated directly by LMs. Within both of these directions, we hope that future work will continue to explore the themes discussed in this paper, in particular that of delineating and testing KB-level memorization versus generalization in LMs. We also expect that more standardized benchmarks and tasks for evaluating LM knowledge will be developed, a direction that has recently seen some progress (Petroni et al., 2021). As research at the intersection of LMs and KBs is rapidly progressing, we look forward to new research that better develops and combines the strengths of both knowledge representations. Acknowledgements We thank the reviewers for their thoughtful feedback. This material is based upon work supported by the National Science Foundation under a Graduate Research Fellowship and CAREER Grant No. IIS 1845491, the Advanced Machine Learning Collaborative Grant from Procter & Gamble, and an Amazon faculty award. ", "conclusion": "", "full_text": "Relational World Knowledge Representation\nin Contextual Language Models: A Review\nTara Safavi, Danai Koutra\nUniversity of Michigan, Ann Arbor\n{tsafavi,dkoutra}@umich.edu\nAbstract\nRelational knowledge bases (KBs) are com-\nmonly used to represent world knowledge in\nmachines. However, while advantageous for\ntheir high degree of precision and interpretabil-\nity, KBs are usually organized according to\nmanually-de\ufb01ned schemas, which limit their\nexpressiveness and require signi\ufb01cant human\nefforts to engineer and maintain. In this review,\nwe take a natural language processing perspec-\ntive to these limitations, examining how they\nmay be addressed in part by training deep con-\ntextual language models (LMs) to internalize\nand express relational knowledge in more \ufb02ex-\nible forms.\nWe propose to organize knowl-\nedge representation strategies in LMs by the\nlevel of KB supervision provided, from no KB\nsupervision at all to entity- and relation-level\nsupervision. Our contributions are threefold:\n(1) We provide a high-level, extensible tax-\nonomy for knowledge representation in LMs;\n(2) Within our taxonomy, we highlight notable\nmodels, evaluation tasks, and \ufb01ndings, in or-\nder to provide an up-to-date review of current\nknowledge representation capabilities in LMs;\nand (3) We suggest future research directions\nthat build upon the complementary aspects of\nLMs and KBs as knowledge representations.\n1\nIntroduction\nKnowledge bases (KBs) are data structures that\nconnect pairs of entities or concepts by seman-\ntically meaningful symbolic relations. Decades\u2019\nworth of research have been invested into using\nKBs as tools for relational world knowledge repre-\nsentation in machines (Minsky, 1974; Lenat, 1995;\nLiu and Singh, 2004; Bollacker et al., 2008; Vran-\nde\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014; Speer et al., 2017; Sap\net al., 2019; Ilievski et al., 2021).\nMost large-scale modern KBs are organized ac-\ncording to a manually engineered schema that spec-\ni\ufb01es which entity and relation types are permitted,\nand how such types may interact with one another.\nThis explicit enforcement of relational structure is\nboth an advantage and a drawback (Halevy et al.,\n2003). On one hand, schemas support complex\nqueries over the data with accurate, consistent, and\ninterpretable answers. On the other hand, schemas\nare \u201contological commitments\u201d (Davis et al., 1993)\nthat limit \ufb02exibility in how knowledge is stored, ex-\npressed, and accessed. Handcrafted schemas also\nrequire signi\ufb01cant human engineering effort to con-\nstruct and maintain, and are therefore often highly\nincomplete (Weikum et al., 2021).\nLanguage models as KBs?\nThe tension be-\ntween structured and unstructured knowledge rep-\nresentations is not new in natural language process-\ning (Banko and Etzioni, 2008; Fader et al., 2011).\nHowever, only recently has an especially promis-\ning solution emerged, brought about by break-\nthroughs in machine learning software, hardware,\nand data. Speci\ufb01cally, deep contextual language\nmodels (LMs) like BERT (Devlin et al., 2019) and\nGPT-3 (Brown et al., 2020) have shown to be ca-\npable of internalizing a degree of relational world\nknowledge within their parameters, and express-\ning this knowledge across various mediums and\ntasks\u2014in some cases, without the need for a prede-\n\ufb01ned entity-relation schema (Petroni et al., 2019;\nRoberts et al., 2020). Consequently, some have be-\ngun to wonder whether LMs will partially or even\nfully replace KBs, given suf\ufb01ciently large training\nbudgets and parameter capacities.\nPresent work\nIn this review, we summarize re-\ncent compelling progress in machine representation\nof relational world knowledge with LMs. We pro-\npose to organize relevant work by the level of KB\nsupervision provided to the LM (Figure 1):\n\u2022 Word-level supervision (\u00a7 3): At this level,\nLMs are not explicitly supervised on a KB, but\nmay be indirectly exposed to KB-like knowl-\nedge via word associations in the training cor-\npus. Here, we cover techniques for probing\narXiv:2104.05837v2  [cs.CL]  10 Sep 2021\n Figure 1: A high-level overview of our taxonomy, orga-\nnized by level of KB supervision provided.\nand utilizing this implicitly acquired knowl-\nedge.\n\u2022 Entity-level supervision (\u00a7 4): At this level,\nLMs are supervised to acquire knowledge of\nKB entities. Here, we organize strategies from\n\u201cless symbolic\u201d to \u201cmore symbolic\u201d: Less sym-\nbolic approaches train LMs with entity-aware\nlanguage modeling losses, but never explicitly\nrequire the LM to link entity mentions to the\nKB. By contrast, more symbolic approaches\ninvolve linking, and may also integrate entity\nembeddings into the LM\u2019s parameters.\n\u2022 Relation-level supervision (\u00a7 5): At this\nlevel, LMs are supervised to acquire knowl-\nedge of KB triples and paths. Again, we or-\nganize strategies from less to more symbolic,\nwhere less symbolic approaches treat triples as\nfully natural language statements, and more\nsymbolic approaches incorporate dedicated\nembeddings of KB relation types.\nFor each supervision level, we provide notable ex-\namples in terms of methodology and/or \ufb01ndings,\nand compare the bene\ufb01ts and drawbacks of differ-\nent approaches. We conclude in \u00a7 6 with our vision\nof the future, emphasizing the complementary roles\nof LMs and KBs as knowledge representations.\nRelated work\nAs this topic is relatively nascent,\nfew related surveys exist.\nClosest to our own\nwork, Colon-Hernandez et al. (2021) cover meth-\nods for combining contextual language representa-\ntions with graph representations, albeit with a com-\nparatively narrow scope and no discussion of im-\nplicit knowledge. Liu et al. (2021a) survey prompt-\nbased learning in LMs, which overlaps with our\ndiscussion of cloze prompting in \u00a7 3.1, although\nrelational world knowledge is not their main focus.\n2\nPreliminaries\nWe brie\ufb02y review preliminaries and assumptions\nnecessary for our survey.\nKnowledge bases\nWe use the term \u201cknowledge\nbase\u201d (KB) to refer to a relational data structure\ncomprising a set of entities E, relation types R,\nand triples (s, r, o) \u2208 E \u00d7 R \u00d7 E, where s, o \u2208 E\nare subject and object entities, respectively.1 We\nconsider two types of KBs under the umbrella of\n\u201crelational world knowledge.\u201d Encyclopedic KBs\nstore facts about typed, disambiguated entities; a\nwell-known example is the Wikidata KB (Vran-\nde\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014), which, like its sister\nproject Wikipedia, is publicly accessible and col-\nlaboratively constructed. By contrast, in common-\nsense KBs, \u201centities\u201d are typically represented by\nnon-canonicalized free-text phrases. Examples in-\nclude the publicly accessible, crowdsourced Con-\nceptNet (Liu and Singh, 2004; Speer et al., 2017)\nand ATOMIC (Sap et al., 2019) KBs.\nLanguage models\nFollowing the contemporary\nNLP literature, we use the term \u201clanguage model\u201d\n(LM) to refer to a deep neural network that is\ntrained to learn contextual text representations.\nLMs generally come pretrained, with parame-\nters pre-initialized for generic text representation\nvia self-supervised training on large corpora, and\nmay be used as-is after pretraining, or further \ufb01ne-\ntuned with supervision on downstream task(s).\nThis work considers LMs based on the Trans-\nformer architecture (Vaswani et al., 2017), ex-\namples of which include the encoder-only BERT\nfamily (Devlin et al., 2019; Liu et al., 2019), the\ndecoder-only GPT family (Brown et al., 2020), and\nthe encoder-decoder T5 (Raffel et al., 2020) and\nBART (Lewis et al., 2020) families.\n3\nWord-level supervision\nThe standard language modeling task is to predict\nthe n-th word in a sequence of n words\u2014that is,\na conditional probability estimation task (Radford\net al., 2019). While many variants of this task\nhave been proposed to allow LMs to condition their\npredictions on different inputs (Devlin et al., 2019;\nRaffel et al., 2020; Lewis et al., 2020), a notable\nfeature of all such approaches is that they operate\nat the word (and subword) level.\nIf these supervision techniques do not incorpo-\nrate KBs at all, how are they relevant when con-\nsidering LMs as relational knowledge representa-\ntions? The answer is simple. Typical language\n1For our purposes, we consider the terms \u201cknowledge base\u201d\nand \u201cknowledge graph\u201d as interchangeable.\n Table 1: Taxonomy and representative examples for extracting relational knowledge in word-level pretrained LMs,\nwith evaluation tasks that have been conducted in the referenced papers. Glossary of evaluation tasks: KP\u2014\nknowledge probing; QA\u2014question answering; CR\u2014compositional reasoning; KC\u2014knowledge base construction.\nKnowledge extracted via...\nExtraction strategy\nRepresentative examples\nEvaluation task(s)\nKP\nQA\nCR\nKC\nCloze prompts (\u00a7 3.1)\nPrompt handcrafting\n(Petroni et al., 2019; Dufter et al., 2021)\n\u0013\nAutomatic prompt engineering\n(Jiang et al., 2020b; Shin et al., 2020; Zhong\net al., 2021; Qin and Eisner, 2021)\n\u0013\nAdversarial prompt modi\ufb01cation\n(Kassner and Sch\u00fctze, 2020; Petroni et al.,\n2020; Poerner et al., 2020; Cao et al., 2021)\n\u0013\nVarying base prompts\n(Elazar et al., 2021; Heinzerling and Inui,\n2021; Jiang et al., 2020a; Kassner et al., 2021)\n\u0013\nSymbolic rule-based prompting\n(Kassner et al., 2020; Talmor et al., 2020a)\n\u0013\n\u0013\nStatement scores (\u00a7 3.2)\nSingle-LM scoring\n(Tamborrino et al., 2020; Zhou et al., 2020)\n\u0013\n\u0013\nDual-LM scoring\n(Davison et al., 2019; Shwartz et al., 2020)\n\u0013\n\u0013\nmodeling corpora like Wikipedia are known to con-\ntain KB-like assertions about the world (Da and\nKasai, 2019). LMs trained on enough such data\ncan be expected to acquire some KB-like knowl-\nedge, even without targeted entity- or relation-level\nsupervision. Therefore, in order to motivate the\nnecessity (if at all) of KB supervision, it is crucial\nto \ufb01rst understand what relational world \u201cknowl-\nedge\u201d LMs acquire from word-level pretraining.\nIn this section, we cover strategies to extract and\nutilize this knowledge under the cloze prompting\n(\u00a7 3.1) and statement scoring (\u00a7 3.2) protocols. Ta-\nble 1 provides a taxonomy for this section, with\nrepresentative examples and evaluation tasks.\n3.1\nCloze prompting\nThe cloze prompting protocol (Taylor, 1953 and\nFigure 2) is a direct approach for extracting and\nevaluating KB-like knowledge in pretrained LMs.\nUnder this protocol, KB triples are \ufb01rst converted\nto natural language assertions using (e.g.) relation\ntemplates. For each assertion, the token(s) corre-\nsponding to the object entity are held out. A frozen\npretrained LM then ranks candidate tokens within\nits vocabulary by the probability that they \ufb01ll in\nthe empty slot(s). Accuracy is typically measured\nby the proportion of prompts for which the cor-\nrect answer appears in the LM\u2019s top-k predictions,\nwith the assumption that better performance im-\nplies more pretrained knowledge within the LM.\nHandcrafted prompts in English with single-\ntoken answers make up LAMA (Petroni et al.,\n2019), one of the earliest and most widely-used LM\ncloze probes. LAMA, which is mapped primarily\nto Wikidata and ConceptNet triples, was initially\nused to compare pretrained LMs\u2019 knowledge to off-\nthe-shelf KB question answering systems. Petroni\net al. (2019) showed that pretrained BERT is com-\nFigure 2: Probing relational knowledge in pretrained\nLMs with cloze prompts generated from KB triples.\npetitive with a supervised relation extraction model\nthat has been provided an oracle for entity link-\ning, particularly for 1-1 queries. Subsequent work\nhas experimented with handcrafted templates for\nprobing the knowledge of both very large (hundred-\nbillion parameter) LMs (Brown et al., 2020) as well\nas non-contextual word embeddings, i.e., as a sim-\nple control baseline for LMs (Dufter et al., 2021).\nBoth studies demonstrate some success, particu-\nlarly in cases where the probed model is provided\na small amount of extra context in the form of con-\nditioning examples (Brown et al., 2020) or entity\ntype information (Dufter et al., 2021).\nAutomatic prompt engineering is a promising al-\nternative to prompt handcrafting for knowledge\nextraction in LMs (Liu et al., 2021a), as prompts\nengineered using discrete (Jiang et al., 2020b;\nShin et al., 2020; Haviv et al., 2021) and continu-\nous (Zhong et al., 2021; Qin and Eisner, 2021; Liu\net al., 2021b) optimization have improved LMs\u2019\nlower-bound performance on LAMA\u2019s underlying\nqueries. Note, however, that optimized prompts\nare not always grammatical or intelligible (Shin\net al., 2020). Prompt optimization methods may\nalso confound knowledge probes by over\ufb01tting\nto the probes\u2019 answer distributions during train-\n ing (Zhong et al., 2021; Cao et al., 2021), and often\nrequire large validation sets for tuning, which may\nnot be feasible in practice (Perez et al., 2021).\nAdversarial modi\ufb01cation of LAMA prompts has\nuncovered weaknesses in pretrained LMs\u2019 world\n\u201cknowledge,\u201d for example that BERT\u2019s accuracy\ndrops precipitously when irrelevant statements or\nnegation words are added to prompts (Kassner and\nSch\u00fctze, 2020; Lin et al., 2020; Petroni et al., 2020),\nand that it can \u201cguess\u201d answers using shallow lex-\nical cues or benchmark artifacts (Poerner et al.,\n2020; Cao et al., 2021). However, the adversarial\nrobustness of LM knowledge improves greatly with\nsupervision in both the pretraining (Petroni et al.,\n2020) and \ufb01ne-tuning (Kassner and Sch\u00fctze, 2020)\nstages, suggesting that explicit KB-level supervi-\nsion is a viable remedy to input sensitivity.\nSeveral collections of prompt variations, includ-\ning paraphrased sets of base prompts (Elazar et al.,\n2021; Heinzerling and Inui, 2021) and multilingual\nsets of base (English) prompts (Jiang et al., 2020a;\nKassner et al., 2021) have been released to expand\nthe original research questions posed by LAMA.\nFor the former, it has been found that pretrained\nBERT-based LMs typically do not output consis-\ntent answers for prompt paraphrases, although their\nconsistency can again be greatly improved by tar-\ngeted pretraining (Elazar et al., 2021; Heinzerling\nand Inui, 2021). For the latter, initial results on\nprompts beyond English indicate high variability\nin pretrained LM performance across languages\nand poor performance on prompts with multi-token\nanswers (Jiang et al., 2020a; Kassner et al., 2021).\nPrompts generated with symbolic rules have\nbeen used to test pretrained LMs\u2019 abilities to learn,\ne.g., equivalence, implication, composition, and\nconjunction. Existing studies vary the degrees of\nexperimental control: Talmor et al. (2020a) use\nBERT-based models with their publicly-available\npretrained weights, whereas Kassner et al. (2020)\npretrain BERT from scratch on synthetic KB triples\nonly. Both studies observe mixed results, conclud-\ning that word-level pretraining alone (at least on\nBERT) does not lead to strong \u201creasoning\u201d skills.\n3.2\nStatement scoring\nBeyond probing, pretrained LM \u201cknowledge\u201d can\nbe purposed toward downstream KB-level tasks in\na zero-shot manner via statement scoring. Here, a\npretrained LM is fed natural language statements\ncorresponding to KB triples, and its token proba-\nbilities across each statement are pooled to yield\nstatement scores. These scores are then treated\nas input to a downstream decision, mirroring the\nway that supervised LMs can be trained to out-\nput probabilities for triple-level prediction tasks\n(\u00a7 5). We categorize statement scoring strategies\nas single- or dual-LM approaches. The single-LM\napproach pools the pretrained LM\u2019s token scores\nover a candidate set of sequences, then takes the\nhighest-scoring sequence as the LM\u2019s \u201cprediction\u201d\nor choice (Tamborrino et al., 2020; Bouraoui et al.,\n2020; Zhou et al., 2020; Brown et al., 2020). The\ndual-LM framework \ufb01rst uses one pretrained LM\nto generate useful context (e.g., clari\ufb01cation text)\nfor the task, then feeds this context to another,\npossibly different pretrained LM to obtain a \ufb01nal\nscore (Davison et al., 2019; Shwartz et al., 2020).\nBoth categories have shown promise over com-\nparable unsupervised (and, under some conditions,\nsupervised) methods for tasks like multiple-choice\nQA (Tamborrino et al., 2020; Shwartz et al., 2020;\nBrown et al., 2020) and commonsense KB comple-\ntion (Davison et al., 2019). However, LM scores\nhave also shown to be sensitive to small perturba-\ntions in text (Zhou et al., 2020), so this approach\nmay be less effective on noisy or long-tail inputs.\n3.3\nSummary and outlook\nThere is still broad disagreement over the nature of\nacquired \u201cknowledge\u201d in pretrained LMs. Whereas\nsome studies suggest that word-level pretraining\nmay be enough to endow LMs with KB-like knowl-\nedge (Petroni et al., 2019; Tamborrino et al., 2020),\nin particular given enough parameters and the right\nset of prompts (Brown et al., 2020), others con-\nclude that such pretraining alone does not yield suf-\n\ufb01ciently precise or robust LM knowledge (Elazar\net al., 2021; Cao et al., 2021)\u2014directly motivating\nthe targeted supervision strategies discussed in the\nremainder of this paper. We observe that differ-\nent studies independently set objectives for what a\npretrained LM should \u201cknow,\u201d and thus naturally\nreach different conclusions. We believe that future\nstudies must reach consensus on standardized tasks\nand benchmarks, addressing questions like: What\ndegree of overlap between a pretraining corpus and\na knowledge probe is permissible, and how can\nthis be accurately uncovered and quanti\ufb01ed? What\nlexical cues or correlations should be allowed in\nknowledge probes? Progress in this direction will\n Table 2: Taxonomy and representative examples of entity-level supervision in LMs, with evaluation tasks that\nhave been conducted in the referenced papers. Glossary of evaluation tasks: KP\u2014knowledge probing; EL\u2014entity\nlinking; ET\u2014entity typing; RC\u2014relation classi\ufb01cation; QA\u2014question answering; GL\u2014the General Language\nUnderstanding Evaluation or GLUE benchmark (Wang et al., 2019), which covers multiple subtasks.\nEntities as...\nSupervision strategy\nRepresentative examples\nEvaluation task(s)\nKP\nEL\nET\nRC\nQA\nGL\nToken mention-spans (\u00a7 4.1)\nMasked token prediction\n(Roberts et al., 2020; Guu et al., 2020)\n\u0013\nContrastive learning\n(Xiong et al., 2020; Shen et al., 2020)\n\u0013\n\u0013\n\u0013\nText-to-KB links\u2014late fusion (\u00a7 4.2)\nLinking w/o external info\n(Broscheit, 2019; Ling et al., 2020)\n\u0013\n\u0013\nLinking w/ textual metadata\n(Wu et al., 2020; De Cao et al., 2021)\n\u0013\n\u0013\n\u0013\nLinking w/ external embeddings\n(Zhang et al., 2019; Chen et al., 2020)\n\u0013\n\u0013\n\u0013\n\u0013\nText-to-KB links\u2014mid/early fusion (\u00a7 4.3)\nEntity embedding retrieval\n(Peters et al., 2019; F\u00e9vry et al., 2020)\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nTreating entities as tokens\n(Yamada et al., 2020; Poerner et al., 2020)\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nnot only further our understanding of the effects of\nword-level supervision on LM knowledge acquisi-\ntion, but will also provide appropriate yardsticks\nfor measuring the bene\ufb01ts of targeted entity- and\nrelation-level supervision.\n4\nEntity-level supervision\nWe next review entity-level supervision strategies\nfor LMs, most often toward improving perfor-\nmance in knowledge probes like LAMA (\u00a7 3.1)\nand canonical NLP tasks like entity typing, entity\nlinking, and question answering. We roughly cate-\ngorize approaches from \u201cleast symbolic\u201d to \u201cmost\nsymbolic.\u201d On the former end of the spectrum, the\nLM is exposed to entity mentions in text but not\nrequired to link these mentions to an external entity\nbank (\u00a7 4.1). On the latter end, the LM is trained\nto link mentions to the KB using late (\u00a7 4.2) or\nmid-to-early fusion approaches (\u00a7 4.3). Table 2\nprovides a taxonomy of supervision strategies for\nthis section with representative examples.\n4.1\nModeling entities without linking\nThe \u201cleast symbolic\u201d entity supervision approaches\nthat we consider input textual contexts containing\nentity mention-spans to the LM, and incorporate\nthese mention-spans into their losses. However,\nthey do not require the LM to link these mentions\nto the KB\u2019s entity set, so the LM is never directly\nexposed to the KB. Figures 3a and 3b provide exam-\nples of input and output for this class of approaches.\nMasking tokens in mention-spans and training\nLMs to predict these tokens may promote knowl-\nedge memorization (Sun et al., 2020). Roberts et al.\n(2020) investigate this strategy using a simple mask-\ning strategy whereby an LM is trained to predict\nthe tokens comprising named entities and dates in\ntext (Figure 3a, originally proposed by Guu et al.,\n2020). The authors \ufb01nd that the largest (11 billion\nparameter) version of T5 generates exact-match\nanswers on open-domain question answering (QA)\nbenchmarks with higher accuracy than extractive\nsystems\u2014even without access to external context\ndocuments, simulating a \u201cclosed-book\u201d exam.\nContrastive learning techniques, which have been\nused for LM supervision at the word and sentence\nlevel (Devlin et al., 2019), have also been devised\nfor supervision on entity mentions (Shen et al.,\n2020). For example, Xiong et al. (2020) replace\na proportion of entity mentions in the pretraining\ncorpus with the names of negatively-sampled en-\ntities of the same type, and train an LM to predict\nwhether the entity in the span has been replaced\n(Figure 3b). Although the previously discussed\nclosed-book T5 model (Roberts et al., 2020) out-\nperforms Xiong et al. (2020)\u2019s open-book BERT\npretrained with contrastive entity replacement on\nopen-domain QA, the latter may generalize better:\nT5\u2019s performance degrades considerably for facts\nnot observed during training, whereas open-book\napproaches appear more robust (Lewis et al., 2021).\n4.2\nLinking with late fusion\nThe next-strongest level of entity supervision is\nto train the LM to link entity-centric textual con-\ntexts to a KB\u2019s entity set E. Here, we cover late\nfusion approaches, which operate at the word level\nin terms of input to the LM and incorporate en-\ntities at the LM\u2019s output layer only, as exempli-\n\ufb01ed in Figure 3c. The simplest representatives\nof this category train LMs to match individual to-\nkens (Broscheit, 2019) or mentions (Ling et al.,\n2020) in a text corpus to an entity bank, without any\nexternal resources. The minimally \u201centity-aware\u201d\nBERT proposed by Broscheit (2019), which adds\na single classi\ufb01cation layer on top of a pretrained\nBERT encoder, achieves competitive results with a\nstate-of-the-art specialized entity linking architec-\n (a) Mention-span masking\n(b) Contrastive learning\n(c) Linking\u2014late fusion\n(d) Linking\u2014early fusion\nFigure 3: Examples of entity-level supervision in LMs, ranging from \u201cless symbolic\u201d to \u201cmore symbolic.\u201d\nture (Kolitsas et al., 2018).\nEntity meta-information such as names and de-\nscriptions are viable external resources for LM-\npowered entity linking (Botha et al., 2020). For\nexample, in zero-shot entity linking (Logeswaran\net al., 2019), textual mentions must be linked to\nentities unseen during training using only entity\ndescriptions as additional data. Here, competi-\ntive solutions train separate BERT models to se-\nlect and rank candidate entities by encoding their\ndescriptions (Logeswaran et al., 2019; Wu et al.,\n2020). More recently, encoder-decoder LMs have\nbeen trained to retrieve entities by generating their\nunique names (De Cao et al., 2021), which has the\nadvantage of scaling with the LM\u2019s vocabulary size\n(usually tens of thousands) instead of the KB entity\nset size (potentially tens of millions). De Cao et al.\n(2021) achieve results competitive to discriminative\napproaches on entity linking and QA, suggesting\nthe potential of generative entity-aware LMs.\nExternal entity embeddings pretrained by a sepa-\nrate model have been used as strong sources of in-\nductive bias for LMs. For example, several variants\nof BERT further pretrain the base model by linearly\nfusing external entity embeddings with contextual\nword representations at the output of the BERT en-\ncoder (Zhang et al., 2019; He et al., 2020). BERT\nhas also been \ufb01ne-tuned to match its output token\nrepresentations to external entity embeddings for\nthe task of end-to-end entity linking (Chen et al.,\n2020). Such approaches rely heavily on the qual-\nity of the externally-learned embeddings, which is\nboth a strength and a drawback: Such embeddings\nmay contain useful implicit structural information\nabout the KB, but on the other hand may propagate\nerrors into the LM (Shen et al., 2020).\n4.3\nLinking with middle or early fusion\nThe last and strongest category of entity supervi-\nsion techniques that we consider are also linking-\nbased, but fuse entity information at earlier stages\nof text encoding. Mid-fusion approaches retrieve\nexternal entity representations in between hidden\nlayers and re-contextualize them into the LM,\nwhereas early fusion approaches simply treat entity\nsymbols as tokens in the vocabulary. Figure 3d pro-\nvides an example of input/output for early fusion.\nRetrieving entity embeddings and integrating\nthem into an LM\u2019s hidden word representations\nis a middle-fusion technique that has the advantage\nof modeling \ufb02exibility: It allows the practitioner\nto choose where (i.e., at which layer) the entity\nembeddings are integrated, and how the entity em-\nbeddings are learned and re-contextualized into\nthe LM. Peters et al. (2019) integrate externally\npre-trained, frozen entity embeddings into BERT\u2019s\n\ufb01nal hidden layers using a word-to-entity attention\nmechanism. F\u00e9vry et al. (2020) learn the external\nentity embeddings jointly during pretraining, and\nperform the integration in BERT\u2019s earlier hidden\nlayers using an attention-weighted sum. The lat-\nter approach is competitive with a 30\u00d7 larger T5\nLM in closed-book QA (\u00a7 4.1), suggesting that\nLMs and KB embeddings can be trained jointly to\nenhance and complement each other.\nTreating entities as \u201ctokens\u201d by appending spe-\ncial reserved entity symbols to the LM\u2019s vocab-\nulary is the earliest of entity fusion approaches\n(Figure 3d). For instance, Yamada et al. (2020)\ninput entity \u201ctokens\u201d alongside textual contexts\nthat mention these entities to RoBERTa, and use\nspecialized word-to-entity and entity-to-entity at-\ntention matrices within its hidden layers. Other ap-\nproaches leave the base LM\u2019s internal architecture\ncompletely unchanged and focus only on aligning\nthe LM\u2019s word and entity embedding spaces at the\ninput level (Rosset et al., 2020; Poerner et al., 2020).\nNote, however, that this approach may signi\ufb01cantly\nenlarge the LM\u2019s vocabulary. For example, plain\nBERT\u2019s vocabulary is around 30k tokens, whereas\n Table 3: Taxonomy and representative examples of relation-level supervision in LMs, with evaluation tasks con-\nducted in the respective referenced papers. Glossary of evaluation tasks: KP\u2014knowledge probing; ET\u2014entity\ntyping; RC\u2014relation classi\ufb01cation; QA\u2014question answering; CR\u2014compositional reasoning; KC\u2014knowledge\nbase construction; TG\u2014text generation; GL\u2014the GLUE family of language tasks (Wang et al., 2019).\nRelations as...\nSupervision strategy\nRepresentative examples\nEvaluation task(s)\nKP\nET\nRC\nQA\nCR\nKC\nTG\nGL\nTemplated sentences (\u00a7 5.1)\nLexicalizing triples\n(Thorne et al., 2021; Guan et al., 2020)\n\u0013\n\u0013\n\u0013\nLexicalizing paths\n(Clark et al., 2020; Talmor et al., 2020a,b)\n\u0013\n\u0013\nLinearized sequences (\u00a7 5.2)\nTraining on triple sequences\n(Yao et al., 2019; Agarwal et al., 2021)\n\u0013\n\u0013\n\u0013\n\u0013\nInjecting triples into text\n(Liu et al., 2020)\n\u0013\nDedicated embeddings (\u00a7 5.3)\nPooling entity representations\n(Baldini Soares et al., 2019; Qin et al., 2021)\n\u0013\n\u0013\n\u0013\nEmbedding relations externally\n(Wang et al., 2021d; Daza et al., 2021)\n\u0013\n\u0013\n\u0013\n\u0013\nTreating relations as tokens\n(Bosselut et al., 2019; Hwang et al., 2021)\n\u0013\nEnglish Wikipedia has around 6 million entities.\nThis can make pretraining on a larger vocabulary\nexpensive in terms of both time and memory us-\nage (Yamada et al., 2020; Dufter et al., 2021).\n4.4\nSummary and outlook\nThe literature on entity supervision in LMs is\ngrowing rapidly.\nIn line with recent trends in\nNLP (Khashabi et al., 2020), a growing number\nof entity supervision strategies use generative mod-\nels (Roberts et al., 2020; De Cao et al., 2021),\nwhich are attractive because they allow for a high\nlevel of \ufb02exibility in output and circumvent the\nneed for classi\ufb01cation over potentially millions of\nentities. However, some studies \ufb01nd that generative\nmodels currently do not perform well beyond what\nthey have memorized from the training set (Wang\net al., 2021b; Lewis et al., 2021). These \ufb01ndings\nsuggest that storing some entity knowledge exter-\nnally (e.g., in a dense memory, F\u00e9vry et al., 2020)\nmay be more robust, for example by allowing for ef-\n\ufb01cient updates to the LM\u2019s knowledge (Verga et al.,\n2020). We believe that future work will need to\nanalyze the tradeoffs between fully-parametric and\nretrieval-based entity modeling in terms of pure\naccuracy, parameter and training ef\ufb01ciency, and\nability to generalize beyond the training set.\n5\nRelation-level supervision\nFinally, we consider methods that utilize KB triples\nor paths to supervise LMs for complex, often com-\npositional tasks like relation classi\ufb01cation, text gen-\neration, and rule-based inference. We again orga-\nnize methods in the order of less to more symbolic.\nIn this context, less symbolic approaches treat\ntriples and paths as fully natural language (\u00a7 5.1,\n5.2). By contrast, more symbolic approaches learn\ndistinct embeddings for relation types in the KB\n(\u00a7 5.3). Table 3 provides a taxonomy of this section\nwith representative examples and evaluation tasks.\n5.1\nRelations as templated assertions\nTemplate-based lexicalization is a popular relation\nsupervision strategy that does not directly expose\nthe LM to the KB. Similar to how KB queries are\nconverted to cloze prompts for knowledge prob-\ning (\u00a7 3.1), triples are \ufb01rst converted to natural\nlanguage assertions using relation templates, usu-\nally handcrafted. These assertions are then fed as\ninput to the LM, which is trained with any num-\nber of task-speci\ufb01c losses. Figure 4 provides an\ninput/output example for this class of approach.\nLexicalized triples from Wikidata have been used\nas LM training data in proof-of-concept studies\ndemonstrating that LMs can serve as natural lan-\nguage querying interfaces to KBs under controlled\nconditions (Heinzerling and Inui, 2021). A promis-\ning approach in this direction uses encoder-decoder\nLMs to generate answer sets to natural language\nqueries over lexicalized Wikidata triples (Thorne\net al., 2020, 2021), toward handling multi-answer\nKB queries with LMs\u2014thus far an understudied\ntask in the LM knowledge querying literature.\nOther approaches convert KB triples to sentences\nusing relation templates in order to construct task-\nspeci\ufb01c training datasets for improved performance\nin, e.g., story generation (Guan et al., 2020), com-\nmonsense QA (Ye et al., 2020; Ma et al., 2021),\nand relation classi\ufb01cation (Bouraoui et al., 2020).\nWhile most of these approaches rely on template\nhandcrafting, a few automatically mine templates\nusing distant supervision on Wikipedia, achieving\ncompetitive results in tasks like relation classi\ufb01-\ncation (Bouraoui et al., 2020) and commonsense\nQA (Ye et al., 2020).\nCompositional paths spanning multiple atoms of\nsymbolic knowledge may also be lexicalized and\ninput to an LM (Lauscher et al., 2020; Talmor\net al., 2020a) in order to train LMs for soft com-\n Figure 4: Strategies for representing relations as se-\nquences: Templating (\u00a7 5.1) and linearization (\u00a7 5.2).\npositional reasoning (Clark et al., 2020; Talmor\net al., 2020b). Notably, when RoBERTa is \ufb01ne-\ntuned on sentences expressing (real or synthetic)\nfacts and rules from a KB, it can answer entailment\nqueries with high accuracy (Clark et al., 2020; Tal-\nmor et al., 2020b). However, as Clark et al. (2020)\nnote, these results do not necessarily con\ufb01rm that\nLMs can \u201creason,\u201d but rather that they can at least\nemulate soft reasoning\u2014raising an open question\nabout how to develop probes and metrics to verify\nwhether LMs can actually reason compositionally.\n5.2\nLinearizing KB triples\nThe main advantage of templating is that it con-\nverts symbolic triples into sequences, which can\nbe straightforwardly input to LMs. However, hand-\ncrafting templates is a manual process, and distant\nsupervision can be noisy. To maintain the advan-\ntage of templates while avoiding the drawbacks,\ntriples can alternatively be fed to an LM by lineariz-\ning them\u2014that is, \ufb02attening the subject, relation,\nand object into an input sequence (Figure 4). With\nlinearization, relation-level supervision becomes\nas simple as feeding the linearized sequences\nto the LM and training again with task-speci\ufb01c\nlosses (Yao et al., 2019; Kim et al., 2020; Ribeiro\net al., 2020; Wang et al., 2021a) or injecting the\nsequences into the pretraining corpus (Liu et al.,\n2020). A notable recent example of the former\napproach (Agarwal et al., 2021) trains T5 on lin-\nearized Wikidata triples in order to generate fully\nnatural language versions of those triples. These\nverbalized triples are used as retrieval \u201cdocuments\u201d\nfor improved LM-based QA over traditional doc-\nument corpora; note, however, that they can also\nbe used as LM training data for other downstream\ntasks in place of handcrafted templates (\u00a7 5.1).\n5.3\nRelations as dedicated embeddings\nThe strategies discussed thus far treat KB triples\nand paths as natural language sequences. A \u201cmore\nsymbolic\u201d approach is to represent KB relation\ntypes with dedicated embeddings, and integrate\nthese embeddings into the LM using late, middle,\nor early fusion approaches. Figures 5a and 5b pro-\nvide input/output examples for late fusion, whereby\nrelation textual contexts are input to the LM, and\nrelation embeddings are constructed or integrated\nat the LM\u2019s output. Figure 5c exempli\ufb01es early fu-\nsion, whereby relations are treated as input tokens.\nContextual representations of entity mention-\nspans may be pooled at an LM\u2019s output layer to\nrepresent a relation (Wang et al., 2021c; Yu et al.,\n2020). For example, Baldini Soares et al. (2019)\nconcatenate the contextual representations of spe-\ncial entity-start markers inserted adjacent to textual\nentity mentions, and \ufb01ne-tune BERT to output sim-\nilar relation representations for statements ranging\nover the same entity pairs (Figure 5a). This ap-\nproach, which proved highly successful for relation\nclassi\ufb01cation, has been applied to the same task\nin languages beyond English (K\u00f6ksal and \u00d6zg\u00fcr,\n2020; Ananthram et al., 2020), and as an additional\nLM pretraining objective (Qin et al., 2021).\nNon-contextual relation embeddings may be\nlearned by de\ufb01ning a separate relation embedding\nmatrix with |R| rows and fusing this matrix into\nthe LM. One advantage of this approach, similar\nto methods for retrieving external entity embed-\ndings (\u00a7 4.3), is that it supports fusion at both the\nlate (Wang et al., 2021d; Daza et al., 2021) and\nmiddle (Liu et al., 2021c) stages. As an exam-\nple of the former, Wang et al. (2021d) propose\nan LM pretraining objective whereby textual de-\nscriptions of KB entities are input to and encoded\nby an LM, then combined with externally-learned\nrelation embeddings at the output using a link pre-\ndiction loss (Figure 5b). Combined with standard\nword-level language modeling objectives, this ap-\nproach enables generalization across both sentence-\nlevel tasks like relation classi\ufb01cation, and graph-\nlevel tasks like KB completion.\nTreating relations as \u201ctokens,\u201d toward early fu-\nsion of relations in LMs, is achieved by append-\ning the KB\u2019s relation types to the LM\u2019s vocabu-\nlary (Figure 5c). A notable instantiation of this\napproach is the COMET commonsense KB con-\nstruction framework (Bosselut et al., 2019; Hwang\net al., 2021; Jiang et al., 2021). Given a subject\nphrase/relation token as input, COMET \ufb01ne-tunes\nan LM to generate object phrases. COMET demon-\n (a) Late fusion\u2014pooling\n(b) Late fusion\u2014external embeddings\n(c) Early fusion\u2014relations as \u201ctokens\u201d\nFigure 5: Examples of relation supervision strategies that incorporate dedicated embeddings of relation types.\nstrates promising improvements over 400\u00d7 larger\nLMs not trained for KB construction (Hwang et al.,\n2021). However, templating (\u00a7 5.1) may yield bet-\nter results than adding special tokens to the vocab-\nulary when the COMET framework is trained and\ntested in a few-shot setting (Da et al., 2021).\n5.4\nSummary and outlook\nRelation-level supervision in LMs is exciting be-\ncause it enables a wide variety of complex NLP\ntasks (Table 3). A unifying theme across many of\nthese tasks is that of compositionality, or the idea\nthat smaller \u201cbuilding blocks\u201d of evidence can be\ncombined to arrive at novel knowledge. As compo-\nsitionality is thought to be key to machine general-\nization (Lake et al., 2017), we believe that further\nfundamental research in understanding and improv-\ning LMs\u2019 soft \u201creasoning\u201d skills (Clark et al., 2020;\nTalmor et al., 2020b, \u00a7 5.1) will be crucial.\nFinally, while most of the open directions we dis-\ncuss involve improving LM knowledge with KBs,\nwe \ufb01nd the direction of generating KBs with LMs\nequally intriguing\u2014re\ufb02ecting the fact that LMs\nand KBs can complement each other in \u201cboth direc-\ntions,\u201d as automating and scaling out the construc-\ntion of KBs will ultimately provide LMs with more\nrelational training data. The generative COMET\nframework (Bosselut et al., 2019, \u00a7 5.3) has made\ninroads in commonsense KB construction, but the\nsame progress has not yet been observed for en-\ncyclopedic knowledge. The latter entails unique\nchallenges: Whereas commonsense entities are not\ndisambiguated and triples need only be plausible\nrather than always true, encyclopedic entities are\nusually disambiguated and facts are often binary\ntrue/false. We look forward to future research that\naddresses these challenges, perhaps building on\nrecent breakthroughs in generative factual entity\nretrieval (De Cao et al., 2021, \u00a7 4.2).\n6\nConclusion and vision\nIn this review, we provide an overview of how LMs\nmay acquire relational world knowledge during\npretraining and \ufb01ne-tuning. We propose a novel\ntaxonomy that classi\ufb01es knowledge representation\nmethodologies based on the level of KB supervi-\nsion provided to an LM, from no explicit supervi-\nsion at all to entity- and relation-level supervision.\nIn the future, we envision a stronger synergy\nbetween the perspectives and tools from the lan-\nguage modeling and knowledge bases communities.\nIn particular, we expect powerful and expressive\nLMs, which are actively being developed in NLP,\nto be increasingly combined with large-scale KB\nresources to improve their knowledge recall and\nreasoning abilities. On the converse, we expect\nsuch KB resources to be increasingly generated di-\nrectly by LMs. Within both of these directions, we\nhope that future work will continue to explore the\nthemes discussed in this paper, in particular that\nof delineating and testing KB-level memorization\nversus generalization in LMs. We also expect that\nmore standardized benchmarks and tasks for evalu-\nating LM knowledge will be developed, a direction\nthat has recently seen some progress (Petroni et al.,\n2021). As research at the intersection of LMs and\nKBs is rapidly progressing, we look forward to\nnew research that better develops and combines the\nstrengths of both knowledge representations.\nAcknowledgements\nWe thank the reviewers for their thoughtful feed-\nback. This material is based upon work supported\nby the National Science Foundation under a Grad-\nuate Research Fellowship and CAREER Grant\nNo. IIS 1845491, the Advanced Machine Learning\nCollaborative Grant from Procter & Gamble, and\nan Amazon faculty award.\n References\nOshin Agarwal, Heming Ge, Siamak Shakeri, and\nRami Al-Rfou. 2021. Large scale knowledge graph\nbased synthetic corpus generation for knowledge-\nenhanced language model pre-training. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics.\nAmith Ananthram, Emily Allaway, and Kathleen McK-\neown. 2020.\nEvent-guided denoising for multilin-\ngual relation learning. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 1505\u20131512, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching the\nblanks: Distributional similarity for relation learn-\ning.\nIn Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 2895\u20132905, Florence, Italy. Association for\nComputational Linguistics.\nMichele Banko and Oren Etzioni. 2008. The tradeoffs\nbetween open and traditional relation extraction. In\nProceedings of ACL-08: HLT, pages 28\u201336, Colum-\nbus, Ohio. Association for Computational Linguis-\ntics.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008.\nFreebase: a col-\nlaboratively created graph database for structuring\nhuman knowledge. In SIGMOD, pages 1247\u20131250.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4762\u20134779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nJan A. Botha, Zifei Shan, and Daniel Gillick. 2020. En-\ntity Linking in 100 Languages. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 7833\u2013\n7845, Online. Association for Computational Lin-\nguistics.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020.\nInducing relational knowledge\nfrom bert. In Proceedings of the AAAI Conference\non Arti\ufb01cial Intelligence, volume 34, pages 7456\u2013\n7463.\nSamuel Broscheit. 2019.\nInvestigating entity knowl-\nedge in BERT with simple neural end-to-end en-\ntity linking.\nIn Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 677\u2013685, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners.\nIn Advances in Neural Information Pro-\ncessing Systems 33 pre-proceedings.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases.\nIn Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1860\u20131874,\nOnline. Association for Computational Linguistics.\nHaotian Chen, Xi Li, Andrej Zukov Gregoric, and Sahil\nWadhwa. 2020.\nContextualized end-to-end neural\nentity linking.\nIn Proceedings of the 1st Confer-\nence of the Asia-Paci\ufb01c Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 637\u2013642, Suzhou, China. Associa-\ntion for Computational Linguistics.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\n2020. Transformers as soft reasoners over language.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Arti\ufb01cial Intelligence, pages\n3882\u20133890.\nPedro Colon-Hernandez,\nCatherine Havasi,\nJason\nAlonso, Matthew Huggins, and Cynthia Breazeal.\n2021.\nCombining pre-trained language mod-\nels and structured knowledge.\narXiv preprint\narXiv:2101.12294.\nJeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and\nAntoine Bosselut. 2021.\nUnderstanding few-shot\ncommonsense knowledge models.\narXiv preprint\narXiv:2101.00297.\nJeff Da and Jungo Kasai. 2019. Cracking the contex-\ntual commonsense code: Understanding common-\nsense reasoning aptitude of deep contextual repre-\nsentations. In Proceedings of the First Workshop on\nCommonsense Inference in Natural Language Pro-\ncessing, pages 1\u201312, Hong Kong, China. Associa-\ntion for Computational Linguistics.\nRandall Davis, Howard Shrobe, and Peter Szolovits.\n1993. What is a knowledge representation? AI mag-\nazine, 14(1):17\u201317.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models.\nIn Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173\u20131178, Hong Kong, China. As-\nsociation for Computational Linguistics.\n Daniel Daza, Michael Cochez, and Paul Groth. 2021.\nInductive entity representations from text via link\nprediction. In Proceedings of the Web Conference\n2021, pages 798\u2013808.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn International Conference on Learning Represen-\ntations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nPhilipp Dufter, Nora Kassner, and Hinrich Sch\u00fctze.\n2021.\nStatic embeddings as ef\ufb01cient knowledge\nbases?\nIn Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics, Online. Association for\nComputational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze,\nand Yoav Goldberg. 2021.\nMeasuring and im-\nproving consistency in pretrained language models.\narXiv preprint arXiv:2102.01017.\nAnthony Fader, Stephen Soderland, and Oren Etzioni.\n2011. Identifying relations for open information ex-\ntraction. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1535\u20131545, Edinburgh, Scotland, UK. Associ-\nation for Computational Linguistics.\nThibault F\u00e9vry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4937\u20134951, Online. Associa-\ntion for Computational Linguistics.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93\u2013108.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Mingwei Chang. 2020.\nRetrieval aug-\nmented language model pre-training.\nIn Inter-\nnational Conference on Machine Learning, pages\n3929\u20133938. PMLR.\nAlon Y Halevy, Oren Etzioni, AnHai Doan, Zachary G\nIves, Jayant Madhavan, Luke K McDowell, and Igor\nTatarinov. 2003. Crossing the structure chasm. In\nConference on Innovative Data Systems Research.\nAdi Haviv, Jonathan Berant, and Amir Globerson.\n2021. BERTese: Learning to speak to BERT. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 3618\u20133623, Online.\nAssociation for Computational Linguistics.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. 2020.\nBERT-\nMK: Integrating graph contextualized knowledge\ninto pre-trained language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 2281\u20132290, Online. Association for\nComputational Linguistics.\nBenjamin Heinzerling and Kentaro Inui. 2021.\nLan-\nguage models as knowledge bases:\nOn entity\nrepresentations, storage capacity, and paraphrased\nqueries. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics: Main Volume, pages 1772\u2013\n1791, Online. Association for Computational Lin-\nguistics.\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2021. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. In Pro-\nceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence.\nFilip Ilievski, Pedro Szekely, and Bin Zhang. 2021.\nCskg: The commonsense knowledge graph. Euro-\npean Semantic Web Conference.\nLiwei Jiang, Antoine Bosselut, Chandra Bhagavatula,\nand Yejin Choi. 2021.\n\"i\u2019m not mad\": Common-\nsense implications of negation and contradiction. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics, Online. Association for Computa-\ntional Linguistics.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a.\nX-\nFACTR: Multilingual factual knowledge retrieval\nfrom pretrained language models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5943\u2013\n5959, Online. Association for Computational Lin-\nguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423\u2013438.\nNora Kassner, Philipp Dufter, and Hinrich Sch\u00fctze.\n2021. Multilingual lama: Investigating knowledge\nin multilingual pretrained language models. In The\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics.\n Nora Kassner, Benno Krojer, and Hinrich Sch\u00fctze.\n2020.\nAre pretrained language models symbolic\nreasoners over knowledge?\nIn Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 552\u2013564, Online. Associa-\ntion for Computational Linguistics.\nNora Kassner and Hinrich Sch\u00fctze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot \ufb02y. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811\u20137818, Online. As-\nsociation for Computational Linguistics.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 1896\u20131907, Online. As-\nsociation for Computational Linguistics.\nBosung Kim, Taesuk Hong, Youngjoong Ko, and\nJungyun Seo. 2020. Multi-task learning for knowl-\nedge graph completion with pre-trained language\nmodels.\nIn Proceedings of the 28th International\nConference on Computational Linguistics, pages\n1737\u20131743, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nAbdullatif K\u00f6ksal and Arzucan \u00d6zg\u00fcr. 2020.\nThe\nRELX dataset and matching the multilingual blanks\nfor cross-lingual relation classi\ufb01cation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 340\u2013350, Online. Association\nfor Computational Linguistics.\nNikolaos\nKolitsas,\nOctavian-Eugen\nGanea,\nand\nThomas Hofmann. 2018. End-to-end neural entity\nlinking.\nIn Proceedings of the 22nd Conference\non Computational Natural Language Learning,\npages 519\u2013529, Brussels, Belgium. Association for\nComputational Linguistics.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenen-\nbaum, and Samuel J Gershman. 2017. Building ma-\nchines that learn and think like people. Behavioral\nand brain sciences, 40.\nAnne Lauscher, Olga Majewska, Leonardo F. R.\nRibeiro, Iryna Gurevych, Nikolai Rozanov, and\nGoran Glava\u0161. 2020.\nCommon sense or world\nknowledge? investigating adapter-based knowledge\ninjection into pretrained transformers. In Proceed-\nings of Deep Learning Inside Out (DeeLIO): The\nFirst Workshop on Knowledge Extraction and Inte-\ngration for Deep Learning Architectures, pages 43\u2013\n49, Online. Association for Computational Linguis-\ntics.\nDouglas B Lenat. 1995. Cyc: A large-scale investment\nin knowledge infrastructure. Communications of the\nACM, 38(11):33\u201338.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871\u20137880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2021.\nQuestion and answer test-train overlap in\nopen-domain question answering datasets. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1000\u20131008, Online.\nAssociation for Computational Linguistics.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of\nPre-Trained Language Models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 6862\u2013\n6868, Online. Association for Computational Lin-\nguistics.\nJeffrey\nLing,\nNicholas\nFitzGerald,\nZifei\nShan,\nLivio Baldini Soares, Thibault F\u00e9vry, David Weiss,\nand Tom Kwiatkowski. 2020.\nLearning cross-\ncontext entity representations from text.\narXiv\npreprint arXiv:2001.03765.\nHugo Liu and Push Singh. 2004. Conceptnet\u2014a practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211\u2013226.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020.\nK-bert:\nEnabling language representation with knowledge\ngraph. In Proceedings of the AAAI Conference on\nArti\ufb01cial Intelligence, volume 34, pages 2901\u20132908.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S\nYu. 2021c. Kg-bart: Knowledge graph-augmented\nbart for generative commonsense reasoning. In Pro-\nceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova, Jacob Devlin, and Honglak Lee.\n2019. Zero-shot entity linking by reading entity de-\nscriptions. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3449\u20133460, Florence, Italy. Association for\nComputational Linguistics.\nKaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan\nBisk, Eric Nyberg, and Alessandro Oltramari. 2021.\nKnowledge-driven data construction for zero-shot\nevaluation in commonsense question answering. In\nProceedings of the AAAI Conference on Arti\ufb01cial In-\ntelligence.\nMarvin Minsky. 1974. A framework for representing\nknowledge.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. arXiv\npreprint arXiv:2105.11447.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations.\nIn Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43\u201354, Hong Kong, China. Associ-\nation for Computational Linguistics.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRockt\u00e4schel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020.\nHow context affects lan-\nguage models\u2019 factual predictions.\nIn Automated\nKnowledge Base Construction.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and\nSebastian Riedel. 2021.\nKILT: a benchmark for\nknowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2523\u20132544,\nOnline. Association for Computational Linguistics.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases?\nIn Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463\u20132473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Sch\u00fctze.\n2020.\nE-BERT: Ef\ufb01cient-yet-effective entity em-\nbeddings for BERT.\nIn Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020,\npages 803\u2013818, Online. Association for Computa-\ntional Linguistics.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203\u20135212, Online. Association for Compu-\ntational Linguistics.\nYujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu,\nPeng Li, Heng Ji, Minlie Huang, Maosong Sun, and\nJie Zhou. 2021. ERICA: Improving entity and re-\nlation understanding for pre-trained language mod-\nels via contrastive learning. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3350\u20133363, Online. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uni\ufb01ed text-to-text trans-\nformer.\nJournal of Machine Learning Research,\n21:1\u201367.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Sch\u00fctze,\nand Iryna Gurevych. 2020. Investigating pretrained\nlanguage models for graph-to-text generation. arXiv\npreprint arXiv:2007.08426.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model?\nIn Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418\u20135426,\nOnline. Association for Computational Linguistics.\nCorby Rosset, Chenyan Xiong, Minh Phan, Xia\nSong, Paul Bennett, and Saurabh Tiwary. 2020.\nKnowledge-aware\nlanguage\nmodel\npretraining.\narXiv preprint arXiv:2007.00655.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning.\nIn Proceedings of the AAAI Con-\nference on Arti\ufb01cial Intelligence, volume 33, pages\n3027\u20133035.\nTao Shen, Yi Mao, Pengcheng He, Guodong Long,\nAdam Trischler, and Weizhu Chen. 2020. Exploit-\ning structured knowledge in text via graph-guided\nrepresentation learning. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 8980\u20138994, On-\nline. Association for Computational Linguistics.\n Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts.\nIn Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4222\u20134235, Online. Association for Computational\nLinguistics.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4615\u20134629, Online. Association for Computa-\ntional Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI Confer-\nence on Arti\ufb01cial Intelligence, volume 31.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. In Proceedings of the AAAI Conference\non Arti\ufb01cial Intelligence, volume 34, pages 8968\u2013\n8975.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020a. olmpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743\u2013758.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020b. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge.\nIn 34th Conference\non Neural Information Processing Systems.\nAlexandre Tamborrino, Nicola Pellican\u00f2, Baptiste Pan-\nnier, Pascal Voitot, and Louise Naudin. 2020. Pre-\ntraining is (almost) all you need: An application\nto commonsense reasoning. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3878\u20133887, Online. As-\nsociation for Computational Linguistics.\nWilson L Taylor. 1953.\n\u201ccloze procedure\u201d: A new\ntool for measuring readability.\nJournalism quar-\nterly, 30(4):415\u2013433.\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fab-\nrizio Silvestri, Sebastian Riedel, and Alon Halevy.\n2020. From natural language processing to neural\ndatabases.\nProceedings of the VLDB Endowment,\n14(6):1033\u20131039.\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fab-\nrizio Silvestri, Sebastian Riedel, and Alon Halevy.\n2021.\nDatabase reasoning over text.\nIn Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3091\u20133104,\nOnline. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000\u20136010.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam W Cohen. 2020. Facts as experts: Adapt-\nable and interpretable neural memory over symbolic\nknowledge. arXiv preprint arXiv:2007.00849.\nDenny Vrande\u02c7ci\u00b4c and Markus Kr\u00f6tzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78\u201385.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding.\nIn 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou,\nYing Wang, and Yi Chang. 2021a.\nStructure-\naugmented text representation learning for ef\ufb01cient\nknowledge graph completion. In Proceedings of the\nWeb Conference 2021, pages 1737\u20131748.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021b.\nCan generative pre-trained language models serve as\nknowledge bases for closed-book QA? In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3241\u20133251,\nOnline. Association for Computational Linguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021c. K-adapter: Infusing\nknowledge into pre-trained models with adapters. In\nFindings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021, volume ACL/IJCNLP 2021 of Findings\nof ACL, pages 1405\u20131418. Association for Compu-\ntational Linguistics.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021d. KEPLER: A uni\ufb01ed model for knowledge\nembedding and pre-trained language representation.\nTrans. Assoc. Comput. Linguistics, 9:176\u2013194.\nGerhard Weikum, Xin Luna Dong, Simon Razniewski,\nand Fabian M. Suchanek. 2021.\nMachine knowl-\nedge:\nCreation and curation of comprehensive\nknowledge bases. Found. Trends Databases, 10(2-\n4):108\u2013490.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2020. Scalable zero-\nshot entity linking with dense entity retrieval.\nIn\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6397\u20136407, Online. Association for Computa-\ntional Linguistics.\n Wenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020.\nPretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel.\nIn International Conference on Learning\nRepresentations.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442\u20136454, On-\nline. Association for Computational Linguistics.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg-\nbert: Bert for knowledge graph completion. arXiv\npreprint arXiv:1909.03193.\nZhi-Xiu Ye, Qian Chen, Wen Wang, and Zhen-Hua\nLing. 2020.\nAlign, mask and select: A simple\nmethod for incorporating commonsense knowledge\ninto language representation models. arXiv preprint\narXiv:1908.06725.\nDonghan Yu, Chenguang Zhu, Yiming Yang, and\nMichael Zeng. 2020.\nJaket:\nJoint pre-training\nof knowledge graph and language understanding.\narXiv preprint arXiv:2010.00796.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019.\nERNIE: En-\nhanced language representation with informative en-\ntities.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441\u20131451, Florence, Italy. Association\nfor Computational Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017\u20135033, Online. Association for\nComputational Linguistics.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020.\nEvaluating commonsense in pre-\ntrained language models.\nIn Proceedings of the\nAAAI Conference on Arti\ufb01cial Intelligence, vol-\nume 34, pages 9733\u20139740.\n"}, "Applied Machine Learning for Games: A Graduate School Course": {"authors": ["Yilei Zeng", "Aayush Shah", "Jameson Thai", "Michael Zyda"], "title": "Applied Machine Learning for Games: A Graduate School Course", "url": "https://arxiv.org/pdf/2012.01148.pdf", "abstract": "The game industry is moving into an era where old-style game engines are being replaced by re-engineered systems with embedded machine learning technologies for the operation, analysis and understanding of game play. In this paper, we describe our machine learning course designed for graduate students interested in applying recent advances of deep learning and reinforcement learning towards gaming. This course serves as a bridge to foster interdisciplinary collaboration among graduate schools and does not require prior experience designing or building games. Graduate students enrolled in this course apply different fields of machine learning techniques such as computer vision, natural language processing, computer graphics, human computer interaction, robotics and data analysis to solve open challenges in gaming. Student projects cover use-cases such as training AI-bots in gaming benchmark environments and competitions, understanding human decision patterns in gaming, and creating intelligent non-playable characters or environments to foster engaging gameplay. Projects demos can help students open doors for an industry career, aim for publications, or lay the foundations of a future product. Our students gained hands-on experience in applying state of the art machine learning techniques to solve real-life problems in gaming.", "arxiv_id": "2012.01148", "published_date": "2020-11-30", "year": 2020, "introduction": "Introduction Applied machine learning in games is now a vividly expanding research \ufb01eld that provides a platform for novel vision, language, robotics, and online social interaction algorithms. Exposure to state-of-the-art research literature is an integral part of the course plan, in part because research community is moving forward at an ever-increasing speed and understanding several backbone papers will clarify the research question and enhance an understanding of the iterations and improvements made. Moreover, an emphasis on the state-ofthe-art research methods fosters an appreciation of research design and methodology, and more generally, of the importance of critical evaluation. Therefore, new ideas can be generated based on critical thinking. As this course does not require prerequisites on machine learning, we encourage learning by doing. A self-proposed project will enable the students to tailor themselves into Copyright \u00a9 2021, Association for the Advancement of Arti\ufb01cial Intelligence (www.aaai.org). All rights reserved. research-oriented, industry-oriented or patent-oriented directions. The projects\u2019 dif\ufb01culties are also dynamically adjustable towards different students\u2019 learning curve or prior experiences in machine learning. In this class, we intend to encourage further research into different gaming areas by requiring students to work on a semester-long research project in groups of up to 8. Students work on incorporating deep learning and reinforcement learning techniques in different aspects of game-play creation, simulation, or spectating. These projects are completely driven by the student along any direction they wish to explore. Giving the students an intrinsic motivation to engage on their favorite ideas will not only make teaching more time ef\ufb01cient but also bestow a long-term meaning to the course project which will open doors for them. By having a semester-long project, students can dive deep into different algorithms. They also receive hands-on experience incorporating various machine learning algorithms for their use case. Writing, presenting and teamwork pro\ufb01ciency is a critical component of a higher education, and this courses involve writing assignments, extensive team collaboration and oral presentation to a public audience. Student performance on formal writing assignments, project actualization and public presentation provides benchmarks for examining student progress, both within and across semesters. This experience report describes a three semester-long effort in an applied machine learning course with advanced research orientations in gaming. This course has withstood the test through in-person, hybrid learning, and completely online modalities separately. We contribute a new course design inline with the most recent advancements in the gaming research community. This course attracts and caters to mutual interests across engineering graduate programs. Of the 292 students enrolled in this course over 3 semesters; 1.3% major in Environmental Engineering, Physics , Chemistry or Computer Networks, 1.3% are Software Engineering or High-Performance Computing, 2% are Game Development, 3.2% are Electrical Engineering, 4% are Intelligent Robotics, 7% are Computer Engineering, 9% are Applied Data Science, 9.2% are Data Science and the majority of students, 63%, are majored in General Computer Science. Students are expected to gain both creative and fun handson experience through a semester-long applied deep learning and reinforcement learning project. This course demonarXiv:2012.01148v2  [cs.CY]  1 Jan 2021 ", "conclusion": "Conclusion Table 1 shows our survey results to evaluate the course. A majority of students gave high ratings for recommending this course to other students, the usefulness of this course for \ufb01nding an internship or a full-time job, and learning from team projects to get applied machine learning hands-on experiences. The survey results indicate positive feedback for the course. From a teaching perspective, we encountered three challenges: At what level should we balance theoretical deep learning and reinforcement learning lecture materials and applied environment demonstrations; How to create an adaptive learning curve for students with varying machine learning backgrounds; and how to form an innovative research pipeline at the graduate school level to facilitate publications. Throughout the three semesters, we learned that more visual aids, such as live demonstrations and videos, are needed to increase online engagement as we move into online virtual courses. Weekly project demonstrations in front of the whole class will create a healthy peer effect that increases learning ef\ufb01cacy. Within three semesters, three research conference papers have been published, and more in preparation. From the students\u2019 self-proposed projects, we strengthened our belief that gaming as an interdisciplinary research domain can reach other \ufb01elds, such as robotics, medical diagnosis, human-computer interactions, etc. Games are the testbeds for advancing state-of-the-arts learning algorithms. In the future, the class can bene\ufb01t from state-of-the-arts paper reading sessions and live coding demonstrations to help graduate students build a comprehensive understanding of how a research project is built. This report summarizes the design of our applied machine learning course for graduate students interested in applying deep learning and reinforcement learning advancements towards gaming. We familiarize students with the current research landscape and improve students\u2019 oral and written presentation skills through practical team projects, regardless of the major and machine learning expertise level. Our course can help students open doors for an industry career, aim for publications, or lay the foundations of future products. ", "full_text": "Applied Machine Learning for Games: A Graduate School Course\nYilei Zeng, Aayush Shah, Jameson Thai, Michael Zyda\nUniversity of Southern California\n{yilei.zeng, aayushsh, jamesont, zyda}@usc.edu\nAbstract\nThe game industry is moving into an era where old-style\ngame engines are being replaced by re-engineered systems\nwith embedded machine learning technologies for the opera-\ntion, analysis and understanding of game play. In this paper,\nwe describe our machine learning course designed for gradu-\nate students interested in applying recent advances of deep\nlearning and reinforcement learning towards gaming. This\ncourse serves as a bridge to foster interdisciplinary collab-\noration among graduate schools and does not require prior\nexperience designing or building games. Graduate students\nenrolled in this course apply different \ufb01elds of machine learn-\ning techniques such as computer vision, natural language\nprocessing, computer graphics, human computer interaction,\nrobotics and data analysis to solve open challenges in gam-\ning. Student projects cover use-cases such as training AI-bots\nin gaming benchmark environments and competitions, under-\nstanding human decision patterns in gaming, and creating in-\ntelligent non-playable characters or environments to foster\nengaging gameplay. Projects demos can help students open\ndoors for an industry career, aim for publications, or lay the\nfoundations of a future product. Our students gained hands-\non experience in applying state of the art machine learning\ntechniques to solve real-life problems in gaming.\nIntroduction\nApplied machine learning in games is now a vividly expand-\ning research \ufb01eld that provides a platform for novel vision,\nlanguage, robotics, and online social interaction algorithms.\nExposure to state-of-the-art research literature is an integral\npart of the course plan, in part because research community\nis moving forward at an ever-increasing speed and under-\nstanding several backbone papers will clarify the research\nquestion and enhance an understanding of the iterations and\nimprovements made. Moreover, an emphasis on the state-of-\nthe-art research methods fosters an appreciation of research\ndesign and methodology, and more generally, of the impor-\ntance of critical evaluation. Therefore, new ideas can be gen-\nerated based on critical thinking.\nAs this course does not require prerequisites on machine\nlearning, we encourage learning by doing. A self-proposed\nproject will enable the students to tailor themselves into\nCopyright \u00a9 2021, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.\nresearch-oriented, industry-oriented or patent-oriented di-\nrections. The projects\u2019 dif\ufb01culties are also dynamically ad-\njustable towards different students\u2019 learning curve or prior\nexperiences in machine learning. In this class, we intend\nto encourage further research into different gaming areas\nby requiring students to work on a semester-long research\nproject in groups of up to 8. Students work on incorporat-\ning deep learning and reinforcement learning techniques in\ndifferent aspects of game-play creation, simulation, or spec-\ntating. These projects are completely driven by the student\nalong any direction they wish to explore. Giving the students\nan intrinsic motivation to engage on their favorite ideas will\nnot only make teaching more time ef\ufb01cient but also bestow\na long-term meaning to the course project which will open\ndoors for them. By having a semester-long project, students\ncan dive deep into different algorithms. They also receive\nhands-on experience incorporating various machine learn-\ning algorithms for their use case.\nWriting, presenting and teamwork pro\ufb01ciency is a critical\ncomponent of a higher education, and this courses involve\nwriting assignments, extensive team collaboration and oral\npresentation to a public audience. Student performance on\nformal writing assignments, project actualization and pub-\nlic presentation provides benchmarks for examining student\nprogress, both within and across semesters.\nThis experience report describes a three semester-long ef-\nfort in an applied machine learning course with advanced\nresearch orientations in gaming. This course has withstood\nthe test through in-person, hybrid learning, and completely\nonline modalities separately. We contribute a new course de-\nsign inline with the most recent advancements in the gam-\ning research community. This course attracts and caters to\nmutual interests across engineering graduate programs. Of\nthe 292 students enrolled in this course over 3 semesters;\n1.3% major in Environmental Engineering, Physics , Chem-\nistry or Computer Networks, 1.3% are Software Engineer-\ning or High-Performance Computing, 2% are Game Devel-\nopment, 3.2% are Electrical Engineering, 4% are Intelligent\nRobotics, 7% are Computer Engineering, 9% are Applied\nData Science, 9.2% are Data Science and the majority of\nstudents, 63%, are majored in General Computer Science.\nStudents are expected to gain both creative and fun hands-\non experience through a semester-long applied deep learn-\ning and reinforcement learning project. This course demon-\narXiv:2012.01148v2  [cs.CY]  1 Jan 2021\n strates the feasibility of teaching and conducting state-of-\nthe-art applied machine learning research within mixed fo-\ncused engineering graduate students. This course also shows\nthe capability to help students open doors for an industry ca-\nreer, aim for publications, or lay the foundations of a future\nproduct.\nBackground\nAiming for approaching Arti\ufb01cial General Intelligence\n(AGI), video games such as Atari, Doom, Minecraft, Dota\n21, StarCraft, and driving games have been used exten-\nsively to test the deep learning and reinforcement learn-\ning methods\u2019 performance and generalizability. Following\nGoogle\u2019s Alpha Go (Silver et al. 2016), researchers have\nmade steady progress in improving AI\u2019s game playing capa-\nbilities. Besides creating intelligent Non-player characters\n(NPC), game testing and level generation have also seen\nadvancement with deep learning for the gaming industry.\nMoreover, Machine learning can unleash the power of data\ngenerated from millions of players worldwide. Gaming pro-\nvides numerous behavioral data for online user pro\ufb01ling, ad-\nvertisement recommendation, modeling social interactions,\nand understanding decision-making strategies. Apart from\nin-game trajectories, Esports and streaming open new re-\nsearch opportunities for multi-modal machine learning that\ncombines textual, audio natural language processing, com-\nputer vision with social media. Gaming simulated interactive\nenvironments can extend beyond gaming and adopt practical\nvalues for robotics, health, and broader social good.\nWe cover all the following topics in our course. The cited\nwork also serve as supplementary reading materials. And\nthese topics will be exempli\ufb01ed in the Student Projects sec-\ntion.\nBenchmark Environments and Competitions\nFor academic and individual researchers, the IEEE Con-\nference on Games(COG), AAAI Conference on Arti\ufb01cial\nIntelligence and Interactive Digital Entertainment(AIIDE),\nConference on the Foundations of Digital Games (FDG),\nand Conference on Neural Information Processing Systems\n(NeurIPS) host a series of annual competitions featuring cre-\nating deep learning and reinforcement learning algorithms\nfor game-play generation or AI playing games.\nMajor technology companies open-sourced a number of\ngaming AI environments to help push forward the bound-\naries of Arti\ufb01cial general intelligence (AGI). OpenAI re-\nleases for public OpenAI Gym\n(Brockman et al. 2016),\nwhich incorporates Arcade Learning Environment (ALE)\nthat emulates Atari 2600 game-playing (Bellemare et al.\n2013), robotics, and expanding third party environments.\nGym Retro (Nichol et al. 2018) extends the integration to\n1000 retro games, including games from the Sega Gene-\nsis and Sega Master System, and Nintendo\u2019s NES, SNES,\nand Game Boy consoles. Facebook AI has released ELF:\nAn Extensive, Lightweight, and Flexible Platform for Game\n1OpenAI Five: https://openai.com/blog/openai-\ufb01ve/ (Last ac-\ncessed: 12/15/2020)\nResearch (Tian et al. 2017), which provides three environ-\nments, i.e., MiniRTS, Capture the Flag, and Tower Defense.\nPySC2 is DeepMind\u2019s Python component of the StarCraft\nII Learning Environment (SC2LE)\n(Vinyals et al. 2017).\nSTARDATA (Lin et al. 2017), a StarCraft: Brood War re-\nplay dataset, is published with the StarCraft II API. Mi-\ncrosoft announced Project Malmo\n(Johnson et al. 2016),\nwhich provides an open-source platform built on top of\nMinecraft. MineRL Environments built on Malmo are re-\nleased for NeurIPS competitions and MineRL imitation\nlearning datasets (Johnson et al. 2016) with over 60 million\nframes of recorded human player data are published to facil-\nitate research. The Unity Machine Learning Agents Toolkit\n(ML-Agents) (Juliani et al. 2018) is an open-source project\nthat enables games and simulations created by individuals\nto serve as environments for training intelligent agents. As\nan active research \ufb01eld, new environments and tasks emerge\ndaily. We leave the constant learning to students as they\nprogress through their projects.\nComputer Vision & Natural Language Processing\nLearning to play from pixels have become a widely accepted\napproach for traning AI agents after DeepMinds paper of\nplaying Atari with Deep Reinforcement Learning\n(Mnih\net al. 2013) using raw pixels as input. Vision-based user in-\nputs augmented automatic face, and gesture recognition has\nenabled the \ufb01tness game genre to boost. With the pandemic\nin 2020, virtual reality devices and \ufb01tness gaming has of-\nfered a safe and entertaining indoor option. With the boom-\ning of streaming platforms, elaborate walk-through, strate-\ngies, and sentiments shared via videos provided a wealth of\ndata for applied computer vision tasks such as motion analy-\nsis and activity recognition. Leveraging the information pro-\nvided in the YouTube videos, researchers can guide deep\nreinforcement learning explorations for games with sparse\nrewards (Aytar et al. 2018).\nUnderstanding players\u2019 textual interactions, both in-game\nand on social networks, is crucial for gaming companies\nto prevent toxicity and increase inclusion. In gaming, lan-\nguage generation techniques are leveraged to generate narra-\ntives for interactive and creative storytelling. Text adventure\ngames is an active task for reinforcement learning (RL) fo-\ncused Natural Language Processing (NLP) researchers. Mi-\ncrosoft introduced TextWorld\n(C\u02c6ot\u00b4e et al. 2018), a text-\nbased game generator, as send box learning environment for\ntraining and testing RL agents.\nRecent progress on deep representations on both com-\nputer vision and natural language processing have enabled\nthe exploration on issues of active perception, long-term\nplanning, learning from interaction, and holding a dialog\ngrounded in an simulated environment. Simulated housing\nenvironments such as the ALFRED (Action Learning From\nRealistic Environments and Directives)\n(Shridhar et al.\n2020) project in Allen Institute and Habitat Lab from Face-\nbook research (Savva et al. 2019), serve for embodied AI\ntasks (e.g. navigation, instruction following, question an-\nswering), con\ufb01guring embodied agents (physical form, sen-\nsors, capabilities), training these agents (via imitation or re-\ninforcement learning), and benchmarking their performance\n on the de\ufb01ned tasks using standard metrics. AI and language\ninstructed MiniRTS project (Hu et al. 2019) from Facebook\nAI is similar to this initiative.\nPlayer Modeling and Human AI Interactions\nSocial gaming, such as the Battle-Royale genre and Animal\nCrossing, has gained increasing popularity. Combined with\nheterogeneous data provided on social media and streaming\nplatforms, understanding and predicting players\u2019 behavior\npatterns considering graph structures becomes increasingly\nimportant. The data provided by major AAA games will of-\nfer resources to imitating and modeling human behaviors\n(Sapienza et al. 2018; Zeng 2020) and facilitate understand-\ning of human collaborations (Zeng, Sapienza, and Ferrara\n2019).\nGaming industry with exuberant data of in-game human\ncollaborations makes suitable sand-box environments for\nconducting multi-agent interaction/collaboration research.\nFor instance, multi-agent Hide-and-Seek\n(Baker et al.\n2019), OpenAI Five\n(Berner et al. 2019), AlphaStar\n(Vinyals et al. 2019), Hanabi (Bard et al. 2020) and capture\nthe \ufb02ag (Jaderberg et al. 2019) are some initial attempts.\nWith detailed human behavior trajectory recorded as\nreplays or demos, gaming environments provide data-\nintensive sources for human-computer interaction research.\nRecent advancements of AI in games has evolved human-\ncomputer interactions in gaming environments into human\nbots interactions. As suggested in paper (Risi and Preuss\n2020), with the increasing popularity in human/AI interac-\ntions, we will see more research on human-like NPC and\nhuman-AI collaboration in the future.\nProcedural Content Generation\nProcedural Content Generation via Machine Learning (ab-\nbreviated PCGML)\n(Summerville et al. 2018) embraces\na broadening scope, incorporating automatic generation\nof levels, gaming environments, characters, stories, music,\neven game-play mechanics. In the future, more reliable and\nexplainable machine learning algorithms will emerge in this\ndirection.\nSimulated Interactive Environments and beyond\nPlaytesting, matchmaking, dynamic dif\ufb01culty adaptation\n(DDA) are some other important tasks for gaming industry\nto solve using machine learning.\nBeyond gaming, interactive environments are used to\nmimic real-life scenes such as training robots or autonomous\nvehicles. Interactive gaming environments can also serve as\ndemonstrations for game theory decision makings that serve\nAI for social good initiatives.\nCourse Design\nThe semester-long course comprises 15 lectures. The de-\ntailed course structure consists of weekly lectures on deep\nlearning and reinforcement learning fundamentals, project\ndemonstrations of how each technique are applied in gaming\nuse cases and openly available tools or environments. Upon\nthe conclusion of the lecture, each team updates their weekly\nprogress to the course instructors. Every alternate week stu-\ndents conduct a power-point presentation along with a demo\non their team\u2019s progress to the entire class. We encourage\nthe students to be prepared with questions before class to\nlearn proactively rather than learning passively. The instruc-\ntor evaluates the progress and provides either algorithmic\nsuggestions or structural suggestions to facilitate their learn-\ning and project formulation every week.\nWe host the midterm and \ufb01nal on the 8th and 15th week.\nEach team will present PowerPoint and live or recorded de-\nmos on their project on both midterm and \ufb01nal. We will\nalso collect the Engineering Design Document (EDD) and\na technical paper draft on both midterm and \ufb01nal to foster\ncontinuous contribution. We require each team to construct\na website to present their project demos to help them on the\njob market. The gradings\u2019 weights are 20% for mid-term\nEDD, 20% for the mid-term draft of technical paper, 10%\nfor midterm presentation, 20% for \ufb01nal EDD, 20% for \ufb01nal\nof technical paper, 10% for \ufb01nal presentation.\nThe learning objective for the course: (1) Students learn\ndeep learning and reinforcement learning fundamentals\nthrough lectures and supplemental materials; (2) Students\nlearn the most recent advancements, landscape, and applied\nuse cases of machine learning for gaming; (3) Students can\nunleash their creativity in projects that cater to their career\nplans; (4) Students engage in teamwork and practice both\noral and written presentation skills.\nThe course \ufb01rst introduces students to the difference be-\ntween Arti\ufb01cial Intelligence, Machine Learning, and Deep\nLearning (Ongsulee 2017). We then cover the survey of\nDeep learning applications in games (Justesen et al. 2019)\nto give students a tentative idea on projects they can pursue.\nFollowing the lecture, students must select a machine learn-\ning project and the game they will work on. The course in-\nstructors will guide and instruct students\u2019 projects according\nto the sub-directions shown in backgrounds, i.e., benchmark\nenvironments and competitions, computer vision and natural\nlanguage processing, player modeling and human-AI inter-\nactions, procedural content generation, simulated interactive\nenvironments, etc.\nApart from building a new research project from scratch,\nstudents can choose to advance on projects created in the\nprevious semesters for better algorithmic AI performances.\nIn the \ufb01rst half of the course, we introduce the funda-\nmentals of deep learning. We start with the concept of back-\npropagation (Hecht-Nielsen 1992), along with gradient de-\nscent (Baldi 1995) is covered to solidify student\u2019s theoret-\nical understanding of Neural Networks. The different acti-\nvation functions covered include the sigmoid, tanh (LeCun\net al. 2012) and ReLu (Nair and Hinton 2010) functions.\nWe cover a tutorial on combining Neural Networks with\nGenetic Algorithms in a simulated game environment for\nFlappy Bird. Students are then introduced to popular Deep\nLearning frameworks like Tensor\ufb02ow and Pytorch.\nWe then move onto Convolutional Neural Networks\n(CNNs). Students are introduced to the convolution layer,\npooling layer, and fully connected layer along with their re-\nspective functionalities. We also cover appropriate activation\nfunctions and loss functions for CNNs. A brief overview of\n state-of-art deep CNN based architectures for object detec-\ntion tasks are given to students. These include R-CNN (Gir-\nshick et al. 2014), Fast R-CNN (Girshick 2015), Faster R-\nCNN (Ren et al. 2015) and YOLO (Redmon et al. 2016;\nRedmon and Farhadi 2017, 2018). We cover a sample pro-\ngram on image classi\ufb01cation tasks (Lee et al. 2018) using\nTensor\ufb02ow. Students are encouraged to experiment with the\nsource code and try different CNN con\ufb01gurations to improve\nthe classi\ufb01er\u2019s accuracy.\nFollowing CNN, we explore different variants of a Re-\ncurrent Neural Network (RNN) (Graves 2012). RNNs are\nused for sequence tasks. Long short-term memory (LSTM)\n(Hochreiter and Schmidhuber 1997) overcome the explod-\ning and vanishing gradient problems (Hochreiter 1998; Pas-\ncanu, Mikolov, and Bengio 2012) in vanilla RNN, which\nenables them to learn long term dependencies more effec-\ntively. We explore a case study on LSTM-based architecture\nimplemented for the game of FIFA 18.2After 400 minutes of\ntraining, the LSTM based bot scored 4 goals in 6 games of\nFIFA 18 on beginner dif\ufb01culty.\nMoving on, we introduce Generative Adversarial Net-\nworks (GANs) (Goodfellow et al. 2014) and its variations.\nWe then give an example of using GANs to generate high-\nquality anime characters (Jin et al. 2017).\nIn the second half of the course, we introduce the fun-\ndamentals of reinforcement learning. We start by answer-\ning the following questions: What is Reinforcement Learn-\ning? Why is it needed in games? What are its advantages in\ngames? Why can\u2019t we use supervised learning in games? We\nthen introduce Markov Decision Process (MDP), Partially\nObservable Markov Decision Process (POMDP) (Mnih et al.\n2015; Astrom 1965), value iteration (Bellman 1957) and\npolicy iteration.\nWe move on to introduce Q-learning (Watkins 1989) and\nDeep Q-Networks (DQN) (Mnih et al. 2013). In 2013, a\nDeep Q-Network was applied to play seven Atari 2600\ngames (Mnih et al. 2013). In 2015 the same network was\nused to beat human-level performance in 49 games (Mnih\net al. 2015). For this course we ask students to refer to a\nsample program that uses a DQN for Flappy Bird game 3.\nStudents are encouraged to tune the model\u2019s parameters and\nrun the training scripts to get a better practical understanding\nof Deep Q-Learning.\nLastly, we introduce students to Policy Gradient algo-\nrithms (Kakade 2002). Policy gradient based algorithms\nsuch as Actor-Critic (Konda and Tsitsiklis 2000; Fujimoto,\nVan Hoof, and Meger 2018; Mnih et al. 2016) and Proximal\nPolicy Optimization (Schulman et al. 2017) have provided\nstate of art performance for Reinforcement Learning tasks\n(Stooke and Abbeel 2018). A Case Analysis to play Torc, a\nracing car game, using Policy Gradient is covered 4 to sup-\nplement the material covered in class. Students are given a\n2FIFA 18 AI (Last accessed: 12/15/2020): https://github.com/\nChintanTrivedi/DeepGamingAI FIFA\n3Flappy\nBird\nAI\n(Last\naccessed:\n12/15/2020):\nhttps:\n//yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\n4TORCS AI (Last accessed: 12/15/2020): https://yanpanlau.\ngithub.io/2016/10/11/Torcs-Keras.html\nchance to develop their agents to play the game Dino Run 5\nand compete with the remainder of the class.\nReading Assignments\nMaterial for reading assignments primarily stems from An-\ndrew Glassner\u2019s textbook titled Deep Learning: From Ba-\nsics to Practice. This course is supplemented by various\nsources, including articles on websites such as Medium,\nTowardsDataScience, tutorials from GDC, TensorFlow, Py-\ntorch, OpenAI Gym, ML-Agents, and survey papers of re-\ncent advancements in gaming AI research. These materials\nincorporate detailed information on implementing speci\ufb01c\ndeep learning or reinforcement learning algorithms, step-\nby-step guides for implementing a gaming AI project from\nscratch, and state-of-the-art research papers as references.\nGuest Lectures\nWe invited 2-3 guest lecturers every semester who were ei-\nther experienced professionals from the gaming industry or\nPh.D. students researching Deep Learning and Reinforce-\nment Learning for games. These lecturers provided valuable\ninsights to students into how machine learning is applied in\ndifferent gaming research areas. Some of the topics covered\nin these lectures include applications of Deep Learning in\nZynga, Policy Gradient based agents for Doom, and current\nresearch frontiers for machine learning in gaming. The lec-\nturers also attended student presentations and provided stu-\ndents with feedback on technologies that they could utilize\nfor their respective projects.\nFigure 1: DQN based agent for Ghostbusters\nStudent Projects\nThis section selected and summarized 32 student course\nprojects, covering various topics based on the different sub-\ndomains illustrated earlier in the background section.\nMachine Learning for Playing Games\nTo train AI agents in League of Legends, one project used\nYOLOv3 object detection algorithm to identify different\nchampions and NPCs in League of Legends. They also\ntrained two separate agents, one combining PPO and LSTM,\nand one supervised LSTM trained on keyboard and mouse\n5Dino Run AI (Last accessed: 12/15/2020): https://blog.\npaperspace.com/dino-run/\n pressed captured from the advanced League of Legends\nplayers. In a one-on-one custom game, agents achieved \ufb01rst\nblood against amateur and average players, respectively (Lo-\nhokare, Shah, and Zyda 2020).\nTackling a tower defense game, one team focused on for-\nmulating a strategy to place towers. The agent also had to\nmonitor gold income from destroying monsters and view the\nbest locations and timing to place the tower as well as tower\nupgrades. Using a CNN, the agent is trained on summarized\ndata of randomly generated tower placements where each\nsample includes the placement of towers, selling and up-\ngrade of towers, and the last wave number achieved.\nScotland Yard and Ghostbusters are two similar projects\nthat aim to train agents to play hide and seek. The agents use\nan asymmetric environment with incomplete information to\neither seek or hide from the other agent. There are one hid-\ning player and \ufb01ve seeker players. For both games, the two\nteams built DQN based agents with different reward shaping\nfunctions for the seekers as well as the hider. Figure 1 shows\nthe environment for training an agent in Scotland Yard.\nAn agent trained to play the online multiplayer game\nSlither.IO aim to achieving a high score against other play-\ners. Applying a DQN and Epsilon Greedy Learning Strategy,\nthe agent observed the game\u2019s current frame to determine a\ndirection to move in.\nPokemonShowdown is an online game simulator to play a\none-on-one match of Pokemon. With a prede\ufb01ned Pokemon\nset, an agent was trained using a DQN with states incorpo-\nrating the arena state, player active, and reserve states to de-\ntermine its next actions. Against a minimax agent, the DQN\nagent won 17 games out of 20 and effectively learned super\neffective moves and generally avoided minimally effective\nones.\nDonkey Kong is a Nintendo 1983 arcade game where\nMario has to reach Donkey Kong while dodging barrels.\nStarting from a minimal interface, a team mapped and fed\neach object\u2019s bit locations to an agent based on a Q-learning.\nThis agent could be further broken down into a priority and\nbackground agent. This project successfully produced an\nagent that can complete the \ufb01rst level in Donkey Kong.\nBenchmark Environments and Competitions\nMarioKart64 is a benchmark game for numerous tutorials\nand competitions. Using a CNN and DAGGER algorithm,\nthe team compared their agent\u2019s recoverability from going\noff track or immediately using power-ups. Moreover, the\nteam applied transfer learning to a Donkey Kong racing\ngame.\nTwo Pommerman teams worked on building an agent to\nplay the NES game Bomberman. Both teams used Pytorch\nand TensorFlow but differed in that one focused on PPO\nand A2C whereas the other team focused on Accelerated\nProximal Policy Optimization (APPO). Along with differ-\nent reward functions, the teams found that PPO and APPO\nagents on average outperformed the A2C agent in explor-\ning the game board but not necessarily in laying bombs or\nwinning the game.\nInspired by DeepMind\u2019s AlphaGo, one team tackled the\ngame of Go with their agent. Despite the hardware differ-\nence, the team successfully trained an agent to win over am-\nateur human player. Using greedy and simple neural network\nagents as a benchmark, the team\u2019s agent utilized both tradi-\ntional and deep learning algorithms to outperform the base-\nline agents, achieving the same rank as an advanced amateur\nplayer (1 dan).\nAnother DeepMind inspired team explored the research\nand underlying architecture of the multi-agent system, Al-\nphaStar, in the RTS environment Starcraft II. Speci\ufb01cally,\nthe project aimed to utilize algorithms such as DQN with\nexperience replay, CNN, Q-learning, and behavior tree to\nmodel different agents against an AI. The team success-\nfully trained four agents where each agent played 100 games\nagainst an easy AI where the win rates were 13, 68, 96, and\n59, respectively.\nFigure 2: Game interface for MapChat: A game designed\nleveraging text-to-speech and automatic speech recognition\nto teach players English\nComputer Vision\nDeep fake applications which uses deep learning to gener-\nate fake images or videos have raised debates in AI com-\nmunity. One project applied realistic and personalized head\nmodels in a one-shot setting as an overlay to video game\ncharacters. They picked Unity3D Mario model for exper-\niment. Taking a video input, the machine learning system\nextracted facial landmarks on a person and mapped them to\na speci\ufb01ed character model. In mapping human features to\na character Mario, the system primarily looked at detecting\nthe head model as well as speci\ufb01c facial features; the eyes,\nnose, and mouth.\nCounter-Strike Global Offensive (CSGO) is a popular on-\nline \ufb01rst-person shooter game. Using object detection mod-\nels based on Single-Shot Detection and Retinanet, an agent\nwas trained to play the game while distinguishing friends\nfrom foes. The agent also demonstrated visual transfer learn-\ning between the newer CSGO and the older Counter-Strike\n1.6 game, where the model learned low-level features com-\nmon to both CS 1.6 and CSGO.\nMotion recognition is an important task in computer vi-\nsion. One team developed a game from scratch while lever-\naging Computer Vision techniques in Unity 3D. The team\ncreated an underwater endless runner game where the agent\nmust overcome random rock hurdles and collect money.\n Python\u2019s OpenCV package was used to detect human body\nmovements and move the submarine correspondingly. As\nthe human player moving left, right, up (jump) or down\n(crouch), the submarine responded in the same directions via\nTensorFlow\u2019s PoseNet.\nA different motion capture project focuses on pose esti-\nmation and accurate \ufb01delity for weightlifting form. The team\ncollected data of both good and bad forms of various exer-\ncises to be marked and fed into a OpenPose model. They\ntackled the project in three approaches; splitting the input\ninto a series of periodic frames, summarizing frames, and\nfeeding full frames into a Keras ResNet CNN model. The\nvideo is evaluated by a voting system model that tells the\nuser if the exercise had good or bad form.\nNatural Language Processing\nFeaturing text-to-speech and automatic speech recogni-\ntion, MapChat helps users practice their English speaking\nskills through a quest-based role-playing game. Users can\nmove around and complete objectives by speaking prompted\nphrases in speci\ufb01c locations using a simple map. The au-\ndio is recorded and processed to provide feedback to the\nuser regarding how clear and cohesive the response is. Fig-\nure 2 shows the game interface developed by students for\nMapchat.\nLanguage generation is a challenging task in NLP. Utiliz-\ning FIFA and Pro Evolution Soccer commentaries, a team\ngenerated on-the-\ufb02y commentary for a football game. Ap-\nplying NLP techniques, the team fed their Tensor\ufb02ow model\ngame prompts as seed words to produce relevant sentences\nthat produced coherent and realistic announcer prompts. An-\nother team sourced their information from movie databases\nlike IMDb and IMSDb. With the goal of dialogue genera-\ntion, their system examines keywords, dialogues, and sen-\ntiment from game text. Using multiple models and frame-\nworks such as Markovify, LSTM, and Watson, the team gen-\nerated coherent character dialogues.\nData Science and Player Modeling\nOne CSGO project (Zeng et al. 2020) proposes a Sequence\nReasoner with Round Attribute Encoder and Multi-Task De-\ncoder to interpret the round-based purchasing decisions\u2019\nstrategies. They adopt few-shot learning to sample multi-\nple rounds in a match and modi\ufb01ed model agnostic meta-\nlearning algorithm Reptile for the meta-learning loop. They\nformulate each round as a multi-task sequence generation\nproblem. The state representations combine action encoder,\nteam encoder, player features, round attribute encoder, and\neconomy encoders to help their agent learn to reason under\nthis speci\ufb01c multi-player round-based scenario. A complete\nablation study and comparison with the greedy approach cer-\nti\ufb01es the effectiveness of their model.\nInstead of looking at the in-game content of CSGO, an-\nother team examined the mental state of the CSGO Twitch\naudience to detect and de\ufb01ne a metric of audience immer-\nsion. Representations of different modalities are fed to a\nmulti-modal fusion system. Representations learned through\nCNN and RNN cover three modalities, i.e., video record-\nings, commentary audio, and Twitch chat. The model as-\nsigns text inputs with positive or negative connotations that\nlater use gameplay audio to capture and map audience im-\nmersion.\nProcedural Content Generation\nPart of the famous Mario genre, Super Mario Bros. is a side-\nscrolling game where levels are meticulously designed from\nscratch. However, with procedural generation, a level can be\nproduced and deployed with minimal or no design changes.\nUsing an RNN, LSTM, and Markov Chain model, the team\nmap a sequence of characters to an object in the Mario world\nthat is later interpreted as a Mario Level. Each generated\nlevel is evaluated by an A* agent to determine if the agent\ncan complete the level. Ultimately the Markov model pro-\nduced the best ratio of completed to incomplete levels fol-\nlowed by the RNN and LSTM models.\nGenerating creative arts play an important role in gam-\ning. One team worked on constructing a GAN for character\nand asset creation in their custom-Unity game. In addition to\nbuilding a GANs, the team used Unity\u2019s ML-agents libraries\nas a framework to build offensive and defensive AI\u2019s which\nwere trained with different reward functions.\nUsing conditional GANs, one team augmented real videos\nwith stylized game environments. A reference style image is\nused as an input to encode two vectors to generate a Gaus-\nsian random noise based model to a video generator. SPADE\nResNet blocks are then used to reinforce the segmentation\nmask and provide coherent and consistent video frames. The\nconstructed standalone model could be used to apply a ref-\nerence style to any real-world input video.\n(a) Quad-copter Project\n(b) Humanoid Project\nFigure 3: Example projects simulating and training robotics\nagents in OpenAI Gym.\nSimulated Interactive Environments\nMotivated by the 2020 Australian Wild\ufb01res, a team simu-\nlated a wild\ufb01re in a forest ecosystem through a Unity video\ngame. The goal is to train an agent, represented by a \ufb01re-\n\ufb01ghter dog, to \ufb01nd and save endangered animals from the\n\ufb01re without hurting itself. Using Unity\u2019s Machine Learning\nLibrary, the team trained the agent using PPO, Behavioral\nCloning (BC), and Generative Adversarial Imitation Learn-\ning (GAIL) to evaluate how long each agent takes to rescue\nan animal.\nThere were several self-driving student projects in this\ncourse. Students developed autonomous driving agents for\n Your initial learning motivation\nfor taking this class?\nSample Size 93.\n52% AI in Interactive Entertainment\n38% Computer Vision\n29% Game theory,\nMulti-agent Systems\n23% NLP, Data Science\nHuman-Computer Interaction\n12% Procedural Content Generation\n5% Gaming Benchmark\nEnvironments and Competitions\nWhat have you learned most\nfrom this class? Can choose\nmore than one, sample size 55.\n75% Hand on Team\nProject Experiences\n74% Applied Machine\nLearning for Gaming\nTasks\n53% Deep Learning and\nReinforcement Learning\nTheory\nWhat did you struggle with\nmost in class?\nCan choose more than one,\nsample size 55.\n38% Division of Labor\nwithin Team\n36% Applying Deep Learning\nAlgorithms\n31% Finding the topic\n16% Finding the team\n9% Lost during\nweekly progress\nOn a scale of 1(low)\n-5(high), how would\nyou recommend\nthis class to your\nfriends?\nSample size 55.\n0%\n1\n2%\n2\n9%\n3\n29%\n4\n60%\n5\nOn a scale of 1(low)\n-5(high), how do you\nthink this class will\nhelp you \ufb01nd a\nfull-time job or\ninternship?\nSample size 55.\n0%\n1\n5%\n2\n33%\n3\n51%\n4\n11%\n5\nTable 1: Our evaluation for the class is based on two sets of surveys containing \ufb01ve questions in total. The detailed survey\nstatistics are listed below each question.\nthe games Grand Theft Auto V, Mario kart, Track Ma-\nnia Nations Forever, TORCS, and Live for Speed racing\nsimulator. Different techniques were applied to each of\nthese projects. Object detection algorithms such as AlexNet,\nVGG16, YOLOv3, and Hough transformations were imple-\nmented to detect the racing track and obstacles within the\ngame and avoid collision with other vehicles. DQN, Imita-\ntion Learning, Policy Gradients, and Transfer Learning were\nexperimented with to train the agent to drive.\nAnother self-navigating team trained an autonomous\nquadcopter in a simulated 3D environment shown in the left\nof Figure 3. The team implement a uni\ufb01ed quadrotor con-\ntrol policy using PPO, curriculum, and supervised learning\nto enable the drone going along a speci\ufb01c path. This policy\ngeneralized the task as a single learning task, signi\ufb01cantly\nreducing the amount of training needed. In addition to re-\nward shaping, the team tested the model across different\ntrack environments, such as a 2D square, 2D square diag-\nonal, and a descending ellipse.\nApart from simulating drones, students also explored on\nsimulated humanoids in robotics seen in the right of Figure\n3. Using PyBullet, the agent was trained in varying environ-\nments to move a ball to a designated area. Applying rein-\nforcement learning algorithms like PPO and A2C, the team\nsimulated two movements; hopping and walking in different\nwind environments for each movement. The team de\ufb01ned\nreward functions based on how long the agent remains alive,\nhow close it is to the target location, and how much move-\nment is taken to achieve the goal.\nResources\nFor this course, we provide each student with $50 Google\nCloud Credit (GCP) to be utilized for training Deep Learn-\ning algorithms. This sums up to $300 for a team of 6 stu-\ndents. In addition to this, students are provided laboratory\naccess to high-end Windows systems. These systems are\nequipped with NVIDIA GTX 1080 GPUs, 32GB RAM, and\nIntel i7 7th generation processors.\nThis course structure withstood the challenges of transi-\ntioning from in-person to hybrid and eventually to fully on-\nline modalities throughout the COVID-19 Pandemic. The\nresistance to risks is mainly credited to the extensive use\nof Google Cloud services, Github for code version control,\nSlack and Zoom for instant communication, as well as Pi-\nazza and Blackboard for course logistics. The semester-long\nteam project has also provided \ufb02exible but adjustable dif\ufb01-\nculties for both students and instructors.\nEvaluations and Conclusion\nTable\n1 shows our survey results to evaluate the course.\nA majority of students gave high ratings for recommending\nthis course to other students, the usefulness of this course for\n\ufb01nding an internship or a full-time job, and learning from\nteam projects to get applied machine learning hands-on ex-\nperiences. The survey results indicate positive feedback for\nthe course.\nFrom a teaching perspective, we encountered three chal-\nlenges: At what level should we balance theoretical deep\nlearning and reinforcement learning lecture materials and\napplied environment demonstrations; How to create an adap-\ntive learning curve for students with varying machine learn-\ning backgrounds; and how to form an innovative research\npipeline at the graduate school level to facilitate publica-\ntions. Throughout the three semesters, we learned that more\nvisual aids, such as live demonstrations and videos, are\nneeded to increase online engagement as we move into on-\nline virtual courses. Weekly project demonstrations in front\nof the whole class will create a healthy peer effect that in-\ncreases learning ef\ufb01cacy. Within three semesters, three re-\nsearch conference papers have been published, and more\nin preparation. From the students\u2019 self-proposed projects,\nwe strengthened our belief that gaming as an interdisci-\nplinary research domain can reach other \ufb01elds, such as\nrobotics, medical diagnosis, human-computer interactions,\netc. Games are the testbeds for advancing state-of-the-arts\nlearning algorithms. In the future, the class can bene\ufb01t\nfrom state-of-the-arts paper reading sessions and live cod-\ning demonstrations to help graduate students build a com-\nprehensive understanding of how a research project is built.\nThis report summarizes the design of our applied machine\nlearning course for graduate students interested in applying\ndeep learning and reinforcement learning advancements to-\nwards gaming. We familiarize students with the current re-\nsearch landscape and improve students\u2019 oral and written pre-\nsentation skills through practical team projects, regardless of\nthe major and machine learning expertise level. Our course\ncan help students open doors for an industry career, aim for\npublications, or lay the foundations of future products.\n References\nAstrom, K. J. 1965. Optimal control of Markov decision\nprocesses with incomplete state estimation. J. Math. Anal.\nApplic. 10: 174\u2013205.\nAytar, Y.; Pfaff, T.; Budden, D.; Paine, T.; Wang, Z.; and\nde Freitas, N. 2018.\nPlaying hard exploration games by\nwatching youtube. In Advances in Neural Information Pro-\ncessing Systems, 2930\u20132941.\nBaker, B.; Kanitscheider, I.; Markov, T.; Wu, Y.; Powell, G.;\nMcGrew, B.; and Mordatch, I. 2019. Emergent tool use from\nmulti-agent autocurricula. arXiv:1909.07528 .\nBaldi, P. 1995.\nGradient descent learning algorithm\noverview: A general dynamical systems perspective. IEEE\nTransactions on neural networks 6(1): 182\u2013195.\nBard, N.; Foerster, J. N.; Chandar, S.; Burch, N.; Lanctot,\nM.; Song, H. F.; Parisotto, E.; Dumoulin, V.; Moitra, S.;\nHughes, E.; et al. 2020. The hanabi challenge: A new fron-\ntier for ai research. Arti\ufb01cial Intelligence 280: 103216.\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The arcade learning environment: An evaluation plat-\nform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch 47: 253\u2013279.\nBellman, R. 1957. A Markovian decision process. Journal\nof mathematics and mechanics 679\u2013684.\nBerner, C.; Brockman, G.; Chan, B.; Cheung, V.; Debiak,\nP.; Dennison, C.; Farhi, D.; Fischer, Q.; Hashme, S.; Hesse,\nC.; et al. 2019. Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv:1912.06680 .\nBrockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.;\nSchulman, J.; Tang, J.; and Zaremba, W. 2016. Openai gym.\narXiv preprint arXiv:1606.01540 .\nC\u02c6ot\u00b4e, M.-A.; K\u00b4ad\u00b4ar, A.; Yuan, X.; Kybartas, B.; Barnes, T.;\nFine, E.; Moore, J.; Tao, R. Y.; Hausknecht, M.; Asri, L. E.;\nAdada, M.; Tay, W.; and Trischler, A. 2018.\nTextWorld:\nA Learning Environment for Text-based Games.\nCoRR\nabs/1806.11532.\nFujimoto, S.; Van Hoof, H.; and Meger, D. 2018. Addressing\nfunction approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477 .\nGirshick, R. 2015. Fast r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision, 1440\u20131448.\nGirshick, R.; Donahue, J.; Darrell, T.; and Malik, J. 2014.\nRich feature hierarchies for accurate object detection and\nsemantic segmentation. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 580\u2013587.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.\n2014. Generative adversarial nets. In Advances in neural\ninformation processing systems, 2672\u20132680.\nGraves, A. 2012.\nSupervised sequence labelling.\nIn Su-\npervised sequence labelling with recurrent neural networks,\n5\u201313. Springer.\nHecht-Nielsen, R. 1992.\nTheory of the backpropagation\nneural network. In Neural networks for perception, 65\u201393.\nElsevier.\nHochreiter, S. 1998. The vanishing gradient problem during\nlearning recurrent neural nets and problem solutions. Inter-\nnational Journal of Uncertainty, Fuzziness and Knowledge-\nBased Systems 6(02): 107\u2013116.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation 9(8): 1735\u20131780.\nHu, H.; Yarats, D.; Gong, Q.; Tian, Y.; and Lewis, M. 2019.\nHierarchical decision making by generating and following\nnatural language instructions. In Advances in neural infor-\nmation processing systems, 10025\u201310034.\nJaderberg, M.; Czarnecki, W. M.; Dunning, I.; Marris, L.;\nLever, G.; Castaneda, A. G.; Beattie, C.; Rabinowitz, N. C.;\nMorcos, A. S.; Ruderman, A.; et al. 2019. Human-level per-\nformance in 3D multiplayer games with population-based\nreinforcement learning. Science 364(6443): 859\u2013865.\nJin, Y.; Zhang, J.; Li, M.; Tian, Y.; Zhu, H.; and Fang,\nZ. 2017.\nTowards the automatic anime characters cre-\nation with generative adversarial networks. arXiv preprint\narXiv:1708.05509 .\nJohnson, M.; Hofmann, K.; Hutton, T.; and Bignell, D. 2016.\nThe Malmo Platform for Arti\ufb01cial Intelligence Experimen-\ntation. In IJCAI, 4246\u20134247.\nJuliani, A.; Berges, V.-P.; Vckay, E.; Gao, Y.; Henry, H.;\nMattar, M.; and Lange, D. 2018. Unity: A general platform\nfor intelligent agents. arXiv preprint arXiv:1809.02627 .\nJustesen, N.; Bontrager, P.; Togelius, J.; and Risi, S. 2019.\nDeep Learning for Video Game Playing. IEEE Transactions\non Games 1\u20131. ISSN 2475-1510. doi:10.1109/TG.2019.\n2896986.\nKakade, S. M. 2002. A natural policy gradient. In Advances\nin neural information processing systems, 1531\u20131538.\nKonda, V. R.; and Tsitsiklis, J. N. 2000. Actor-critic algo-\nrithms. In Advances in neural information processing sys-\ntems, 1008\u20131014.\nLeCun, Y. A.; Bottou, L.; Orr, G. B.; and M\u00a8uller, K.-R. 2012.\nEf\ufb01cient backprop. In Neural networks: Tricks of the trade,\n9\u201348. Springer.\nLee, S.-J.; Chen, T.; Yu, L.; and Lai, C.-H. 2018. Image clas-\nsi\ufb01cation based on the boost convolutional neural network.\nIEEE Access 6: 12755\u201312768.\nLin, Z.; Gehring, J.; Khalidov, V.; and Synnaeve, G. 2017.\nStardata: A starcraft ai research dataset.\narXiv preprint\narXiv:1708.02139 .\nLohokare, A.; Shah, A.; and Zyda, M. 2020. Deep Learning\nBot for League of Legends.\nIn Proceedings of the AAAI\nConference on Arti\ufb01cial Intelligence and Interactive Digital\nEntertainment, volume 16, 322\u2013324.\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asyn-\nchronous methods for deep reinforcement learning. In In-\nternational conference on machine learning, 1928\u20131937.\n Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-\ning atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602 .\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fid-\njeland, A. K.; Ostrovski, G.; et al. 2015. Human-level con-\ntrol through deep reinforcement learning. Nature 518(7540):\n529\u2013533.\nNair, V.; and Hinton, G. E. 2010. Recti\ufb01ed linear units im-\nprove restricted boltzmann machines. In Proceedings of the\n27th international conference on machine learning (ICML-\n10), 807\u2013814.\nNichol, A.; Pfau, V.; Hesse, C.; Klimov, O.; and Schulman,\nJ. 2018. Gotta Learn Fast: A New Benchmark for General-\nization in RL. arXiv preprint arXiv:1804.03720 .\nOngsulee, P. 2017. Arti\ufb01cial intelligence, machine learning\nand deep learning. In 2017 15th International Conference\non ICT and Knowledge Engineering (ICT KE), 1\u20136. ISSN\n2157-0981. doi:10.1109/ICTKE.2017.8259629.\nPascanu, R.; Mikolov, T.; and Bengio, Y. 2012. Understand-\ning the exploding gradient problem. CoRR, abs/1211.5063\n2: 417.\nRedmon, J.; Divvala, S.; Girshick, R.; and Farhadi, A. 2016.\nYou only look once: Uni\ufb01ed, real-time object detection. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 779\u2013788.\nRedmon, J.; and Farhadi, A. 2017.\nYOLO9000: better,\nfaster, stronger. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 7263\u20137271.\nRedmon, J.; and Farhadi, A. 2018. Yolov3: An incremental\nimprovement. arXiv preprint arXiv:1804.02767 .\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal net-\nworks. In Advances in neural information processing sys-\ntems, 91\u201399.\nRisi, S.; and Preuss, M. 2020. From Chess and Atari to Star-\nCraft and Beyond: How Game AI is Driving the World of\nAI. KI-K\u00a8unstliche Intelligenz 34(1): 7\u201317.\nSapienza, A.; Zeng, Y.; Bessi, A.; Lerman, K.; and Fer-\nrara, E. 2018. Individual performance in team-based online\ngames. Royal Society open science 5(6): 180329.\nSavva, M.; Kadian, A.; Maksymets, O.; Zhao, Y.; Wijmans,\nE.; Jain, B.; Straub, J.; Liu, J.; Koltun, V.; Malik, J.; Parikh,\nD.; and Batra, D. 2019. Habitat: A Platform for Embodied\nAI Research. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV).\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347 .\nShridhar, M.; Thomason, J.; Gordon, D.; Bisk, Y.; Han, W.;\nMottaghi, R.; Zettlemoyer, L.; and Fox, D. 2020. ALFRED:\nA Benchmark for Interpreting Grounded Instructions for Ev-\neryday Tasks. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the\ngame of Go with deep neural networks and tree search. na-\nture 529(7587): 484\u2013489.\nStooke, A.; and Abbeel, P. 2018.\nAccelerated meth-\nods for deep reinforcement learning.\narXiv preprint\narXiv:1803.02811 .\nSummerville, A.; Snodgrass, S.; Guzdial, M.; Holmg\u02daard, C.;\nHoover, A. K.; Isaksen, A.; Nealen, A.; and Togelius, J.\n2018. Procedural content generation via machine learning\n(PCGML). IEEE Transactions on Games 10(3): 257\u2013270.\nTian, Y.; Gong, Q.; Shang, W.; Wu, Y.; and Zitnick, C. L.\n2017.\nELF: An Extensive, Lightweight and Flexible Re-\nsearch Platform for Real-time Strategy Games. Advances in\nNeural Information Processing Systems (NIPS) .\nVinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;\nDudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds,\nT.; Georgiev, P.; et al. 2019.\nGrandmaster level in Star-\nCraft II using multi-agent reinforcement learning. Nature\n575(7782): 350\u2013354.\nVinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhn-\nevets, A. S.; Yeo, M.; Makhzani, A.; K\u00a8uttler, H.; Agapiou, J.;\nSchrittwieser, J.; et al. 2017. Starcraft ii: A new challenge for\nreinforcement learning. arXiv preprint arXiv:1708.04782 .\nWatkins, C. J. C. H. 1989. Learning from delayed rewards.\nPh.D. thesis, University of Cambridge.\nZeng, Y. 2020. How Human Centered AI Will Contribute\nTowards Intelligent Gaming Systems. The Thirty-Fifth AAAI\nConference on Arti\ufb01cial Intelligence (AAAI-21) .\nZeng, Y.; Lei, D.; Li, B.; Jiang, G.; Ferrara, E.; and Zyda, M.\n2020. Learning to Reason in Round-Based Games: Multi-\nTask Sequence Generation for Purchasing Decision Making\nin First-Person Shooters. In Proceedings of the AAAI Con-\nference on Arti\ufb01cial Intelligence and Interactive Digital En-\ntertainment, volume 16, 308\u2013314.\nZeng, Y.; Sapienza, A.; and Ferrara, E. 2019.\nThe In\ufb02u-\nence of Social Ties on Performance in Team-based Online\nGames. IEEE Transactions on Games .\n"}, "Argumentation Mining: Exploiting Multiple Sources and Background Knowledge": {"authors": ["Anastasios Lytos", "Thomas Lagkas", "Panagiotis Sarigiannidis", "Kalina Bontcheva"], "title": "Argumentation Mining: Exploiting Multiple Sources and Background Knowledge", "url": "https://arxiv.org/pdf/1809.06943.pdf", "abstract": "The field of Argumentation Mining has arisen from the need of determining the underlying causes from an expressed opinion and the urgency to develop the established fields of Opinion Mining and Sentiment Analysis. The recent progress in the wider field of Artificial Intelligence in combination with the available data through Social Web has create great potential for every sub-field of Natural Language Process including Argumentation Mining.", "arxiv_id": "1809.06943", "published_date": "2018-09-18", "year": 2018, "introduction": "", "conclusion": "", "full_text": " \n66 \n \nArgumentation Mining: Exploiting Multiple Sources and \nBackground Knowledge \nAnastasios Lytos1, Thomas Lagkas2, Panagiotis Sarigiannidis3, and Kalina Bontcheva4 \n1Department of Computer Science, South-East European Research Centre, The University of \nSheffield, Thessaloniki, Greece \n2Computer Science Department, The University of Sheffield International Faculty, \nCITY College, Thessaloniki, Greece \n3Department of Informatics and Telecommunications Engineering, University of \nWestern Macedonia, Kozani, Greece \n4Department of Computer Science, The University of Sheffield, Sheffield, UK \nAbstract. The field of Argumentation Mining has arisen from the need of deter-\nmining the underlying causes from an expressed opinion and the urgency to de-\nvelop the established fields of Opinion Mining and Sentiment Analysis. The re-\ncent progress in the wider field of Artificial Intelligence in combination with the \navailable data through Social Web has create great potential for every sub-field \nof Natural Language Process including Argumentation Mining. \nKeywords: Argumentation Mining \u00b7 Web mining \u00b7 Background knowledge \u00b7 \nArtificial Intelligence \u00b7 Computational Linguistics \u00b7 Machine Learning \u00b7 Social \nMedia \n1 \nIntroduction \nArgumentation Mining (AM) is a multi-disciplinary field that first introduced in 2009 \nfrom Palau and Moens [20] and gained the interest of the scientific community because \nof the progress in many fields in Artificial Intelligence (AI) mostly due to the develop-\nment of Machine Learning (ML) techniques, algorithms and platforms. Another reason \nfor the development of the field is the explosion in the use of Social Media and other \ninteractive capabilities such as comments sections, on-line product reviews and per-\nsonal blogs. \nIn human reasoning interpreting an argument is a natural process that is realized \nautomatically by analyzing many different aspects regarding the discussing topic. How-\never, modeling the argumentation process for the purpose of Automatic Argumentation \nMining is a challenging task with doubts that is even feasible. \nThe main reason behind the effortless and instant grasp of the underlying argument \nbehind from an expressed opinion is the capability of the human being to perceive the \ncontext of the expressed opinion by combining multiple sources of information. The \nmodeling and the exploitation of the background knowledge is a difficult task especially \nif we consider hardware limitations both in memory and speed. \n  \n67 \n \nThe problems related to AM become more severe when web-generated data are used \nas the use of slang is quite often, abbreviations are frequent and there are fallacies in \nthe reasoning process. The aforementioned problems are the main reason behind the \nexploitation of domains that have structured and standardized reasoning for the tasks of \nAM such as law [28, 33] and scientific text [5, 12, 15]. \nThe exploitation of data generated from Web is a more challenging task comparing \nto AM in structured data as the data are unstructured, the expressed opinions are short \nand quite often they don\u2019t include an argument. Opinion Mining and Sentiment Analy-\nsis are thriving in the era of Social Web, however AM seems not to be able to exploit \nthe capabilities from the volume and variety of data that are offered. \nIn order to take advantage of the new capabilities offered from the Web in the field \nof AM we propose alternative paths from the implementation of the existing Argumen-\ntation theories. We propose the development of two new schemes for the exploitation \nof Web-generated data and small texts in general, relying on two basic principles 1) the \nconstant production of background knowledge 2) the combination of multiple sources \nand the evaluation of them. In this paper we propose two abstract frameworks that are \ncharacterized by the capabilities of modification and extensibility. \n2 \nRelated Work \nAM as a research field is quite recent but the study and the analysis on arguments in \nthe speech are held from 4 century B.C. [2] changing forms and objectives through \nyears. In this section we will present the Argumentation Models which have proposed \nat the end of the 20th century and the first attempts of AI implementing Argumentation \nModels before proceeding to the task of Automatic AM. \nThe illustration of argument parts with the use of interconnected nodes is a common \ntechnique nowadays and first introduced by Beardsley in [4]. In Beardsley\u2019s model \nthere are defined three basic categories of arguments: 1) Convergent Argument 2) Di-\nvergent Argument and 3) Serial Argument. Beardsley\u2019s theory laid the foundation of \nmany recent Argumentation Schemes, however it has the weakness of not defining the \nrelationships between the nodes. \nA more detailed scheme is introduced from Toulmin [30] which stands out because \nof the precise description of the argument entities. In Toulmin\u2019s model six functional \nroles were suggested, datum, warrant, backing, qualifiers and rebuttal. These roles pro-\nvide a quite detailed view of the expressed arguments, showing the completeness or not \nof an argument. In figure 1 an example of Toulmin\u2019s theory is depicted where datum is \nthe first part of the argument in the left, the warrant part is expressed through the con-\njuction since, the backing supports the warrant and both of them lead to the qualifier. \nThe rebuttal part is optional and provides additional support to qualifier. \nA different approach is followed in Mann and Thompson [18] aiming at the organ-\nization of the text into different regions. The proposed architecture is characterized as \nan open scheme with few established rules and is offered for extensions and modifica-\ntions. The basic concepts of the scheme is a central part under the name nucleus which \nis framed with a number of satellites. The distinction of nucleus-satellite is applied \n  \n68 \n \nrecursively until every part of the argument is associated with another one. The relations \nthat have created are depicted either in tree-structure format either in XML format [27]. \nThe first attempts on modeling arguments with AI techniques and models took place \nin the 1990\u2019s with the pioneer research of Pollock [24] which describes the connection \nbetween philosophy non-monotonic reasoning in AI. These first attempts were focused \nin fields where communication takes place according a series of well-established rules \nof communication such as law, rhetorics and scientific text. \nThe connection between argumentation and Logic Programming was researched in \nDung [9] and in Krause et al. [16]. In both attempts the approach that was followed is \nthe establishment of a series of rules, definitions and propositions aiming at the accept-\nability and the integrity of the reasoning process. A similar approach was also followed \nfrom Parsons and Jennings [22] without focusing on the optimum solution but on an \nacceptable compromise. \nThe supremacy of tailored arguments in an advising system was researched in \nCarenini and Moore [8] based on the generation system they have established earlier \n[7]. A similar problem, the solution of possible conflicts in a dialectical argumentation \nschemes, is researched on the work of Grasso et al. [11] with the implementation of the \ntheory that was developed from Perelman and OlbrechtsTyteca [23]. \n \nFig. 1. Toulmin\u2019s Scheme Example [30] \n \n  \n69 \n \n \nFig. 2. Proposed Architecture for Combination of Multiple Sources \n3 \nArgumentation Mining and Automatic Classification \nThe term of AM refers to many different interrelated tasks that can be research either \nindependently either in the wider context of extracting arguments from text. The auto-\nmatic classification of arguments is the last step of the AM pipeline and usually is the \noutcome of the previous steps applied on a specific task on a specific field. There is \nrecent work which exploits Internet sources like twitter [1, 10, 19, 29], reddit [13] and \nother forums [6, 17, 21, 31] but none of the previous work do not combine the Internet \nunstructured data with objective and measurable metrics. \n3.1 \nCombining Multiple Sources \nIn this subsection we propose a scheme which combines multiple sources in order to \nassess the reliability of them. The proposed pipeline exceeds the established boundaries \nof AM as is it can be applied only to commercial problems revealing real processing \nmechanisms of the human brain. Eventually the proposed pipeline examines the ques-\ntion of quantity vs quality as the core of the research is to find what affects more the \ngeneral opinion, the plethora of anonymous opinions or thoughtful opinions and objec-\ntive metrics. \nIn figure 2 the proposed pipeline is presented in an abstract from which can be mod-\nified and extended. In order to illustrate better the proposed architecture we will provide \nstep-by-step directions of every step. Assuming a controversial topic that has intrigued \n  \n70 \n \nboth the Social Media platforms and traditional media like the recent privacy scandal \nof facebook1. \nThe first step of the proposed pipeline is the collection of data through the conver-\nsation on Social Media, articles of recognized media in their digital format and the mon-\nitoring of the stock market for the specific firm and its competitors. Provided that the \ncollection of the data has completed the second step is the Source Identification of each \ninput data followed by the stage of Facts Recognition. These two steps are crucial in \nour analysis as through research the appropriate weight will be assigned and form the \nfinal pipeline. The next step is the Sentiment Classification (SC) where different senti-\nment lexicons evaluate the collected data, SC is a well-known challenge for the scien-\ntific community and different lexicons have created [3, 14, 25, 26, 32]. The ultimate \nstep of the proposed pipeline is the Stance Detection where the outcome of the archi-\ntecture is finally formed and the degree of success of the modeling process is evaluated. \n \n \nFig. 3. Proposed Architecture for Generating Background Knowledge \n3.2 \nExploiting and Adapting Background Knowledge \nApart from the combination of multiple sources the second challenge we try to answer \nis the constant generation and adaptation of the background knowledge of the system. \nIn the process of the human reasoning the interpretation of the natural language is an \n                                                           \n1 https://www.theverge.com/2018/4/9/17214814/facebook-data-notification-\ncambridge-analytica \n  \n71 \n \ninstant operation that combines the linguistic, lexical and sentiment characteristics of \nthe language with the background knowledge that was obtained previously from multi-\nple sources. The most remarkable process of the human brain is the adaptation of the \nbackground knowledge depending from the subject of the discussion, the reliability of \nthe source and the completeness of the argument itself. \nIn the proposed architecture, depicted in fig. 3, a constant feedback of the AM pipe-\nline is presented for the dynamic adaptation of it. The feedback loop is based on the \ninteraction of the human factor, which evaluates the model and proceeds in the rede-\nsigning of the system if necessary. In this way the pipeline is evaluated regularly and \nremains up-to-date with possible changes in the complicated environment it interacts. \nFor the clarification of the proposed scheme we will proceed in a step-by-step ex-\nample. We have the input collected for a specific topic (e.g. facebook privacy scandal) \nfollowed by two processes, the Argument Discovery and the Sentiment Classification. \nIn the process of Argument Discovery the effort is focused in the existence or not of an \nargument in the text, whereas the Sentiment Classification provide the information for \nsentiment tone of the text. These two processes lead to the step of the Stance Detection \nwhere the arguments in a specific topic are summarized and eventually in the Modeling \nstep their significance and influenced are determined. The last step of the suggested \nmodel is the evaluation of it from a human perspective which could find flaws and de-\nfects in the system and could proposed modifications. \nThe ultimate target of AM is the automatic interpretation, evaluation and classifica-\ntion of arguments. However, every suggested pipeline requires the human supervision \nfor evaluation of the proposed model, thus neither our model could avoid the human \ninteraction. \n4 \nConclusions and Future Work \nIn this short paper we provide a solid background in the area of AM covering different \naspects and presenting the evolution of the Argumentation problem through time and \ndifferent scientific views. We stressed the significance of the field for the better under-\nstanding of the natural language and identified the prosperous environment for the \ngrowth of the field in the era of Social Web due to the huge amount of web-generated \ndata. \nIn the third section of the paper we propose two schemes in an effort to interpret \nand exploit the constantly changing environment of the Web providing new ideas and \nschemes for the task of AM. Concerning the proposed pipeline for the exploitation of \ndifferent sources of input data, the pipeline is open to modification and able to cover \ndifferent problems such as the location of underlying relations between crowd-sourcing \nand stock market or the effect of different sources of news. The second scheme stress \nthe need for more adaptable models in the field of AM which can provide feedback \nfrom the results instead of stiff architectures that cannot adapt to new requirements and \nneeds. \nWe strongly believe that the field of AM could flourish in the era of Social Web \nprovided that the scientific community can adapt to the new conditions and exploit the \n  \n72 \n \nnew opportunities. The proposed AM schemes should be flexible considering argu-\nments with flaws or weakly expressed and presenting solutions for the industry such as \nprecise recommendation system or advertising impact in Social Media. Apart from in-\ndustry applications another concern AM has to face technical challenges like Big Data, \nDeep Learning and Unsupervised Machine Learning Techniques, technologies that \ncould boost the field of AM and the wider area of Natural Language Process. \nReferences \n1. Addawood, A. A. and Bashir, M. N. (2016). What is Your Evidence? A Study of \nControversial Topics on Social Media. In Proceedings of the 3rd Workshop on Ar-\ngument Mining, pages 1\u201311, Berlin, Germany. \n2. Aristotle and Kennedy, G. A. (2006). On Rhetoric: A Theory of Civic Discourse. \n3. Baccianella, S., Esuli, A., and Sebastiani, F. (2010). SentiWordNet 3.0: An En-\nhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Nicoletta \nCalzolari (Conference Chair) and Khalid Choukri and Bente Maegaard and Joseph \nMariani and Jan Odijk and Stelios Piperidis and Mike Rosner and Daniel Tapias, \neditor, Proceedings of the Seventh International Conference on Language Re-\nsources and Evaluation (LREC\u201910), Valletta, Malta. \n4. Beardsley, M. C. (1950). Practical Logic. The Philosophical Quarterly. \n5. Blake, C. (2010). Beyond genes, proteins, and abstracts: Identifying scientific \nclaims from full-text biomedical articles. Journal of Biomedical Informatics, \n43(2):173\u2013189. \n6. Boltuzic, F. and Snajder, J. (2014). Back up your Stance: Recognizing Ar-\u02c7 guments \nin Online Discussions. In Proceedings of the First Workshop on Argumentation \nMining, pages 49\u201358, Stroudsburg, PA, USA. Association for Computational Lin-\nguistics. \n7. Carenini, G. (2000). Generating and Evaluating Evaluative Arguments. PhD thesis, \nUniversity of Pittsburgh. \n8. Carenini, G. and Moore, J. D. (2001). An empirical study of the influence of user \ntailoring on evaluative argument effectiveness. In IJCAI\u201901 Proceedings of the 17th \ninternational joint conference on Artificial intelligence - Volume 2, pages 1307\u2013\n1312, Seattle, WA, USA. \n9. Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in \nnonmonotonic reasoning, logic programming and n-person games. Artificial Intel-\nligence, 77(2):321\u2013357. \n10. Dusmanu, M., Cabrio, E., and Villata, S. (2017). Argument Mining on Twitter: Ar-\nguments, Facts and Sources. In Proceedings of the 2017 Conference on Empirical \nMethods in Natural Language Processing, pages 2317\u20132322, Copenhagen, Den-\nmark. \n11. Grasso, F., Cawsey, A., and Jones, R. (2000). Dialectical argumentation to solve \nconflicts in advice giving: a case study in the promotion of healthy nutrition. Inter-\nnational Journal of Human-Computer Studies, 53(6):1077\u2013 1115. \n  \n73 \n \n12. Guo, Y., Korhonen, A., and Lattice, T. P. (2011). A Weakly-supervised Approach \nto Argumentative Zoning of Scientific Documents. In Proceedings of the 2011 Con-\nference on Empirical Methods in Natural Language Processing, pages 273\u2013283, \nEdinburgh, Scotland. \n13. Hidey, C., Musi, E., Hwang, A., Muresan, S., and McKeown, K. (2017). Analyzing \nthe Semantic Types of Claims and Premises in an Online Persuasive Forum. In Pro-\nceedings of the 4th Workshop on Argument Mining, pages 11\u201321, Copenhagen, \nDenmark. Association for Computational Linguistics. \n14. Hu, M. and Liu, B. (2004). Mining and summarizing customer reviews. In Proceed-\nings of the 2004 ACM SIGKDD international conference on Knowledge discovery \nand data mining - KDD \u201904, page 168, New York, New York, USA. ACM Press. \n15. Kirschner, C., Eckle-Kohler, J., and Gurevych, I. (2015). Linking the Thoughts: \nAnalysis of Argumentation Structures in Scientific Publications. In Proceedings of \nthe 2nd Workshop on Argumentation Mining, pages 1\u201311, Denver, Colorado. \n16. Krause, P., Ambler, S., Elvang-Goransson, M., and Fox, J. (1995). A Logic of Ar-\ngumentation for Reasoning under Uncertainty. Computational Intelligence, \n11(1):113\u2013131. \n17. Liebeck, M., Esau, K., and Conrad, S. (2016). What to Do with an Airport? Mining \nArguments in the German Online Participation Project Tempelhofer Feld. In Pro-\nceedings of the Third Workshop on Argument Mining (ArgMining2016), pages \n144\u2013153, Stroudsburg, PA, USA. Association for Computational Linguistics. \n18. Mann, W. C. and Thompson, S. A. (1987). Rhetorical Structure Theory: Descrip-\ntion and Construction of Text Structures. In Natural Language Generation, pages \n85\u201395. Springer Netherlands, Dordrecht. \n19. Mohammad, S. M., Sobhani, P., and Kiritchenko, S. (2017). Stance and Sentiment \nin Tweets. ACM Transactions on Internet Technology, 17(3):1\u201323. \n20. Palau, R. M. and Moens, M.-F. (2009). Argumentation mining. In Proceedings of \nthe 12th International Conference on Artificial Intelligence and Law ICAIL \u201909, \npage 98, New York, New York, USA. ACM Press. \n21. Park, J. and Cardie, C. (2014). Identifying Appropriate Support for Propositions in \nOnline User Comments. pages 29\u201338. \n22. Parsons, S. D. and Jennings, N. R. (1996). Neogotiation Through Argumentation - \nA Preliminary Report. In 2nd Int. Conf. on Multi-Agent Systems, pages 267\u2013274, \nJapan. \n23. Perelman, C. and Olbrechts-Tyteca, L. (1969). The new rhetoric: a treatise on argu-\nmentation. University of Notre Dame Press. \n24. Pollock, J. L. (1987). Defeasible reasoning. Cognitive Science, 11(4):481\u2013 518. \n25. Potts, C. (2010). On the negativity of negation. In Semantics and Linguistic Theory, \nvolume 20, page 636. \n26. Potts, C. (2011). Developing adjective scales from user-supplied textual metadata. \nIn NSF Workshop on Restructuring Adjectives in WordNet, Arlington, VA, USA. \n27. Reitter, D. and Stede, M. (2003). Step By Step: Underspecified Markup in Incre-\nmental Rhetorical Analysis. In Proceedings of the 4th International Workshop on \nLinguistically Interpreted Corpora (LINC-03), pages 77\u201384. \n  \n74 \n \n28. Savelka, J. and Ashley, K. D. (2016). Extracting Case Law Sentences for Argumen-\ntation about the Meaning of Statutory Terms. In Proceedings of the 3rd Workshop \non Argument Mining, pages 50\u201359. \n29. Schneider Nathan, O\u2019Connor Brendan, Saphra Naomi, Bamman David, Faruqui \nManaal, Smith A. Noah, Dyer Chris, and Baldridge Jason (2013). A Framework for \n(Under)specifying Dependency Syntax without Overloading Annotators - Semantic \nScholar. \n30. Toulmin, S. E. (1958). The Uses of Argument. \n31. Wei, Z., Xia, Y., Li, C., Liu, Y., Stallbohm, Z., Li, Y., and Jin, Y. (2016). A Pre-\nliminary Study of Disputation Behavior in Online Debating Forum. In Proceedings \nof the 3rd Workshop on Argument Mining, pages 166\u2013171, Berlin, Germany. \n32. Wiebe, J. and Mihalcea, R. (2006). Word sense and subjectivity. In Proceedings of \nthe 21st International Conference on Computational Linguistics and the 44th annual \nmeeting of the ACL - ACL \u201906, pages 1065\u20131072, Morristown, NJ, USA. Associ-\nation for Computational Linguistics. \n33. Wyner, A., Mochales-Palau, R., Moens, M.-F., and Milward, D. (2010). Ap-\nproaches to Text Mining Arguments from Legal Cases. In Semantic Processing of \nLegal Texts, pages 60\u201379. Springer, Berlin, Heidelberg. \n"}, "Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing": {"authors": ["Joohong Lee", "Sangwoo Seo", "Yong Suk Choi"], "title": "Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing", "url": "https://arxiv.org/pdf/1901.08163.pdf", "abstract": "Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing (NLP). Most previous models for relation classification rely on the high-level lexical and syntactic features obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET. Experimental results on the SemEval-2010 Task 8, one of the most popular relation classification task, demonstrate that our model outperforms existing state-of-the-art models without any high-level features.", "arxiv_id": "1901.08163", "published_date": "2019-01-23", "year": 2019, "introduction": "Introduction Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks, such as information extraction, question answering and knowledge base population [14]. A task of relation classi\ufb01cation is de\ufb01ned as predicting a semantic relationship between two tagged entities in a sentence. Figure 1: A Sample of Relation Classi\ufb01cation. For example, given a sentence with tagged entity pair, crash and attack, this sentence is classi\ufb01ed into the re\u2217Department of Computer Science and Engineering, Hanyang University, Seoul, Republic of Korea, {roomylee, ssw1591, cys}@hanyang.ac.kr \u2020Corresponding author. lation Cause-E\ufb00ect(e1,e2)1 between the entity pair like Figure 1. A \ufb01rst entity is surrounded by \u27e8e1\u27e9 and \u27e8/e1\u27e9, and a second entity is surrounded by \u27e8e2\u27e9 and \u27e8/e2\u27e9. Most previous relation classi\ufb01cation models rely heavily on high-level lexical and syntactic features obtained from NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizer (NER). The classi\ufb01cation models relying on such features su\ufb00er from propagation of implicit error of the tools and they are computationally expensive. Recently, many studies therefore propose end-toend neural models without the high-level features. Among them, attention-based models, which focus to the most important semantic information in a sentence, show state-of-the-art results in a lot of NLP tasks. Since these models are mainly proposed for solving translation and language modeling tasks, they could not fully utilize the information of tagged entities in relation classi\ufb01cation task. However, tagged entity pairs could be powerful hints for solving relation classi\ufb01cation task. For example, even if we do not consider other words except the crash and attack, we intuitively know that the entity pair has a relation Cause-E\ufb00ect(e1,e2)1 better than Component-Whole(e1,e2)1 in Figure 1 To address these issues, We propose a novel endto-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET). To capture the context of sentences, We obtain word representations by self attention mechanisms and build the recurrent neural architecture with Bidirectional Long Short-Term Memory (LSTM) networks. Entity-aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by LET. The contributions of our work are summarized as follows: (1) We propose an novel end-to-end recurrent neural model and an entity-aware attention mechanism with a LET which focuses to semantic information of entities and their latent types; (2) Our model obtains 85.2% F1-score in SemEval-2010 Task 8 and it outper1It is one of the pre-de\ufb01ned relation classes in the SemEval2010 Task 8 [6]. arXiv:1901.08163v1  [cs.CL]  23 Jan 2019 ", "conclusion": "Conclusion In this paper, we proposed entity-aware attention mechanism with latent entity typing and a novel end-to-end recurrent neural model which incorporates this mechanism for relation classi\ufb01cation. Our model achieves 85.2% F1-score in SemEval-2010 Task 8 using only raw sentence and word embeddings without any high-level features from NLP tools and it outperforms existing state-of-the-art methods. In addition, our three visualizations of attention mechanisms applied to the model demonstrate that our model is more interpretable than previous models. We expect our model to be extended not only the relation classi\ufb01cation task but also other tasks that entity plays an important role. Especially, latent entity typing can be e\ufb00ectively applied to sequence modeling task using entity information without NER. In the future, we will propose a new method in question answering or knowledge base population based on relations between entities extracted from our model. ", "full_text": "Semantic Relation Classi\ufb01cation via Bidirectional LSTM Networks with\nEntity-aware Attention using Latent Entity Typing\nJoohong Lee\u2217\nSangwoo Seo\u2217\nYong Suk Choi\u2217\u2020\nAbstract\nClassifying semantic relations between entity pairs in sen-\ntences is an important task in Natural Language Processing\n(NLP). Most previous models for relation classi\ufb01cation rely\non the high-level lexical and syntatic features obtained by\nNLP tools such as WordNet, dependency parser, part-of-\nspeech (POS) tagger, and named entity recognizers (NER).\nIn addition, state-of-the-art neural models based on atten-\ntion mechanisms do not fully utilize information of entity\nthat may be the most crucial features for relation classi\ufb01ca-\ntion. To address these issues, we propose a novel end-to-end\nrecurrent neural model which incorporates an entity-aware\nattention mechanism with a latent entity typing (LET)\nmethod. Our model not only utilizes entities and their latent\ntypes as features e\ufb00ectively but also is more interpretable by\nvisualizing attention mechanisms applied to our model and\nresults of LET. Experimental results on the SemEval-2010\nTask 8, one of the most popular relation classi\ufb01cation task,\ndemonstrate that our model outperforms existing state-of-\nthe-art models without any high-level features.\n1\nIntroduction\nClassifying semantic relations between entity pairs in\nsentences plays a vital role in various NLP tasks, such as\ninformation extraction, question answering and knowl-\nedge base population [14]. A task of relation classi\ufb01-\ncation is de\ufb01ned as predicting a semantic relationship\nbetween two tagged entities in a sentence.\nFigure 1: A Sample of Relation Classi\ufb01cation.\nFor example, given a sentence with tagged entity pair,\ncrash and attack, this sentence is classi\ufb01ed into the re-\n\u2217Department of Computer Science and Engineering, Hanyang\nUniversity,\nSeoul,\nRepublic\nof\nKorea,\n{roomylee,\nssw1591,\ncys}@hanyang.ac.kr\n\u2020Corresponding author.\nlation Cause-E\ufb00ect(e1,e2)1 between the entity pair like\nFigure 1. A \ufb01rst entity is surrounded by \u27e8e1\u27e9 and \u27e8/e1\u27e9,\nand a second entity is surrounded by \u27e8e2\u27e9 and \u27e8/e2\u27e9.\nMost previous relation classi\ufb01cation models rely\nheavily on high-level lexical and syntactic features ob-\ntained from NLP tools such as WordNet, dependency\nparser, part-of-speech (POS) tagger, and named entity\nrecognizer (NER). The classi\ufb01cation models relying on\nsuch features su\ufb00er from propagation of implicit error\nof the tools and they are computationally expensive.\nRecently, many studies therefore propose end-to-\nend neural models without the high-level features.\nAmong them, attention-based models, which focus to\nthe most important semantic information in a sentence,\nshow state-of-the-art results in a lot of NLP tasks. Since\nthese models are mainly proposed for solving translation\nand language modeling tasks, they could not fully utilize\nthe information of tagged entities in relation classi\ufb01ca-\ntion task. However, tagged entity pairs could be pow-\nerful hints for solving relation classi\ufb01cation task. For\nexample, even if we do not consider other words except\nthe crash and attack, we intuitively know that the en-\ntity pair has a relation Cause-E\ufb00ect(e1,e2)1 better than\nComponent-Whole(e1,e2)1 in Figure 1\nTo address these issues, We propose a novel end-\nto-end recurrent neural model which incorporates an\nentity-aware attention mechanism with a latent entity\ntyping (LET). To capture the context of sentences, We\nobtain word representations by self attention mecha-\nnisms and build the recurrent neural architecture with\nBidirectional Long Short-Term Memory (LSTM) net-\nworks. Entity-aware attention focuses on the most im-\nportant semantic information considering entity pairs\nwith word positions relative to these pairs and latent\ntypes obtained by LET.\nThe contributions of our work are summarized as\nfollows: (1) We propose an novel end-to-end recurrent\nneural model and an entity-aware attention mechanism\nwith a LET which focuses to semantic information of\nentities and their latent types; (2) Our model obtains\n85.2% F1-score in SemEval-2010 Task 8 and it outper-\n1It is one of the pre-de\ufb01ned relation classes in the SemEval-\n2010 Task 8 [6].\narXiv:1901.08163v1  [cs.CL]  23 Jan 2019\n Figure 2: The architecture of our model (best viewed in color). Entity 1 and 2 corresponds to the 3 and (n\u22121)-th\nwords, respectively, which are fed into the LET.\nforms existing state-of-the-art models without any high-\nlevel features; (3) We show that our model is more inter-\npretable since it\u2019s decision making process could be vi-\nsualized with self attention, entity-aware attention, and\nLET.\n2\nRelated Work\nThere are several studies for solving relation classi\ufb01-\ncation task. Early methods used handcrafted features\nthrough a series of NLP tools or manually designing ker-\nnels [16]. These approaches use high-level lexical and\nsyntactic features obtained from NLP tools and manu-\nally designing kernels, but the classi\ufb01cation models rely-\ning on such features su\ufb00er from propagation of implicit\nerror of the tools.\nOn the other hands, deep neural networks have\nshown outperform previous models using handcraft fea-\ntures.\nEspecially, many researches tried to solve the\nproblem based on end-to-end models using only raw sen-\ntences and pre-trained word representations learned by\nSkip-gram and Continuous Bag-of-Words [12, 11, 15].\nZeng et al. employed a deep convolutional neural net-\nwork (CNN) for extracting lexical and sentence level fea-\ntures [30]. Dos Santos et al. proposed model for learning\nvector of each relation class using ranking loss to reduce\nthe impact of arti\ufb01cial classes [2]. Zhang and Wang used\nbidirectional recurrent neural network (RNN) to learn\nlong-term dependency between entity pairs [31]. Fur-\nthermore, Zhang et al. proposed bidirectional LSTM\nnetwork (BLSTM) utilizing position of words, POS tags,\nnamed entity information, dependency parse [32]. This\nmodel resolved vanishing gradient problem appeared in\nRNNs by using BLSTM.\nRecently, some researcher have proposed attention-\nbased models which can focus to the most important\nsemantic information in a sentence. Zhou et al. com-\nbined attention mechanisms with BLSTM [34].\nXiao\nand Liu split the sentence into two entities and used\ntwo attention-based BLSTM hierarchically [21]. Shen\nand Huang proposed attention-based CNN using word\nlevel attention mechanism that is able to better deter-\nmine which parts of the sentence are more in\ufb02uential\n[8].\nIn contrast with end-to-end model, several works\nproposed models utilizing the shortest dependency path\n(SDP) between entity pairs of dependency parse trees.\nSDP-LSTM model proposed by Yan et al.\nand deep\nrecurrent neural networks (DRNNs) model proposed\nby Xu et al eliminate irrelevant words out of SDP\nand use neural network based on the meaningful words\ncomposing SDP [24, 23].\n3\nModel\nIn this section, we introduce a novel recurrent neural\nmodel that incorporate an entity-aware attention mech-\nanism with a LET method in detail. As shown in Fig-\n ure 2, our model consists of four main components: (1)\nWord Representation that maps each word in a sen-\ntence into vector representations; (2) Self Attention\nthat captures the meaning of the correlation between\nwords based on multi-head attention [20]; (3) BLSTM\nwhich sequentially encodes the representations of self at-\ntention layer; (4) Entity-aware Attention that calcu-\nlates attention weights with respect to the entity pairs,\nword positions relative to these pairs, and their latent\ntypes obtained by LET. After that, the features are av-\neraged along the time steps to produce the sentence-\nlevel features.\n3.1\nWord Representation Let a input sentence\nis denoted by S = {w1, w2, ..., wn}, where n is the\nnumber of words. We transform each word into vector\nrepresentations by looking up word embedding matrix\nWword \u2208 Rdw\u00d7|V |, where dw is the dimension of the\nvector and |V | is the size of vocabulary. Then the word\nrepresentations X = {x1, x2, ..., xn} are obtained by\nmapping wi, the i-th word, to a column vector xi \u2208 Rdw\nare fed into the next layer.\n3.2\nSelf Attention The word representations are\n\ufb01xed for each word, even though meanings of words\nvary depending on the context.\nMany neural models\nencoding sequence of words may expect to learn implic-\nitly of the contextual meaning, but they may not learn\nwell because of the long-term dependency problems [1].\nIn order for the representation vectors to capture the\nmeaning of words considering the context, we employ\nthe self attention, a special case of attention mechanism,\nthat only requires a single sequence. Self attention has\nbeen successfully applied to various NLP tasks such as\nmachine translation, language understanding, and se-\nmantic role labeling [20, 17, 19].\nWe adopt the multi-head attention formulation [20],\none of the methods for implementing self attentions.\nFigure 3 illustrates the multi-head attention mechanism\nthat consists of several linear transformations and scaled\ndot-product attention corresponding to the center block\nof the \ufb01gure.\nGiven a matrix of n vectors, query Q,\nkey K, and value V , the scaled dot-product attention is\ncalculated by the following equation:\n(3.1)\nAttention(Q, K, V ) = softmax(QK\u22a4\n\u221adw\n)V\nIn multi-head attention, the scaled dot-product atten-\ntion with linear transformations is performed on r par-\nallel heads to pay attention to di\ufb00erent parts.\nThen\nformulation of multi-head attention is de\ufb01ned by the\nFigure 3: Multi-Head Self Attention. For self attention,\nthe Q(query), K(key), and V (value), inputs of multi-\nhead attention, should be the same vectors.\nIn our\nwork, they are equivalent to X, the word representation\nvectors.\nfollows:\n(3.2)\nMultiHead(Q, K, V ) = W M[head1; ...; headr]\n(3.3)\nheadi = Attention(W Q\ni Q, W K\ni K, W V\ni V )\nwhere [;] indicates row concatenation and r is the num-\nber of heads.\nThe weights W M \u2208 Rdw\u00d7dw, W Q\ni\n\u2208\nRdw/r\u00d7dw, W K\ni\n\u2208 Rdw/r\u00d7dw, and W V\ni\n\u2208 Rdw/r\u00d7dw are\nlearnable parameter for linear transformation. W M is\nfor concatenation outputs of scaled dot-product atten-\ntion and the others are for query, key, value of i-th head\nrespectively.\nBecause our work requires self attention, the input\nmatrices of multi-head attention, Q, K, and V are all\nequivalent to X, the word representation vectors. As a\nresult, outputs of multi-head attention are denoted by\nM = {m1, m2, ..., mn} = MultiHead(X, X, X), where\nmi is the output vector corresponding to i-th word.\nThe output of self attention layer is the sequence of\nrepresentations whose include informative factors in the\ninput sentence.\n 3.3\nBidirectional LSTM Network For sequen-\ntially encoding the output of self attention layer, we use\na BLSTM [5, 4] that consists of two sub LSTM networks:\na forward LSTM network which encodes the context of\na input sentence and a backward LSTM network which\nencodes that one of the reverse sentence. More formally,\nBLSTM works as follows:\n(3.4)\n\u2212\u2192\nht = \u2212\u2212\u2212\u2212\u2192\nLSTM(mt)\n(3.5)\n\u2190\u2212\nht = \u2190\u2212\u2212\u2212\u2212\nLSTM(mt)\n(3.6)\nht = [\u2212\u2192\nht; \u2190\u2212\nht]\nThe representation vectors M obtained from self atten-\ntion layer are forwarded into to the network step by\nstep. At the time step t, the hidden state ht \u2208 R2dh of a\nBLSTM is obtained by concatenating \u2212\u2192\nht \u2208 Rdh, the hid-\nden state of forward LSTM network, and \u2190\u2212\nht \u2208 Rdh, the\nbackward one, where dh is dimension of each LSTM\u2019s\nstate.\n(3.7)\n\u2212\u2192\nht \u2208 Rdh \u2190\u2212\nht \u2208 Rdh\n3.4\nEntity-aware\nAttention\nMechanism Al-\nthough\nmany\nmodels\nwith\nattention\nmechanism\nachieved state-of-the-art performance in many NLP\ntasks.\nHowever, for the relation classi\ufb01cation task,\nthese models lack of prior knowledge for given entity\npairs, which could be powerful hints for solving the\ntask.\nRelation classi\ufb01cation di\ufb00ers from sentence\nclassi\ufb01cation in that information about entities is given\nalong with sentences.\nWe propose a novel entity-aware attention mecha-\nnism for fully utilizing informative factors in given en-\ntity pairs. Entity-aware attention utilizes the two addi-\ntional features except H = {h1, h2, ..., hn}, (1) relative\nposition features, (2) entity features with LET, and the\n\ufb01nal sentence representation z, result of the attention,\nis computed as follows:\n(3.8) ui = tanh(W H[hi; pe1\ni ; pe2\ni ] + W E[he1; t1; he2; t2])\n(3.9)\n\u03b1i =\nexp(v\u22a4ui)\n\ufffdn\nj=1 exp(v\u22a4uj)\n(3.10)\nz =\nn\n\ufffd\ni=1\n\u03b1ihi\n3.4.1\nRelative Position Features In relation clas-\nsi\ufb01cation, the position of each word relative to entities\nhas been widely used for word representations [30, 14, 8].\nRecently, position-aware attention is published as a way\nto use the relative position features more e\ufb00ectively [33].\nIt is a variant of attention mechanisms, which use not\nonly outputs of BLSTM but also the relative position\nfeatures when calculating attention weights.\nWe adopt this method with slightly modi\ufb01cation\nas shown in Equation 3.8. In the equation, pe1\ni\n\u2208 Rdp\nand pe2\ni\n\u2208 Rdp corresponds to the position of the i-th\nword relative to the \ufb01rst entity (e1-th word) and sec-\nond entity (e2-th word) in a sentence respectively, where\nej\u2208{1,2} is a index of j-th entity. Similar to word em-\nbeddings, the relative positions are converted to vector\nrepresentations by looking up learnable embedding ma-\ntrix Wpos \u2208 Rdp\u00d7(2L\u22121), where dp is the dimension of\nthe relative position vectors and L is the maximum sen-\ntence length.\nFinally, the representations of BLSTM layer take\ninto account the context and the positional relation-\nship with entities by concatenating hi, pe1\ni , and pe2\ni .\nThe representation is linearly transformed by W H \u2208\nRda\u00d7(2dh+2dp) as in the Equation 3.8.\n3.4.2\nEntity Features with Latent Type Since\nentity pairs are powerful hints for solving relation classi-\n\ufb01cation task, we involve the entity pairs and their types\nin the attention mechanism to e\ufb00ectively train relations\nbetween entity pairs and other words in a sentence. We\nemploy the two entity-aware features. The \ufb01rst is the\nhidden states of BLSTM corresponding to positions of\nentity pairs, which are high-level features representing\nentities. These are denoted by hei \u2208 R2dh, where ei is\nindex of i-th entity.\nIn addition, latent types of the entities obtained\nby LET, our proposed novel method, are the second\none.\nUsing types as features can be a great way to\nimprove performance, since the types of entities alone\ncan be inferred the approximate relations.\nBecause\nthe annotated types are not given, we use the latent\ntype representations by applying the LET inspired by\nlatent topic clustering, a method for predicting latent\ntopic of texts in question answering task [26].\nThe\nLET constructs the type representations by weighting\nK latent type vectors based on attention mechanisms.\nThe mathematical formulation is the follows:\n(3.11)\naj\ni =\nexp((hej)\u22a4ci)\n\ufffdK\nk=1 exp((hej)\u22a4ck)\n(3.12)\ntj\u2208{1,2} =\nK\n\ufffd\ni=1\naj\nici\n where ci is the i-th latent type vector and K is the\nnumber of latent entity types.\nAs a result, entity features are constructed by con-\ncatenating the hidden states corresponding entity po-\nsitions and types of entity pairs.\nAfter linear trans-\nformation of the entity features, they add up with the\nrepresentations of BLSTM layer as in Equation 3.8, and\nthe representation of sentence z \u2208 R2dh is computed by\nEquations from 3.8 to 3.10.\n3.5\nClassi\ufb01cation and Training The sentence rep-\nresentation obtained from the entity-aware attention z\nis fed into a fully connected softmax layer for classi\ufb01-\ncation. It produces the conditional probability p(y|S, \u03b8)\nover all relation types:\n(3.13)\np(y|S, \u03b8) = softmax(W Oz + bO)\nwhere y is a target relation class and S is the input\nsentence. The \u03b8 is whole learnable parameters in the\nwhole network including W O \u2208 R|R|\u00d72dh, bO \u2208 R|R|,\nwhere |R| is the number of relation classes.\nA loss\nfunction L is the cross entropy between the predictions\nand the ground truths, which is de\ufb01ned as:\n(3.14)\nL = \u2212\n|D|\n\ufffd\ni=1\nlog p(y(i)|S(i), \u03b8) + \u03bb||\u03b8||2\n2\nwhere |D| is the size of training dataset and (S(i), y(i))\nis the i-th sample in the dataset.\nWe minimize the\nloss L using AdaDelta optimizer [29] to compute the\nparameters \u03b8 of our model.\nTo alleviate over\ufb01tting, we constrain the L2 reg-\nularization with the coe\ufb03cient \u03bb [13].\nIn addition,\nthe dropout method is applied after word embedding,\nLSTM network, and entity-aware attention to prevent\nco-adaptation of hidden units by randomly omitting fea-\nture detectors [7, 28].\n4\nExperiments\n4.1\nDataset and Evaluation Metrics We eval-\nuate\nour\nmodel\non\nthe\nSemEval-2010\nTask\n8\ndataset,\nwhich\nis\nan\ncommonly\nused\nbenchmark\nfor relation classi\ufb01cation [6] and compare the re-\nsults with the state-of-the-art models in this area.\nThe\ndataset\ncontains\n10\ndistinguished\nrelations,\nCause-E\ufb00ect,\nInstrument-Agency,\nProduct-Producer,\nContent-Container, Entity-Origin, Entity-Destination,\nComponent-Whole, Member-Collection, Message-Topic,\nand Other. The former 9 relations have two directions,\nwhereas Other is not directional, so the total number\nof relations is 19. There are 10,717 annotated sentences\nwhich consist of 8,000 samples for training and 2,717\nsamples for testing.\nWe adopt the o\ufb03cial evaluation\nmetric of SemEval-2010 Task 8, which is based on the\nmacro-averaged F1-score (excluding Other), and takes\ninto consideration the directionality.\n4.2\nImplementation Details We tune the hyper-\nparameters for our model on the development set ran-\ndomly sampled 800 sentences for validation. The best\nhyperparameters in our proposed model are shown in\nfollowing Table 1.\nHyper-\nparameter\nDescription\nValue\ndw\nSize of Word Embeddings\n300\nr\nNumber of Heads\n4\ndh\nSize of Hidden Layer\n300\ndp\nSize of Position Embeddings\n50\nda\nSize of Attention Layer\n50\nK\nNumber of Latent Entity Types\n3\nbatch_size\nSize of Mini-Batch\n20\n\u03b7\nInitial Learning Rate\n1.0\ndropout\nrate\nWord Embedding layer\n0.3\nBLSTM layer\n0.3\nEntity-aware Attention layer\n0.5\n\u03bb\nL2 Regularization Coe\ufb03cient\n10\u22125\nTable 1: Hyperparameters.\nWe use pre-trained weights of the publicly available\nGloVe model [15] to initialize word embeddings in our\nmodel, and other weights are randomly initialized from\nzero-mean Gaussian distribution [3].\n4.3\nExperimental Results Table 2 compares our\nEntity-aware Attention LSTM model with state-of-the-\nart models on this relation classi\ufb01cation dataset. We\ndivide the models into three groups, Non-Neural Model,\nSDP-based Model, and End-to-End Model.\nFirst, the\nSVM [16], Non-Neural Model, was top of the SemEval-\n2010 task, during the o\ufb03cial competition period. They\nused many handcraft feature and SVM classi\ufb01er.\nAs\na result, they achieved an F1-score of 82.2%.\nThe\nsecond is SDP-based Model such as MVRNN [18], FCM\n[27], DepNN [9], depLCNN+NS [22], SDP-LSTM [24],\nand DRNNs [23]. The SDP is reasonable features for\ndetecting semantic structure of sentences.\nActually,\nthe SDP-based models show high performance, but\nSDP may not always be accurate and the parsing\ntime is exponentially increased by long sentences. The\nlast model is End-to-End Model automatically learned\ninternal representations can occur between the original\n inputs and the \ufb01nal outputs in deep learning. There are\nCNN-based models such as CNN [30, 14], CR-CNN [2],\nand Attention-CNN [8] and RNN-based models such as\nBLSTM [32], Attention-BLSTM [34], and Hierarchical-\nBLSTM (Hier-BLSTM) [25] for this task.\nModel\nF1\nNon-Neural\nModel\nSVM\n82.2\nSDP-based\nModel\nMVRNN\n82.4\nFCM\n83.0\nDepNN\n83.6\ndepLCNN+NS\n85.6\nSDP-LSTM\n83.7\nDRNNs\n86.1\nEnd-to-End\nModel\nCNN\n82.7\nCR-CNN\n84.1\nAttention-CNN\n84.3\n+ POS, WN, WAN\n85.9\nBLSTM\n82.7\n+ PF, POS, NER, DEP, WN\n84.3\nAttention-BLSTM\n84.0\nHier-BLSTM\n84.3\nOur Model\n84.7\n+ Latent Entity Typing\n85.2\nTable\n2:\nComparison\nwith\nPrevious\nResults\non\nSemEval-2010 Task 8 dataset, where the WN, WAN,\nPF, and DEP are WordNet (hypernyms), words around\nnominals, position features, and dependency features,\nrespectively.\nOur\nproposed\nmodel\nachieves\nan\nF1-score\nof\n85.2% which outperforms all competing state-of-the-\nart approaches except depLCNN+NS, DRNNs, and\nAttention-CNN. However, they rely on high-level lex-\nical features such as WordNet, dependency parse trees,\nPOS tags, and NER tags from NLP tools.\nThe experimental results show that the LET is\ne\ufb00ective for relation classi\ufb01cation. The LET improve a\nperformance of 0.5% than the model not applied it. The\nmodel showed the best performance with three types.\n5\nVisualization\nThere are three di\ufb00erent visualization to demonstrate\nthat our model is more interpretable. First, the visual-\nization of self attention shows where each word focus on\nparts of a sentence. By showing the words that the en-\ntity pair attends, we can \ufb01nd the words that well repre-\nsent the relation between them. Next, the entity-aware\nattention visualization shows where the model pays at-\ntend to a sentence. This visualization result highlights\nimportant words in a sentence, which are usually im-\nportant keywords for classi\ufb01cation. Finally, we visual-\nize representation of type in LET by using t-SNE [10],\na method for dimensionality reduction, and group the\nwhole entities in the dataset by the its latent types.\n5.1\nSelf Attention We can obtain the richer word\nrepresentations by using self attentions.\nThese word\nrepresentations are considered the context based on\ncorrelation between words in a sentence.\nThe Fig-\nure 4 illustrates the results of the self attention\nin the sentence,\n\u201cthe \u2329e1\u232apollution\u2329/e1\u232awas caused\nby the \u2329e2\u232ashipwrek\u2329/e2\u232a\u201d, which is labeled Cause-\nE\ufb00ect(e1,e2). There are visualizations of the two heads\nin the multi-head attention applied for self attention.\nThe color density indicates the attention values, results\nof Equation 3.1, which means how much an entity fo-\ncuses on each word in a sentence.\nFigure 4: Visualization of Self Attention.\nIn Figure 4, the left represents the words that\npollution, the \ufb01rst entity, focuses on and the right\nrepresents the words that shipwreck, the second entity,\nfocuses on.\nWe can recognize that the entity pair is\ncommonly concentrated on was, caused, and each other.\nActually, these words play the most important role in\nsemantically predicting the Cause-E\ufb00ect(e1,e2), which\nis the relation class of this entity pair.\n5.2\nEntity-aware Attention Figure 5 shows where\nthe model focuses on the sentence to compute relations\nbetween entity pairs, which is the result of visualizing\nthe alpha vectors in Equation 3.9. The important words\nin sentence are highlighted in yellow, which means that\nthe more clearly the color is, the more important it is.\nFor example, in the \ufb01rst sentence, the inside is strongly\nhighlighted, which is actually the best word representing\nthe relation Component-whole(e1,e2) between the given\nentity pair. As another example, in the third sentence,\nthe highlighted assess and using represent the relation,\n Figure 5: Visualization of Entity-aware Attention\nInstrument-Agency(e2,e1) between entity pair, analysts\nand frequency, well.\nWe can see that the using is\nmore highlighted than the assess, because the former\nrepresents the relation better.\nFigure 6: Visualization of latent type representations\nusing t-SNE\n5.3\nLatent Entity Type Figure 6 visualizes latent\ntype representation tj\u2208{1,2} in Equation 3.12 Since the\ndimensionality of representation vectors are too large to\nvisualize, we applied the t-SNE, one of the most popular\ndimensionality reduction methods. In Figure 6, the red\npoints represent latent type vectors ci\u2208K and the rests\nare latent type representations tj, where the colors of\npoints are determined by the closest of the latent type\nvectors in the vector space of the original dimensionality.\nThe points are generally well divided and are almost\nuniformly distributed without being biased to one side.\nFigure 7 summarizes the results of extracting 50\nentities in close order with each latent type vector. This\nallows us to roughly understand what latent types of\nentities are.\nWe use a total of three types and \ufb01nd\nthat similar characteristics appear in words grouped\nby together.\nIn the type 1, the words are related to\nhuman\u2019s jobs and foods. The type2 has a lot of entities\nrelated to machines and engineering like engine, woofer,\nand motor.\nFinally, in type3, there are many words\nwith bad meanings related associated with disasters and\nFigure 7: Sets of Entities grouped by Latent Types\ndrugs. As a result, each type has a set of words with\nsimilar characteristics, which can prove that LET works\ne\ufb00ectively.\n6\nConclusion\nIn this paper, we proposed entity-aware attention mech-\nanism with latent entity typing and a novel end-to-end\nrecurrent neural model which incorporates this mech-\nanism for relation classi\ufb01cation.\nOur model achieves\n85.2% F1-score in SemEval-2010 Task 8 using only raw\nsentence and word embeddings without any high-level\nfeatures from NLP tools and it outperforms existing\nstate-of-the-art methods. In addition, our three visual-\nizations of attention mechanisms applied to the model\ndemonstrate that our model is more interpretable than\nprevious models. We expect our model to be extended\nnot only the relation classi\ufb01cation task but also other\ntasks that entity plays an important role. Especially, la-\ntent entity typing can be e\ufb00ectively applied to sequence\nmodeling task using entity information without NER.\nIn the future, we will propose a new method in ques-\ntion answering or knowledge base population based on\nrelations between entities extracted from our model.\nReferences\n[1] Y. Bengio, P. Simard, and P. Frasconi, Learning\nlong-term dependencies with gradient descent is di\ufb03-\ncult, IEEE transactions on neural networks, 5 (1994),\npp. 157\u2013166.\n [2] C. dos Santos, B. Xiang, and B. Zhou, Clas-\nsifying relations by ranking with convolutional neural\nnetworks, in Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), vol. 1,\n2015, pp. 626\u2013634.\n[3] X. Glorot and Y. Bengio, Understanding the dif-\n\ufb01culty of training deep feedforward neural networks, in\nProceedings of the thirteenth international conference\non arti\ufb01cial intelligence and statistics, 2010, pp. 249\u2013\n256.\n[4] A. Graves,\nA.-r. Mohamed,\nand G. Hinton,\nSpeech recognition with deep recurrent neural networks,\nin Acoustics, speech and signal processing (icassp),\n2013 ieee international conference on, IEEE, 2013,\npp. 6645\u20136649.\n[5] A.\nGraves\nand\nJ.\nSchmidhuber,\nFramewise\nphoneme classi\ufb01cation with bidirectional lstm and other\nneural network architectures,\nNeural Networks,\n18\n(2005), pp. 602\u2013610.\n[6] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov,\nD. \u00d3 S\u00e9aghdha, S. Pad\u00f3, M. Pennacchiotti,\nL. Romano, and S. Szpakowicz, Semeval-2010 task\n8:\nMulti-way classi\ufb01cation of semantic relations be-\ntween pairs of nominals, in Proceedings of the Work-\nshop on Semantic Evaluations: Recent Achievements\nand Future Directions, Association for Computational\nLinguistics, 2009, pp. 94\u201399.\n[7] G. E. Hinton, N. Srivastava, A. Krizhevsky,\nI. Sutskever, and R. R. Salakhutdinov, Improv-\ning neural networks by preventing co-adaptation of fea-\nture detectors, arXiv preprint arXiv:1207.0580, (2012).\n[8] X. Huang et al., Attention-based convolutional neu-\nral network for semantic relation extraction, in Pro-\nceedings of COLING 2016, the 26th International Con-\nference on Computational Linguistics: Technical Pa-\npers, 2016, pp. 2526\u20132536.\n[9] Y. Liu, F. Wei, S. Li, H. Ji, M. Zhou, and\nH. Wang, A dependency-based neural network for re-\nlation classi\ufb01cation, arXiv preprint arXiv:1507.04646,\n(2015).\n[10] L. v. d. Maaten and G. Hinton, Visualizing data\nusing t-sne, Journal of machine learning research, 9\n(2008), pp. 2579\u20132605.\n[11] T. Mikolov, K. Chen, G. Corrado, and J. Dean,\nE\ufb03cient estimation of word representations in vector\nspace, arXiv preprint arXiv:1301.3781, (2013).\n[12] T. Mikolov, I. Sutskever, K. Chen, G. S. Cor-\nrado, and J. Dean, Distributed representations of\nwords and phrases and their compositionality, in Ad-\nvances in neural information processing systems, 2013,\npp. 3111\u20133119.\n[13] A. Y. Ng, Feature selection, l 1 vs. l 2 regulariza-\ntion, and rotational invariance, in Proceedings of the\ntwenty-\ufb01rst international conference on Machine learn-\ning, ACM, 2004, p. 78.\n[14] T. H. Nguyen and R. Grishman, Relation ex-\ntraction:\nPerspective from convolutional neural net-\nworks, in Proceedings of the NAACL Workshop on Vec-\ntor Space Modeling for Natural Language Processing,\n2015, pp. 39\u201348.\n[15] J. Pennington, R. Socher, and C. Manning,\nGlove: Global vectors for word representation, in Pro-\nceedings of the 2014 conference on empirical meth-\nods in natural language processing (EMNLP), 2014,\npp. 1532\u20131543.\n[16] B. Rink and S. Harabagiu, Utd:\nClassifying se-\nmantic relations by combining lexical and semantic re-\nsources, in Proceedings of the 5th International Work-\nshop on Semantic Evaluation, Association for Compu-\ntational Linguistics, 2010, pp. 256\u2013259.\n[17] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan,\nand C. Zhang, Disan: Directional self-attention net-\nwork for rnn/cnn-free language understanding, arXiv\npreprint arXiv:1709.04696, (2017).\n[18] R. Socher,\nB. Huval,\nC. D. Manning,\nand\nA. Y. Ng, Semantic compositionality through recursive\nmatrix-vector spaces, in Proceedings of the 2012 joint\nconference on empirical methods in natural language\nprocessing and computational natural language learn-\ning, Association for Computational Linguistics, 2012,\npp. 1201\u20131211.\n[19] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi,\nDeep semantic role labeling with self-attention, arXiv\npreprint arXiv:1712.01586, (2017).\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin, Attention is all you need, in Ad-\nvances in Neural Information Processing Systems,\n2017, pp. 5998\u20136008.\n[21] M. Xiao and C. Liu, Semantic relation classi\ufb01cation\nvia hierarchical recurrent neural network with atten-\ntion, in Proceedings of COLING 2016, the 26th In-\nternational Conference on Computational Linguistics:\nTechnical Papers, 2016, pp. 1254\u20131263.\n[22] K. Xu, Y. Feng, S. Huang, and D. Zhao, Se-\nmantic relation classi\ufb01cation via convolutional neural\nnetworks with simple negative sampling, arXiv preprint\narXiv:1506.07650, (2015).\n[23] Y. Xu, R. Jia, L. Mou, G. Li, Y. Chen, Y. Lu,\nand Z. Jin, Improved relation classi\ufb01cation by deep re-\ncurrent neural networks with data augmentation, arXiv\npreprint arXiv:1601.03651, (2016).\n[24] Y. Xu, L. Mou, G. Li, Y. Chen, H. Peng, and\nZ. Jin, Classifying relations via long short term mem-\nory networks along shortest dependency paths, in Pro-\nceedings of the 2015 conference on empirical methods\nin natural language processing, 2015, pp. 1785\u20131794.\n[25] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola,\nand E. Hovy, Hierarchical attention networks for\ndocument classi\ufb01cation, in Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics:\nHuman\nLanguage Technologies, 2016, pp. 1480\u20131489.\n[26] S. Yoon, J. Shin, and K. Jung, Learning to rank\n question-answer pairs using hierarchical recurrent en-\ncoder with latent topic clustering, in Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), Asso-\nciation for Computational Linguistics, 2018, pp. 1575\u2013\n1584.\n[27] M. Yu, M. Gormley, and M. Dredze, Factor-based\ncompositional embedding models, in NIPS Workshop on\nLearning Semantics, 2014, pp. 95\u2013101.\n[28] W. Zaremba,\nI. Sutskever,\nand O. Vinyals,\nRecurrent neural network regularization, arXiv preprint\narXiv:1409.2329, (2014).\n[29] M. D. Zeiler, Adadelta: an adaptive learning rate\nmethod, arXiv preprint arXiv:1212.5701, (2012).\n[30] D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao,\nRelation classi\ufb01cation via convolutional deep neural\nnetwork, in Proceedings of COLING 2014, the 25th In-\nternational Conference on Computational Linguistics:\nTechnical Papers, 2014, pp. 2335\u20132344.\n[31] D.\nZhang\nand\nD.\nWang,\nRelation\nclassi\ufb01ca-\ntion via recurrent neural network,\narXiv preprint\narXiv:1508.01006, (2015).\n[32] S. Zhang, D. Zheng, X. Hu, and M. Yang, Bidi-\nrectional long short-term memory networks for relation\nclassi\ufb01cation, in Proceedings of the 29th Paci\ufb01c Asia\nConference on Language, Information and Computa-\ntion, 2015, pp. 73\u201378.\n[33] Y. Zhang, V. Zhong, D. Chen, G. Angeli, and\nC. D. Manning, Position-aware attention and super-\nvised data improve slot \ufb01lling, in Proceedings of the\n2017 Conference on Empirical Methods in Natural Lan-\nguage Processing, 2017, pp. 35\u201345.\n[34] P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao,\nand B. Xu, Attention-based bidirectional long short-\nterm memory networks for relation classi\ufb01cation, in\nProceedings of the 54th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 2: Short\nPapers), vol. 2, 2016, pp. 207\u2013212.\n"}, "An Evaluation of State-of-the-Art Large Language Models for Sarcasm Detection": {"authors": ["Juliann Zhou"], "title": "An Evaluation of State-of-the-Art Large Language Models for Sarcasm Detection", "url": "https://arxiv.org/pdf/2312.03706.pdf", "abstract": "Sarcasm, as defined by Merriam-Webster, is the use of words by someone who means the opposite of what he is trying to say. In the field of sentimental analysis of Natural Language Processing, the ability to correctly identify sarcasm is necessary for understanding people's true opinions. Because the use of sarcasm is often context-based, previous research has used language representation models, such as Support Vector Machine (SVM) and Long Short-Term Memory (LSTM), to identify sarcasm with contextual-based information. Recent innovations in NLP have provided more possibilities for detecting sarcasm. In BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin et al. (2018) introduced a new language representation model and demonstrated higher precision in interpreting contextualized language. As proposed by Hazarika et al. (2018), CASCADE is a context-driven model that produces good results for detecting sarcasm. This study analyzes a Reddit corpus using these two state-of-the-art models and evaluates their performance against baseline models to find the ideal approach to sarcasm detection.", "arxiv_id": "2312.03706", "published_date": "2023-10-07", "year": 2023, "introduction": "", "conclusion": "", "full_text": " \n \n1 \nAn Evaluation of State-of-the-Art Large Language \nModels for Sarcasm Detection  \nJuliann Zhou \nNew York University, New York, NY 10002 USA  \ne-mail: kyz224@nyu.edu \n \nABSTRACT Sarcasm, as defined by Merriam-Webster, is the use of words by someone who means the opposite of what he \nis trying to say. In the field of sentimental analysis of Natural Language Processing, the ability to correctly identify sarcasm is \nnecessary for understanding people's true opinions. Because the use of sarcasm is often context-based, previous research has \nused language representation models, such as Support Vector Machine (SVM) and Long Short-Term Memory (LSTM), to \nidentify sarcasm with contextual-based information. Recent innovations in NLP have provided more possibilities for detecting \nsarcasm. In BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin et al. (2018) \nintroduced a new language representation model and demonstrated higher precision in interpreting contextualized language. \nAs proposed by Hazarika et al. (2018), CASCADE is a context-driven model that produces good results for detecting sarcasm. \nThis study analyzes a Reddit corpus using these two state-of-the-art models and evaluates their performance against baseline \nmodels to find the ideal approach to sarcasm detection. \n \nKeywords BERT, CASCADE, language representation model, NLP, sarcasm, sarcasm detection, \ntransformer \n1. \nINTRODUCTION \nSarcasm refers to words that are opposite to their surface \nmeanings. Commonly used in delivering sneering and \nmocking remarks, the word sarcasm comes from the ancient \nGreek word sark\u00e1zein, meaning \u201cto tear flesh\u201d. It is \nsometimes used in a humorous or ambivalent manner and is \nlargely context dependent. Identifying sarcasm is a \nchallenging task in NLP, because the meaning of the words \nspoken opposes the speaker\u2019s true subjective opinion. The \ncontextual nature of sarcasm makes detection even more \ndifficult because it requires an analysis of the speaker\u2019s prior \nknowledge, tone, and intention.   \n    Most computational models for detecting sarcasm rely on \nthe content of the utterances in isolation. However, the \nspeaker\u2019s intention may not be obvious without additional \ncontext. For example, the sentence \u2018I love solving math \nproblems all weekend\u2019 may be sarcastic to many students but \nmay not be so for a student who loves math. Hence, the \ncontext of a text is crucial for identifying sarcasms. This \nstudy investigated the use of contextual analysis using \nContextuAl \nSarCasm \nDetector \n(CASCADE) \nand \nBidirectional Encoder Representations from Transformers  \n(BERT) for sarcasm detection. We conducted a qualitative \nanalysis of the CASCADE model (Hazarika et al., 2018)  and \na BERT-based classifier model (Potamias et al., 2020) for \nsarcasm detection using a Reddit Corpus. Additionally, we \nwill use BERT in conjunction with CASCADE to perform \nbidirectional training in our model with additional contextual \ninformation supplied by CASCADE and determine whether \nthere is a significant performance improvement. \n    The language representation model BERT achieves state-\nof-the-art results for various NLP tasks (Devlin et al. 2018). \nBERT uses an innovative technique of applying bidirectional \ntraining to its transformer architecture in language modeling. \nThe bidirectionally trained BERT model can grasp a deeper \nsense of language context and flow than single-direction \nlanguage models, producing better results in NLP tasks, such \nas Question Answering, Natural Language Inference, and \nmany others. \n    Recently, Hazarika et al. (2018) proposed a ContextuAl \nSarCasm Detector (CASCADE) model specifically designed \nfor sarcasm detection. It utilizes a hybrid of content- and \ncontext-driven \napproaches. \nIt \nprocesses \ncontextual \ninformation by first profiling every author and every online \ncommunity in their dataset before training and creates user \nembeddings of behavioral traits for sarcasm. It then performs \nconcurrent training of the text and its surrounding \ndiscussions on the forum. Finally, CASCADE performs \ncontent modeling using a Convolutional Neural Network \n(CNN) to extract syntactic features of the text. The \nconcatenated CNN representation with user embedding and \ndiscussion features is then used for classification. \n    Because sarcasm is most often dependent on its spoken \ncontext and BERT has achieved accurate results in other \ncontextual analysis tasks, we want to apply the BERT model \nto sarcasm detection using a Reddit corpus. We also \n  \n \n2 \nperformed the same task using CASCADE, which is the \ncurrent state-of-the-art model designed explicitly for \ncontextual analysis in sarcasm detection. Finally, we conduct \nquantitative and qualitative analyses to compare the results \nof the two models. Additionally, we will investigate the \n possibility of using the two models in conjunction, \naugmenting BERT with additional contextual analyzing \nfeatures supplied by CASCADE and determine if there is an \nimprovement in performance. \n \n \n \n \n \n2. \nRELATED WORK \nBecause of the prevalence of sarcasm in sentiment-bearing \ntexts and the challenges in predicting sarcastic language with \nan automatic process, sarcasm detection has been a topic of \ninterest in natural language processing research. In general, \nthree types of approaches are used in sarcasm detection: rule-\nbased approaches, content and contextual-based approaches, \nand deep learning- and transformer-based approaches.   \n    Rule-based approaches identify sarcasm using rules that \nrely on the indicators of sarcasm. Reyes et al. (2012) first \nattempted to automate the identification of irony and sarcasm \nin social media. They used a rule-based approach that \nmeasured the unexpectedness factor in analyzing figurative \nlanguage, including sarcasm. Barbieri et al. measured \nunexpectedness using the American National Corpus \nFrequency Data and the morphology of tweets with random \nforest (RF) and decision tree (DT) classifiers. Buschmeier et \nal. (2014) defined unexpectedness as an imbalance of \nemotions between words in a text.   \n    Other rule-based approaches include the use of hashtag \nsentiment as a key identifier of sarcasm, which is frequently \nused in sarcastic tweets. In the study by Maynard and \nGreenwood (2014), sarcasm was detected by locating \nhashtags with sentiments that did not agree with the rest of \nthe text. Reyes et al. (2012) used the likelihood of a simile \nbeing sarcastic as a rule and presented a 9-step approach \nwhere a simile is determined as non-sarcastic using the \nnumber of Google search results. Ghosh et al. (2015) \nidentified sarcasm with support vector machines (SVM) to \nidentify contradictions within a tweet as a measure of \nunexpectedness and achieved state-of-the-art results.   \nContent and contextual-based approaches rely on the \ndependency of sarcasm on context. It utilizes features that \nreveal information about the content, including acronyms \nand adverbs, N-gram patterns, statistical and semantic \nfeatures, and semi-supervised attributes such as word \nfrequencies. Iraz\u00fa et al. (2018) used the affective and \nstructural features of text with a conventional learning \nclassifier to analyze sarcastic language contexts. They then \nconducted a follow-up study with a knowledge-based k-NN \nclassifier and a feature set that captured both the structural \nand emotional linguistic phenomena in the context of \nsarcasm. Van Hee et al. (2015) achieved a significant \nimprovement in LSTM deep neural network baseline results \nusing a combination of lexical, semantic, and syntactic \nfeatures with an SVM classifier. Through their study, \nWallace et al. (2015) found that capturing contextual \ninformation by including previous and following comments \non a Reddit corpus increased the accuracy of sarcasm \ndetection. In a study by Rajadesingan et al. (2015), users\u2019  \n \n \nbehavioral information was used as contextual information \nin sentiment analysis of tweets.  Deep-learning approaches, \nsuch as recurrent neural networks and other language \ntransformers, use word embeddings (the mapping of words \nto real-valued vectors) as the key factor in sarcasm detection. \nGhosh et al. (2015) implemented a combination of word \nembeddings and convolutional neural networks (CNN). \nKumar and Garg engineered a bidirectional LSTM model \nusing an ensemble of shallow classifiers with lexical, \npragmatic, and semantic features, and coupled the model \nwith a CNN in a subsequent study by Kumar and Garg \n(2019). Recently, transformers, a specific class of deep \nlearning models, have become prominent in sarcasm \ndetection and natural language processing. The bidirectional \nencoder representation transformer (BERT) model has \nachieved state-of-the-art results in many NLP tasks. Khatri \net al. (2020) used the BERT model in sarcasm detection by \nfine tuning an instance of BertForSequenceClassification \nand has shown significant improvement in performance \ncompared to bidirectional LSTM and SVM sarcasm \ndetection. \n \n3. \nEVALUATION \nA. Objective \nThe objective of this study is to utilize both contextual- and \ndeep-learning-based approaches in sarcasm detection, given \ntheir successes in previous studies, and to compare their \nperformances. The first approach (Hazarika et al., 2018) \nutilized a contextual-based approach to analyze sarcastic \nonline comments with users\u2019 personality, stylometrics, and \ndiscourse features. The second was a deep learning approach \nusing the RoBERTa model (Potamias et al., 2020). \n   We implemented these two models, as they have \npreviously achieved state-of-the-art results and are \nrepresentative of contextual-based and deep learning-based \napproaches in the area of sarcasm detection. These two \nmodels will be used in the Reddit SARC corpus (Khodak et \nal., 2017) for sarcasm detection. Finally, we analyzed their \nperformance and compared it with those of other baseline \nsystems.    \n \nB. Methodology \nWe applied two models, BERT and CASCADE, to test the \nperformance of the transformer-based and contextual-based \nmethods on the same Reddit corpus SARC. Both methods \nhave shown success and are currently state-of-the-art in their \nrespective approaches for sarcasm detection.   \n \n \n  \n \n3 \n1) CASCADE \n   The CASCADE model (Hazarika et al., 2018) leverages \ncontent- and context-based information from a Reddit \ncomment for classification. For content modeling, the \ncomment was converted into a vector representation using a \nConvolutional Neural Network (CNN). Using a CNN, the \nmodel can extract location-invariant local patterns and \ngenerate abstract representations of the text, which \nencapsulates useful syntactic and semantic information for \nsarcasm detection.   \n   The contextual modeling of CASCADE comprises the \nuser embeddings and discourse features of all users and \ndiscussion forums. The contextual model generates user \nembeddings by modeling the user\u2019s stylometric and \npersonality features and combining them with CCA for a \nsingle representation.   \n   For the personality features, the user embeddings capture \nusers\u2019 traits to determine their sarcasm tendencies by \nanalyzing each user\u2019s accumulated historical posts. Hazarika \net al. (2018) used a CNN pretrained on a benchmark corpus \nof essays labeled with Big Five personality traits. By \niterating the overall comments, it was possible to identify the \npersonality traits inferred from each comment. Finally, it \nuses the expectations of all comments made by the user as \nthe user\u2019s overall personality feature vector. \n   For the stylometric feature, the user embeddings evaluate \nthe writing style of the user. Hazarika et al. (2018) applied \nan \nunsupervised \nrepresentation \nlearning \nmethod, \nParagraphVector, developed by Le and Markolov, which \nencodes a user\u2019s writing style and has been well-tested for \nsentiment classification tasks. Finally, the model combines \nboth stylometric and personality features in the user \nembeddings with CCA (Canonical Correlation Analysis). \n   In addition, CASCADE incorporates a discourse feature \nthat considers the contextual information provided by the \nsurrounding discourse and the characteristics unique to the \ndiscussion forum section in which the comment is made. \nHazarika et al. (2018) used a measure similar to \nParagraphVector to generate feature vectors. \n   Finally, the user embeddings and discourse feature vectors \nare concatenated into a unified representation, which is \nprojected onto the output layer of the CNN to obtain a \nsoftmax probability that classifies the comment as either \nsarcastic or nonsarcastic. Finally, Hazarika et al. (2018) \ncalculated the categorical cross-entropy using probability \nestimates. \n \n2) BERT \nFor the BERT model, we implemented RCNN-RoBERTa \n(Recurrent CNN Robustly Optimized BERT Approach) \nmodel (Potamias et al., 2020). The RoBERTa model \noptimized BERT with training from ten times more data and \nfar more epochs. It also uses 8-times larger batch sizes and a \nbyte-level BPE vocabulary instead of the character-level \nvocabulary used in BERT. RoBERTa also modified the \nsingle static mask used in BERT with dynamic masking \ntechniques.   \n \n \nFIGURE 1. \nRCNN-RoBERTa transformer-based \narchitecture proposed by Potamias et al. (2020) \n \n   The end-to-end model devised combines pretrained \nRoBERTa weights with an RCNN to capture contextual \ninformation. Identifying sarcasm within a sentence requires \nfinding dependencies within RoBERT as pre-trained word \nembeddings. Potamias et al. (2020) applied an RNN layer \nthat captures reliable temporal information instead of 1D \nconvolution layers in vanilla RoBERTa, which cannot detect \ndependencies among word embeddings. The contradictions \nand long-time dependencies within a sentence are used as \nstrong identifiers of sarcastic language. Because RNN\u2019s \ncapturing of temporal relationships between words is \nstrongly biased toward later words than previous ones, but \nunbiased CNNs are dependent on kernel sizes, Potamias et \nal. (2020) utilized an RCNN model to capture unbiased \nrecurrent informative relationships within text.   \n   A bidirectional LSTM layer was then applied and fed into \nRoBERT as the final hidden layer weight. Potamias et al. \n(2020)  concatenated the output of LSTM with embedded \nweights and passed the result through a feedforward \nnetwork, which acted as a 1D convolution layer with a large \nkernel and max-pooling layer. Finally, the output layer was \nprocessed using a softmax function. \n \nC. Dataset \nThe SARC corpus (Khodak et al., 2017)  includes 1.3 million \nremarks of self-annotated comments from the online forum \nReddit. Each entry in the dataset contains the original \ncomment sentence, response sentence, and annotation by the \nauthor of whether the comment is sarcastic. Reddit is \norganized into topic-specific discussion forums known as \nsubreddits. In each subreddit, users comment on each other \neither on a titled post or on each other\u2019s comments under the \ntitled post. This results in a tree-like comment structure, \nwhere the roots are topic-specific subreddits, followed by \nposts within a subreddit, comments made in posts on the \nsubreddit, and comments on a post\u2019s other comments. The \nstructure was then unraveled in a linear format, incorporating \nthe author\u2019s details for each comment. The following is an \nexample of a data point: \n \nAncestor: What will we call Bill Clinton if Hillary is elected \npresident? \n  \n \n4 \nResponse: I can think of a few names (Label: Sarcastic) \n \n   In our evaluation, we considered one variant of the SARC \ndataset, r/politics, a subreddit within the main Reddit forum \nthat centers around politics. There were several reasons for \nthis selection. The r/politics subreddit is a benchmark dataset \n(Khodak et al., 2017). It is also the dataset used for the \nevaluation of most baseline systems, with which we compare \nour results. Another reason is that sarcasm detection requires \nunderstanding of the background information being \ndiscussed. Even humans have trouble detecting sarcasm on \nunfamiliar topics, such as topics on obscure art and hobbies \nforums. Hence, this is the only subsample of the SSARC \ndataset evaluated by humans in the original tests conducted \nby Khodak et al. (2017) because most evaluators had \nsufficient background information. The SARC r/politics \nsubsample consists of 17 thousand sequences, with an \naverage proportion of sarcastic comments of 23.2%. \n \nD. Training Details \nFor the CASCADE model, 20% of the training data were \nreserved for validation. Table 1 shows the distribution of the \nSARC Politics dataset for training and testing.   \n \nTABLE I \nDISTRIBUTION OF SARC POLITICS FOR TRAINING AND TESTING \nTraining set \nTesting set \nno. \nof \ncomments \navg. no. of \nwords \nper \ncomments \nno. \nof \ncomments \navg. no. of \nwords \nper \ncomments \nnon-\nsarc \nsarc \nnon-\nsarc \nsarc \nnon-\nsarc \nsarc \nnon-\nsarc \nsarc \n6834 \n6834 \n64.74 \n62.36 \n1703 \n1703 \n62.99 \n62.14 \n \nNon-sarc: non-sarcastic, sarc: sarcastic  \n \n   With the validation set, hyperparameter tuning was \nperformed through RandomSearch and optimized using the \nAdam optimizer, as proposed by Kingma and Ba. Each \ncomment was padded to a uniform length of 100 words for \nbatch modeling in the CNN. The optimal hyperparameters \nwere {\ud835\udc51\ud835\udc60, \ud835\udc51\ud835\udc5d, \ud835\udc51\ud835\udc61, \ud835\udc3e} = 100, \ud835\udc51\ud835\udc52\ud835\udc5a = 300, \ud835\udc58\ud835\udc60 = 2, \ud835\udc40 = 128, \nand a = \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48.  \n   For training with the RCNN-RoBERTa (Potamias et al., \n2020), we implemented the same RandomSearch of the \ntraining data to find the following combinations of \nhyperparameters that produced the best results. The model \nwas trained for five epochs and took approximately 3h to \ntrain. \n \nE. Evaluation Metrics \nThe evaluation metric we will use for this paper is accuracy \nand F1 score.  \n \nAccuracy = truePositive + trueNegative\nall\n \n \n \nF1 = 2 \u00d7 precision \u00d7  recall\nprecision + recall  \nWe chose accuracy as the primary evaluation metric because \nit is the evaluation metric used for most baseline models, \nwhich allows for easy comparisons. Because the dataset had \na balanced number of data points in each class, class \nimbalance, such as a large number of actual negatives, will \nnot be a problem. In this case, accuracy would be a good \nmetric for a dataset with an even number of data points. \nHowever, we also include the F1 score as our secondary \nmeasure, which will guard against an uneven class \ndistribution.   \n \nTABLE 2 \nRCNN-ROBERTA TRAINING HYPERPARAMETERS \nHyperparameter \n                                  Value \nRoBERTa layers \n12 \nRoBERTa attention heads \n12 \nLSTM units \n64 \nLSTM dropout \n0.1 \nBatch size \n10 \nAdam Epsilon \n1e-6 \nEpochs \n5 \nLearning rate \n2e-5 \nWeight decay \n1e-5 \nRoBERTa layers \n12 \nRoBERTa attention heads \n12 \nLSTM units \n64 \nLSTM dropout \n0.1 \nBatch size \n10 \nAdam Epsilon \n1e-6 \n \n \nF. Baseline Models \n\u2022 \nAverage Human Performance: This baseline of \naverage human performance (Khodak et al., 2017) \nproposed the SARC dataset and asked human evaluators \nto classify sarcastic comments in the SARC politics \ndataset.   \n\u2022 \nBag of Words Baseline: The model uses an SVM \nclassifier with the input features of a comment\u2019s word \ncount. \n\u2022 \nCNN-SVM: This model (Poria et al, 2016) uses a CNN \nfor content modeling and  \n\u2022 \nSVM for classification. \n\u2022 \nCUE-SVM: This approach (Amir et al., 2016) utilizes a \nsimilar user embedding with ParagraphVector as that \nimplemented in CASCADE, which is then fed into the \nCNN. \n \nG. Results \nTable 2 presents the performance results for the SARC \npolitics dataset for the two models implemented in our study \nand the baseline models. The baseline models are presented \nin the previous section. \n \n  \n \n5 \n \n \n \nTABLE 3 \nEVALUATION RESULTS ON SARC POL \nModel \nAccuracy \nF1 \n \nAverage Human Performance \n \n0.82 \n \n- \nBag of Words Baseline \n0.59 \n0.60 \nCNN-SVM \n0.65 \n0.67 \nCUE-SVM (Amir et al., 2016) \n0.69 \n0.70 \nCASCADE \n0.74 \n0.75 \nRCNN-RoBERTa \n0.79 \n0.78 \n \n \n \n \n \n \n   The \nbag-of-words \napproach \nachieved \nthe \nlowest \nperformance, and most neural network approaches achieved \nhigher performance, indicating that more sophisticated deep \nlearning methods are better able to classify sarcasm than the \nword-targeting BoW model. As the SARC politics dataset is \nbalanced, there is little significant difference between the \naccuracy and F1. Because CUE-SVM implemented similar \nuser embeddings as the stylometric embeddings in \nCASCADE, we observed improved performance compared \nwith a similar model without user embeddings, suggesting \nthat contextual information on users improves sarcasm \ndetection.   \n   Both CASCADE and RCNN-RoBERTa outperformed the \nbaseline models with statistical significance. Because the \nCASCADE model outperforms CUE-SVM, the personality \nand discourse features in the CASCADE model may account \nfor this difference. Hence, contextual information on \nauthors\u2019 personalities and surrounding discussions may \ncontribute \nto \nsarcasm \ndetection. \nRCNN-RoBERTas \nachieved better performance than all other methods, which \nindicates that implementing transformer architectures in \ndeep learning methods of sarcasm detection could improve \nthe performance of traditional deep learning approaches.   \n \n4. \nCONCLUSION \nIn this study, we implemented two different state-of-the-art \nmodels, CASCADE (Hazarika et al., 2018) and RCNN-\nRoBERTa (Potamias et al., 2020) to classify sarcastic \ncomments in a Reddit corpus. CASCADE and RCNN-\nRoBERTa are representative contextual-based and deep-\nlearning-based approaches used for sarcasm detection. We \nfound that contextual information, such as user personality \nembeddings, could significantly improve performance, as \nwell as the incorporation of a transformer RoBERTa, \ncompared with a more traditional CNN approach. Given the \nsuccess of both contextual- and transformer-based \napproaches, as shown in our results, augmenting a \ntransformer with additional contextual information features \nmay be an avenue for future experiments. \nREFERENCES \nS. Amir, B. C. Wallace, H. Lyu, P. Carvalho, and M. J. Silva, \u201cModelling \nContext with User Embeddings for Sarcasm Detection in Social Media,\u201d \nACLWeb, Aug. 01, 2016. https://aclanthology.org/K16-1017/ (accessed \nMay 31, 2023). \nDevamanyu Hazarika, Soujanya Poria, Sruthi Gorantla, E. Cambria, R. \nZimmermann, and Rada Mihalcea, \u201cCASCADE: Contextual Sarcasm \nDetection in Online Discussion Forums,\u201d Association for Computational \nLinguistics, vol. C18, no. 1156, pp. 1837\u20131848, May 2018, Accessed: Sep. \n16, 2023. [Online]. Available: https://arxiv.org/abs/1805.06413 \nF. Barbieri, H. Saggion, and F. Ronzano, \u201cModelling Sarcasm in Twitter, a \nNovel Approach,\u201d Meeting of the Association for Computational \nLinguistics, vol. W14, no. 2609, pp. 50\u201358, Jun. 2014, doi: \nhttps://doi.org/10.3115/v1/w14-2609. \nD. Iraz\u00fa, M. Montes-y-G\u00f3mez, Hugo Jair Escalante, P. Rosso, and V. Patti, \n\u201cA Knowledge-Based Weighted KNN for Detecting Irony in Twitter,\u201d \nSpringer \neBooks, \nvol. \n11289, \npp. \n194\u2013206, \nJan. \n2018, \ndoi: \nhttps://doi.org/10.1007/978-3-030-04497-8_16. \nA. Ghosh et al., \u201cSemEval-2015 Task 11: Sentiment Analysis of Figurative \nLanguage in Twitter,\u201d Association for Computational Linguistics, vol. S15, \nno. 2080, Jan. 2015, doi: https://doi.org/10.18653/v1/s15-2080. \nKonstantin Buschmeier, Philipp Cimiano, and R. Klinger, \u201cAn Impact \nAnalysis of Features in a Classification Approach to Irony Detection in \nProduct Reviews,\u201d Association for Computational Linguistics, vol. W14, \nno. 2608, Jan. 2014, doi: https://doi.org/10.3115/v1/w14-2608. \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of \nDeep Bidirectional Transformers for Language Understanding,\u201d CoRR, vol. \nabs/1810.04805, Oct. 2018, Accessed: Sep. 16, 2023. [Online]. Available: \nhttps://arxiv.org/abs/1810.04805 \nA. Khatri and P. P, \u201cSarcasm Detection in Tweets with BERT and GloVe \nEmbeddings,\u201d \nACLWeb, \nJul. \n01, \n2020. \nhttps://aclanthology.org/2020.figlang-1.7/ (accessed May 21, 2022). \nA. Kumar and G. Garg, \u201cEmpirical study of shallow and deep learning \nmodels for sarcasm detection using context in benchmark datasets,\u201d Journal \nof Ambient Intelligence and Humanized Computing, vol. 14, no. 5327\u20135342, \nAug. 2019, doi: https://doi.org/10.1007/s12652-019-01419-7. \nD. Maynard and M. Greenwood, \u201cWho cares about Sarcastic Tweets? \nInvestigating the Impact of Sarcasm on Sentiment Analysis.,\u201d Language \nResources and Evaluation, vol. Proceedings of the Ninth International \nConference on Language Resources and Evaluation (LREC\u201914), pp. 4238\u2013\n4243, Mar. 2014, Accessed: Sep. 16, 2023. [Online]. Available: \nhttps://aclanthology.org/L14-1527/ \nMikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli, \u201cA Large Self-\nAnnotated Corpus for Sarcasm,\u201d European Language Resources \nAssociation (ELRA), vol. arXiv:1704.05579, no. 4, Apr. 2017, Accessed: \nSep. 16, 2023. [Online]. Available: https://arxiv.org/abs/1704.05579 \nR. A. Potamias, G. Siolas, and A. - G. Stafylopatis, \u201cA transformer-based \napproach to irony and sarcasm detection,\u201d Neural Computing and \nApplications, vol. 32, no. 23, pp. 17309\u201317320, Jun. 2020, doi: \nhttps://doi.org/10.1007/s00521-020-05102-3. \nA. Reyes, P. Rosso, and T. Veale, \u201cA multidimensional approach for \ndetecting irony in Twitter,\u201d Language Resources and Evaluation, vol. 47, \nno. 1, pp. 239\u2013268, Jul. 2012, doi: https://doi.org/10.1007/s10579-012-\n9196-x. \nA. Rajadesingan, R. Zafarani, and H. Liu, \u201cSarcasm Detection on Twitter,\u201d \nProceedings of the Eighth ACM International Conference on Web Search \nand \nData \nMining \n- \nWSDM \n\u201915, \n2015, \ndoi: \nhttps://doi.org/10.1145/2684822.2685316. \nSoujanya Poria, E. Cambria, Devamanyu Hazarika, and P. Vij, \u201cA Deeper \nLook into Sarcastic Tweets Using Deep Convolutional Neural Networks,\u201d \nInternational Conference on Computational Linguistics, pp. 1601\u20131612, \nDec. \n2016, \nAccessed: \nSep. \n16, \n2023. \n[Online]. \nAvailable: \nhttps://arxiv.org/abs/1610.08815 \nCynthia Van Hee, E. Lefever, and V. Hoste, \u201cLT3: Sentiment Analysis of \nFigurative Tweets: piece of cake #NotReally,\u201d Ghent University Academic \nBibliography (Ghent University), vol. S15, no. 2115, Jan. 2015, doi: \nhttps://doi.org/10.18653/v1/s15-2115. \nB. C. Wallace, Do Kook Choe, and E. Charniak, \u201cSparse, Contextually \nInformed Models for Irony Detection: Exploiting User Communities, \nEntities and Sentiment,\u201d Association for Computational Linguistics, vol. \nP15, no. 1100, Jan. 2015, doi: https://doi.org/10.3115/v1/p15-1100. \n \n \n \n"}, "MathGloss: Building mathematical glossaries from text": {"authors": ["Lucy Horowitz", "Valeria de Paiva"], "title": "MathGloss: Building mathematical glossaries from text", "url": "https://arxiv.org/pdf/2311.12649.pdf", "abstract": "MathGloss is a project to create a knowledge graph (KG) for undergraduate mathematics from text, automatically, using modern natural language processing (NLP) tools and resources already available on the web. MathGloss is a linked database of undergraduate concepts in mathematics. So far, it combines five resources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph hosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses at the University of Chicago, (iii) the syllabus of the French undergraduate mathematics curriculum which includes hyperlinks to the automated theorem prover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by mathematicians, and (v) the nLab, a wiki for category theory also curated by mathematicians. MathGloss's goal is to bring together resources for learning mathematics and to allow every mathematician to tailor their learning to their own preferences. Moreover, by organizing different resources for learning undergraduate mathematics alongside those for learning formal mathematics, we hope to make it easier for mathematicians and formal tools (theorem provers, computer algebra systems, etc) experts to \"understand\" each other and break down some of the barriers to formal math.", "arxiv_id": "2311.12649", "published_date": "2023-11-21", "year": 2023, "introduction": "Introduction When mathematicians read mathematical texts, the first thing they do is try to uncover the important concepts\u2014what is the work about? We can think of mathematics as an enormous web of definitions and concepts together with theorems that relate them. While it is hard for humans to learn how to produce proofs of theorems, it should be much easier for anyone to find all the definitions required for the theorems and to organize them in structured ways. Many concepts and theorems are written down where almost anyone can find them, but some of it \u201cexists nearly as folklore\u201d [Har20]. That is, the math has been done, but it isn\u2019t organized in a way that is accessible to everyone outside the field. Ultimately, we hope to aid the organization of the math that is in this state by starting with the basics: an undergraduate curriculum. Much of math accessible to a typical undergraduate is very well-understood and even well-organized in textbooks and online. However, all is not perfect. When asked to write down the definition of a \u201cgroup,\u201d different mathematicians may write down different things. For example, the authors of the Wikipedia article for \u201cgroup\u201d1 ease the reader into abstraction, beginning with an explanation of the group structure of the integers and a quote about the mysterious nature of mathematical definitions before writing down the group axioms in careful detail. At a higher level, the authors of the nLab article for \u201cgroup\u201d2 jump right into \u201cmonoid with inverses\u201d and offhandedly reference some slightly nontrivial properties in the same sentence. A category theorist probably doesn\u2019t need a careful treatment of the group axioms, but someone learning group theory who happened to come across the nLab might be confused by this definition, while the Wikipedia article may have been more accessible. By putting them together, we can ensure common understanding across all levels of mathematics. This principle extends to learning formal mathematics, which involves getting a computer to \u201cdo\u201d mathematics and verify all of the steps in a proof. No matter how abstract the nLab or Wikipedia entries may get, they were written for humans to read and understand. On the other hand, the definition of \u201cgroup\u201d in Lean, a formal theorem prover,3 is written for an audience of computers. Yet if we want people to learn to do formal mathematics (and we do, as it shows great promise for modern 1https://en.wikipedia.org/wiki/Group_(mathematics) 2https://ncatlab.org/nlab/show/group 3https://lean-lang.org/ 1 arXiv:2311.12649v1  [cs.CL]  21 Nov 2023 mathematics [Har21]), they will have to learn to interpret at least some things that were mostly meant for computers. Some people describe writing formal proofs as feeding definitions to a computer, and they note that the computers will often \u201cwhine\u201d when they don\u2019t \u201cunderstand\u201d something they\u2019ve been given [Rob23]. If we can present formal mathematical concepts alongside the more familiar natural-language concepts, the communication between mathematician and computer will be greatly improved. Moreover, highlighting concepts that often appear in undergraduate curricula could help students taking and professors designing \u201cbilingual\u201d math courses that teach both natural and formal proof-writing simultaneously. This document serves as a written companion to a presentation given at the EuroProofNet joint meeting between the Workshop on Natural Formal Mathematics and the Workshop on Libraries of Formal Proofs and Natural Mathematical Language in Cambridge on September 7, 2023.4 2 Math concepts and Wikidata Wikidata [VK14] is a knowledge graph that contains the structured data behind Wikipedia. A knowledge graph represents a network of real-world entities\u2014that is, objects, events, situations, or concepts\u2014and the relationships between them. This information is usually stored in a graph database and visualized as a graph structure, hence the term knowledge \u201cgraph.\u201d Wikipedia has a large number of articles on mathematical concepts, described in its \u201cMath Portal.\u201d5 We want to use WikiData as a knowledge repository for the math concepts we collect. Wikidata assigns a unique identification number of the form Qx...x (where x is a digit) to each concept it describes. Some of the concepts are connected with descriptive links about their relationships to each other. For example, according to Wikidata a \u201cbook\u201d (Q571) is an \u201cinstance of\u201d \u201cwritten media.\u201d These links are not present between every pair of items that \u201cshould\u201d be linked because creating these links is a labor-intensive process. We attempt to map each term in our corpora to its Wikidata identifier (ID), and base our organization on common mappings. That is, we present the terms that get mapped to the same Wikidata ID as the same mathematical concept. Some of the mappings were done using wikimapper,6 a Python package written by Jan-Kristoph Klie, a researcher at the Ubiquitous Knowledge Processing Lab of the Technical University of Darmstadt. Some of the mappings were done manually. The library wikimapper takes in the name of an \u201citem\u201d and produces a Wikidata ID whose page has a title matching that name. This often produces undesirable mappings due to the overloading of words in English and especially in mathematical English. For example, the words \u201cgroup,\u201d \u201cring,\u201d and \u201cfield\u201d represent fundamentally important concepts in mathematics, but they also refer to everyday objects and can are often even used as verbs! When \u201cgroup\u201d is put through wikimapper, the output is Q654302 instead of the hoped-for Q83478. The former is a \u201cdisambiguation page,\u201d or a central hub page listing different things that go by the same name or similar names. We were able to almost completely resolve the disambiguation issue by appending parenthetical subject names to the ends of the terms. The parentheticals we used were \u201cmathematics,\u201d \u201clinear algebra,\u201d \u201calgebraic geometry,\u201d \u201ccalculus,\u201d \u201ccategory theory,\u201d \u201ccommutative algebra,\u201d \u201cfield theory,\u201d \u201cgame theory,\u201d \u201ctopology,\u201d \u201cdifferential geometry,\u201d \u201cgraph theory,\u201d \u201cinvariant theory,\u201d \u201cgroup theory,\u201d \u201cmodule theory,\u201d \u201corder theory,\u201d \u201cprobability,\u201d \u201cstatistics,\u201d \u201cring theory,\u201d \u201crepresentation theory,\u201d \u201cset theory,\u201d \u201cstring theory, \u201csymplectic geometry,\u201d and \u201ctensor theory.\u201d We chose these fields as parentheticals from the Wikipedia page listing the glossaries of mathematics.7 The name of the Wikipedia page describing mathematical groups is technically \u201cGroup (mathematics).\u201d We were able to leverage this fact and the way that wikimapper can take a Wikipedia page name as input to drastically reduce the number of disambiguation pages returned. For the annotation of low resource domains, see [KEdCG20], wikimapper is not perfect but helps considerably with the mapping task. 4https://europroofnet.github.io/cambridge-2023/#horowitz 5https://en.wikipedia.org/wiki/Portal:Mathematics 6https://github.com/jcklie/wikimapper 7https://en.wikipedia.org/wiki/Category:Glossaries_of_mathematics 2 3 Math Resources There are plenty of good sources of mathematics online. One of the issues beginners face is how to choose between these sources. We describe some of the sources we want to make available through MathGloss and we hope to be able to make other sources available in the near future. Currently, MathGloss consists of terms collected from several resources for mathematical knowledge mapped to Wikidata, either manually or using wikimapper. 3.1 Chicago The Chicago corpus8 consists of approximately 700 terms related to courses in mathematics taken by the first author at the University of Chicago. With respect to MathGloss, it represents a \u201cgold standard\u201d of definitions of mathematical concepts that are well-known enough to appear in an undergraduate mathematics curriculum. That is, each entry in the corpus is annotated with its status as a definition. This corpus is not exhaustive, rather it reflects the first author\u2019s interests and the topics covered within these interests by individual professors. Each concept in the corpus (e.g., \u201cgroup\u201d) has its own Markdown file containing a definition of the term and links to the Markdown files corresponding to other terms. The links under the Chicago column on the MathGloss website lead to the content of these Markdown files, which contain links to other definitions. 3.2 French undergraduate curriculum in Lean 4 This corpus consists of terms (translated from French) that are listed by the Minist\u00b4ere de l\u2019\u00b4Education Nationale et de la Jeunesse as concepts undergraduate mathematics students are expected to know by the end of their degree. The Lean Community has added links from these concepts to their representation in Lean 4,9 and the links on the MathGloss webpage lead to those Lean entries. Some terms do not have Lean counterparts yet, or the link to its counterpart is not included in the corpus. The mappings from terms to WikiData were done using wikimapper. There are 543 terms in total and 369 of them have Wikidata counterparts. Figure 1 below shows such a mapping for the term \u201c0-1 Law\u201d. Figure 1: Clicking through to Lean 4 3.3 Multilingual Mathematics (MuLiMa) The translation of mathematical terms between (natural) languages should be much easier than it actually is, given that terms in math are supposed to be unambiguous in their definitions. However, mathematicians are really at liberty to choose any name for any concept. Often, this means words that refer to the same mathematical object in two different languages will not \u201ctranslate\u201d to each other. For example, the word in French for what we in English call a \u201cfield\u201d is \u201ccorps,\u201d which literally means \u201cbody.\u201d Moreover, much of mathematics that was first written about in English has no real translation into other languages\u2014mathematicians will sometimes just use the English term. Collecting translations of mathematical concepts is therefore an important task. Tim Hosgood, a researcher at 8https://github.com/MathGloss/MathGloss/tree/main/chicago 9https://leanprover-community.github.io/undergrad.html 3 the Topos Institute, is working on this problem. He created a cross-language dictionary10 for math with a similar structure to that of MathGloss. It has 305 terms at the moment, which were all manually mapped to Wikidata. 3.4 nLab The nLab11 is a wiki for higher mathematics, specifically category theory. It is not a resource intended for undergraduates, but we have included it here filtered along those terms which also appear in the other three corpora. The terms are the titles of the pages hosted on the nLab, but some of these are pages about people or books. The filtration by other corpora should ensure that only mathematical concepts make it into the final table. There were more than 18,000 page titles at the time of writing, and we found that 5377 of these had Wikidata items. Fewer than 5377 terms were included in the final table because of the filtering via the other resources. 3.5 More online math So far we have only included four corpora, which are not comprehensive of all undergraduate mathematics. One place we hope to look next is open-source textbooks for different topics and use natural language processing (NLP) to find important terms and concepts there. Inspired by [CdPFS22], we were successful in extracting terms from the journal Theory and Applications of Categories (TAC)12 using NLP before we decided to focus on undergraduate rather than research mathematics. 4 Using MathGloss The table of terms in MathGloss can be found at https://mathgloss.github.io/MathGloss/database. Figure 2 shows the first few rows of the table of mappings. As an example of how to use MathGloss, let\u2019s say we want to find out more about \u2018abelian groups\u2019. If we haven\u2019t already seen it in row three, a simple page search (Ctrl+F) can help find it in the table. Figure 2: The main table of MathGloss Clicking on the link labeled \u201cQ181296\u201d takes us to the Wikidata entry for abelian group, which contains much useful information about abelian groups and their relationships to other kinds of algebraic objects. In particular, it has a link to the Wikipedia page for \u201cAbelian group.\u201d Clicking on the link labeled \u201cabelian group\u201d under the header \u201cChicago\u201d takes us to a definition of abelian group, hosted on the MathGloss website. This page contains links to other relevant definitions, also on MathGloss. There is currently no Lean 4 link for abelian group in the provided list of undergraduate concepts, but 10https://thosgood.com/maths-dictionary/ 11https://ncatlab.org/nlab/show/HomePage 12http://www.tac.mta.ca/tac/ 4 if there were, clicking on it would take us to the entry in the Lean 4 documentation defining abelian groups. Since \u201cabelian group\u201d appears under the MuLiMa heading, going to the MuLiMa website will allow us to see its translation into several languages. Finally, if we click on the link under the nLab heading, we will see the nLab page for \u201cabelian group,\u201d which takes on a distinctly categorical point of view. The lack of a link to an instance of \u201cabelian group\u201d in Lean 4 highlights the need to collect information from multiple resources. Certainly one can talk about abelian groups in Lean, but the list of undergraduate concepts we used just happens not to link there. 5 Tools for NLP At another stage in the project, we collected terms from the abstracts of articles published in Theory and Applications of Categories (TAC) using natural language processing (TAC). We did not include it in this iteration of MathGloss because as a corpus of research mathematics, it does not fit our goal of organizing undergraduate math. However, we hope to apply this technique to other corpora of undergraduate mathematics in the future. We describe the technique below. To extract terms from TAC, we used the Python library spaCy13 to perform grammatical analysis on the text of the 755 abstracts from the articles in TAC.14 SpaCy performs syntactic parsing of sentences using the Universal Dependencies (UD)[NdMG+16] framework. UD is an open-source project that works towards standardizing grammatical annotation to make linguistics research more consistent. The output of this analysis is in a UD-developed format called CoNLL-U,15 which we then inspect using a script from UD. A CoNLL-U file is a plaintext file that displays sentence analysis in a particular structure. It allows comment lines, which are indicated by \u201c#\u201d, and in the CoNLL-U files we created, the comments include the text of the sentence, the number of its entry in the corpus, and the length (in \u201ctokens\u201d) of the sentence. Figure 3 shows an example of a CoNLL-U file containing a sentence analyzed in this way after application of the \u201cdetextor\u201d pipeline component described below. Each word in the sentence is written on its own line following the text of the sentence along with information about the word, for example its part of speech. Figure 3: Analysis of the definition of \u201cabelian group\u201d in CoNLL-U format At its simplest, spaCy takes in a section of text (called a Doc, short for \u201cdocument\u201d), and then uses its models to split the text into sentences, split sentences into tokens, and then assigns parts of speech and assigns Universal Dependency relations to each token. A token can be thought of as a generalization of a word: it can be a punctuation mark, or especially in our case, a piece of mathematical notation. The process of splitting a Doc into sentences is called \u201csentencization,\u201d and the process of splitting a sentence into tokens is called \u201ctokenization.\u201d We used the smallest model provided by spaCy, called \u201cen core web sm,\u201d because of our limitations in computing power. Using larger models made no difference in output, but took more time and resources to generate that output. 13https://spacy.io/ 14https://github.com/ToposInstitute/tac-corpus 15https://universaldependencies.org/format.html 5 It is possible to run spaCy on LATEX code, but without making specific modifications for mathematical notation, the results are very poor due to incorrect sentencization (including both sentences that are too long and sentences that are too short) and the over-tokenization of the sentence. LATEX code consists of many punctuation marks and commands that often resemble English words, but none of these things should be parsed as normal English. Without modifications to the default spaCy pipeline, each piece of the code that represents a mathematical expression is fragmented and treated separately. Our goal is to extract definitions and concepts from texts written in mathematical English, and LATEX is an integral part of that. First-year mathematics students are taught that mathematical expressions should always be situated within a grammatically correct sentence. They should never stand on their own, and the statement \u201cx = y\u201d should be read \u201cx equals y,\u201d and can therefore be considered an independent clause. Within abstracts and definitions, one does not usually make declarative statements like \u201cx = y,\u201d so most of the LATEX we encounter in our corpora should be thought of as nouns or as names of instances of mathematical objects. Because code itself is not \u201cnatural language,\u201d one way to analyze it is to take each piece of LATEX code to represent a single \u201cword.\u201d This makes up our main approach to solving the tokenization problem. It is as simple as telling spaCy to treat everything in between two dollar signs as one token. The implementation of this pipeline component is done using the \u201cretokenizer\u201d context manager and requires that dollar signs (and for best results, hyphens) be padded with spaces. This new pipeline component, called \u201cdetextor,\u201d is implemented after the \u201ctagger\u201d component, which assigns parts of speech to tokens. The detextor pipeline component represents only a partial solution to the problem of annotating LATEX code as it struggles to capture the actual information conveyed by formulas. However, it greatly improves the accuracy of sentencization, which is crucial for annotating regular English words with their parts of speech. The part-of-speech annotation forms the foundation of our term extraction. We are able to pick out what we expect to be mathematical terms or concepts with a simple heuristic. From the CoNLLU files, we are able to compile lists of \u201clemmas,\u201d or basic forms of words (e.g. \u201cbe\u201d for the word \u201cis,\u201d or \u201cgroup\u201d for the word \u201cgroups\u201d) according to their part of speech. We suppose that the most frequently occurring nouns, adjective-noun phrases, and \u201ccompounds\u201d are math terms. \u201cCompound\u201d is a UD annotation for certain multi-word phrases that represent a single thing16. Adjective-noun phrases consist of any number of adjectives followed by a noun or by a compound. The upper ends of the frequency tables for these types of terms are almost exclusively populated by math concepts, but we do not yet know if the lists exclude some concepts. This process is what we hope to use on other bodies of mathematical text when they do not themselves provide ready-made lists of terms. Unfortunately, it does not produce any kind of link to an explanation or definition of a given term. In the future we hope to be able to pinpoint the definitions of these extracted terms and give pointers to their locations in text. In other words, we want to perform entity linking on math text. 6 Future Work First, we hope to develop a method to more easily collect and map terms from different math resources. In particular, we want to include more theorem provers in our mappings. Presently we are looking into adding links to Agda, Coq, and Isabelle. At the Dagstuhl seminar on automated mathematics,17 there was discussion on how best to find and compile instances of undergraduate math concepts from these provers. Hopefully they will follow the example of Lean and produce such resources. On the natural language side of collecting more terms from more resources, we want to use machine learning techniques to extract definitions of the terms we already know how to find. Some work has been done in this direction, even specifically tailored to mathematical text [VLSR20], but results do not look impressive [CdPS23]. Another future project is to come up with a way to verify that the mappings to Wikidata are indeed correct. The addition of subject parentheticals definitely reduces the number of disambiguation pages output by Wikimapper, but some still slip through the cracks. We could potentially do this by verifying 16https://universaldependencies.org/u/dep/compound.html 17https://www.dagstuhl.de/23401 6 ", "conclusion": "Conclusion MathGloss aims to help people from different backgrounds make sense of the diverse resources for mathematics available online through organization by individual concept. Should one encyclopedia\u2019s article for a particular construction be inscrutable, we would like it to be easy to find another article that is more closely aligned with the reader\u2019s background and therefore easier to understand. Moreover, MathGloss represents a step in the direction of bridging the gap between natural math as done by humans and formal math. By creating a knowledge graph of undergraduate mathematics, we hope to empower students, mathematicians, and those who use mathematics in their work to both better navigate the intricate web of definitions and theorems and to embrace the use of formal systems. ", "full_text": "MathGloss: Building mathematical glossaries from text\nLucy Horowitz\nValeria de Paiva\nNovember 22, 2023\nAbstract\nMathGloss is a project to create a knowledge graph (KG) for undergraduate mathematics from\ntext, automatically, using modern natural language processing (NLP) tools and resources already\navailable on the web. MathGloss is a linked database of undergraduate concepts in mathematics.\nSo far, it combines five resources: (i) Wikidata, a collaboratively edited, multilingual knowledge\ngraph hosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses at the\nUniversity of Chicago, (iii) the syllabus of the French undergraduate mathematics curriculum\nwhich includes hyperlinks to the automated theorem prover Lean 4, (iv) MuLiMa, a multilingual\ndictionary of mathematics curated by mathematicians, and (v) the nLab, a wiki for category theory\nalso curated by mathematicians.\nMathGloss\u2019s goal is to bring together resources for learning\nmathematics and to allow every mathematician to tailor their learning to their own preferences.\nMoreover, by organizing different resources for learning undergraduate mathematics alongside\nthose for learning formal mathematics, we hope to make it easier for mathematicians and formal\ntools (theorem provers, computer algebra systems, etc) experts to \u201cunderstand\u201d each other and\nbreak down some of the barriers to formal math.\n1\nIntroduction\nWhen mathematicians read mathematical texts, the first thing they do is try to uncover the important\nconcepts\u2014what is the work about? We can think of mathematics as an enormous web of definitions\nand concepts together with theorems that relate them. While it is hard for humans to learn how to\nproduce proofs of theorems, it should be much easier for anyone to find all the definitions required\nfor the theorems and to organize them in structured ways. Many concepts and theorems are written\ndown where almost anyone can find them, but some of it \u201cexists nearly as folklore\u201d [Har20]. That\nis, the math has been done, but it isn\u2019t organized in a way that is accessible to everyone outside the\nfield. Ultimately, we hope to aid the organization of the math that is in this state by starting with the\nbasics: an undergraduate curriculum.\nMuch of math accessible to a typical undergraduate is very well-understood and even well-organized\nin textbooks and online. However, all is not perfect. When asked to write down the definition of a\n\u201cgroup,\u201d different mathematicians may write down different things. For example, the authors of the\nWikipedia article for \u201cgroup\u201d1 ease the reader into abstraction, beginning with an explanation of the\ngroup structure of the integers and a quote about the mysterious nature of mathematical definitions\nbefore writing down the group axioms in careful detail. At a higher level, the authors of the nLab article\nfor \u201cgroup\u201d2 jump right into \u201cmonoid with inverses\u201d and offhandedly reference some slightly nontrivial\nproperties in the same sentence. A category theorist probably doesn\u2019t need a careful treatment of the\ngroup axioms, but someone learning group theory who happened to come across the nLab might be\nconfused by this definition, while the Wikipedia article may have been more accessible. By putting\nthem together, we can ensure common understanding across all levels of mathematics.\nThis principle extends to learning formal mathematics, which involves getting a computer to \u201cdo\u201d\nmathematics and verify all of the steps in a proof. No matter how abstract the nLab or Wikipedia\nentries may get, they were written for humans to read and understand.\nOn the other hand, the\ndefinition of \u201cgroup\u201d in Lean, a formal theorem prover,3 is written for an audience of computers. Yet\nif we want people to learn to do formal mathematics (and we do, as it shows great promise for modern\n1https://en.wikipedia.org/wiki/Group_(mathematics)\n2https://ncatlab.org/nlab/show/group\n3https://lean-lang.org/\n1\narXiv:2311.12649v1  [cs.CL]  21 Nov 2023\n mathematics [Har21]), they will have to learn to interpret at least some things that were mostly meant\nfor computers.\nSome people describe writing formal proofs as feeding definitions to a computer, and they note\nthat the computers will often \u201cwhine\u201d when they don\u2019t \u201cunderstand\u201d something they\u2019ve been given\n[Rob23]. If we can present formal mathematical concepts alongside the more familiar natural-language\nconcepts, the communication between mathematician and computer will be greatly improved. More-\nover, highlighting concepts that often appear in undergraduate curricula could help students taking\nand professors designing \u201cbilingual\u201d math courses that teach both natural and formal proof-writing\nsimultaneously.\nThis document serves as a written companion to a presentation given at the EuroProofNet joint\nmeeting between the Workshop on Natural Formal Mathematics and the Workshop on Libraries of\nFormal Proofs and Natural Mathematical Language in Cambridge on September 7, 2023.4\n2\nMath concepts and Wikidata\nWikidata [VK14] is a knowledge graph that contains the structured data behind Wikipedia. A knowl-\nedge graph represents a network of real-world entities\u2014that is, objects, events, situations, or con-\ncepts\u2014and the relationships between them. This information is usually stored in a graph database\nand visualized as a graph structure, hence the term knowledge \u201cgraph.\u201d\nWikipedia has a large number of articles on mathematical concepts, described in its \u201cMath Portal.\u201d5\nWe want to use WikiData as a knowledge repository for the math concepts we collect. Wikidata assigns\na unique identification number of the form Qx...x (where x is a digit) to each concept it describes.\nSome of the concepts are connected with descriptive links about their relationships to each other. For\nexample, according to Wikidata a \u201cbook\u201d (Q571) is an \u201cinstance of\u201d \u201cwritten media.\u201d These links\nare not present between every pair of items that \u201cshould\u201d be linked because creating these links is a\nlabor-intensive process.\nWe attempt to map each term in our corpora to its Wikidata identifier (ID), and base our organi-\nzation on common mappings. That is, we present the terms that get mapped to the same Wikidata\nID as the same mathematical concept. Some of the mappings were done using wikimapper,6 a Python\npackage written by Jan-Kristoph Klie, a researcher at the Ubiquitous Knowledge Processing Lab of\nthe Technical University of Darmstadt. Some of the mappings were done manually.\nThe library wikimapper takes in the name of an \u201citem\u201d and produces a Wikidata ID whose page\nhas a title matching that name. This often produces undesirable mappings due to the overloading of\nwords in English and especially in mathematical English. For example, the words \u201cgroup,\u201d \u201cring,\u201d and\n\u201cfield\u201d represent fundamentally important concepts in mathematics, but they also refer to everyday\nobjects and can are often even used as verbs! When \u201cgroup\u201d is put through wikimapper, the output\nis Q654302 instead of the hoped-for Q83478. The former is a \u201cdisambiguation page,\u201d or a central hub\npage listing different things that go by the same name or similar names.\nWe were able to almost completely resolve the disambiguation issue by appending parenthetical\nsubject names to the ends of the terms. The parentheticals we used were \u201cmathematics,\u201d \u201clinear\nalgebra,\u201d \u201calgebraic geometry,\u201d \u201ccalculus,\u201d \u201ccategory theory,\u201d \u201ccommutative algebra,\u201d \u201cfield theory,\u201d\n\u201cgame theory,\u201d \u201ctopology,\u201d \u201cdifferential geometry,\u201d \u201cgraph theory,\u201d \u201cinvariant theory,\u201d \u201cgroup theory,\u201d\n\u201cmodule theory,\u201d \u201corder theory,\u201d \u201cprobability,\u201d \u201cstatistics,\u201d \u201cring theory,\u201d \u201crepresentation theory,\u201d\n\u201cset theory,\u201d \u201cstring theory, \u201csymplectic geometry,\u201d and \u201ctensor theory.\u201d We chose these fields as\nparentheticals from the Wikipedia page listing the glossaries of mathematics.7\nThe name of the\nWikipedia page describing mathematical groups is technically \u201cGroup (mathematics).\u201d We were able\nto leverage this fact and the way that wikimapper can take a Wikipedia page name as input to\ndrastically reduce the number of disambiguation pages returned. For the annotation of low resource\ndomains, see [KEdCG20], wikimapper is not perfect but helps considerably with the mapping task.\n4https://europroofnet.github.io/cambridge-2023/#horowitz\n5https://en.wikipedia.org/wiki/Portal:Mathematics\n6https://github.com/jcklie/wikimapper\n7https://en.wikipedia.org/wiki/Category:Glossaries_of_mathematics\n2\n 3\nMath Resources\nThere are plenty of good sources of mathematics online. One of the issues beginners face is how to\nchoose between these sources. We describe some of the sources we want to make available through\nMathGloss and we hope to be able to make other sources available in the near future. Currently,\nMathGloss consists of terms collected from several resources for mathematical knowledge mapped to\nWikidata, either manually or using wikimapper.\n3.1\nChicago\nThe Chicago corpus8 consists of approximately 700 terms related to courses in mathematics taken\nby the first author at the University of Chicago. With respect to MathGloss, it represents a \u201cgold\nstandard\u201d of definitions of mathematical concepts that are well-known enough to appear in an under-\ngraduate mathematics curriculum. That is, each entry in the corpus is annotated with its status as a\ndefinition. This corpus is not exhaustive, rather it reflects the first author\u2019s interests and the topics\ncovered within these interests by individual professors. Each concept in the corpus (e.g., \u201cgroup\u201d) has\nits own Markdown file containing a definition of the term and links to the Markdown files correspond-\ning to other terms. The links under the Chicago column on the MathGloss website lead to the content\nof these Markdown files, which contain links to other definitions.\n3.2\nFrench undergraduate curriculum in Lean 4\nThis corpus consists of terms (translated from French) that are listed by the Minist\u00b4ere de l\u2019\u00b4Education\nNationale et de la Jeunesse as concepts undergraduate mathematics students are expected to know\nby the end of their degree.\nThe Lean Community has added links from these concepts to their\nrepresentation in Lean 4,9 and the links on the MathGloss webpage lead to those Lean entries. Some\nterms do not have Lean counterparts yet, or the link to its counterpart is not included in the corpus.\nThe mappings from terms to WikiData were done using wikimapper. There are 543 terms in total\nand 369 of them have Wikidata counterparts. Figure 1 below shows such a mapping for the term \u201c0-1\nLaw\u201d.\nFigure 1: Clicking through to Lean 4\n3.3\nMultilingual Mathematics (MuLiMa)\nThe translation of mathematical terms between (natural) languages should be much easier than it\nactually is, given that terms in math are supposed to be unambiguous in their definitions. However,\nmathematicians are really at liberty to choose any name for any concept. Often, this means words\nthat refer to the same mathematical object in two different languages will not \u201ctranslate\u201d to each\nother. For example, the word in French for what we in English call a \u201cfield\u201d is \u201ccorps,\u201d which literally\nmeans \u201cbody.\u201d Moreover, much of mathematics that was first written about in English has no real\ntranslation into other languages\u2014mathematicians will sometimes just use the English term. Collecting\ntranslations of mathematical concepts is therefore an important task. Tim Hosgood, a researcher at\n8https://github.com/MathGloss/MathGloss/tree/main/chicago\n9https://leanprover-community.github.io/undergrad.html\n3\n the Topos Institute, is working on this problem. He created a cross-language dictionary10 for math\nwith a similar structure to that of MathGloss. It has 305 terms at the moment, which were all manually\nmapped to Wikidata.\n3.4\nnLab\nThe nLab11 is a wiki for higher mathematics, specifically category theory. It is not a resource intended\nfor undergraduates, but we have included it here filtered along those terms which also appear in the\nother three corpora.\nThe terms are the titles of the pages hosted on the nLab, but some of these are pages about people\nor books. The filtration by other corpora should ensure that only mathematical concepts make it into\nthe final table. There were more than 18,000 page titles at the time of writing, and we found that\n5377 of these had Wikidata items. Fewer than 5377 terms were included in the final table because of\nthe filtering via the other resources.\n3.5\nMore online math\nSo far we have only included four corpora, which are not comprehensive of all undergraduate mathe-\nmatics. One place we hope to look next is open-source textbooks for different topics and use natural\nlanguage processing (NLP) to find important terms and concepts there. Inspired by [CdPFS22], we\nwere successful in extracting terms from the journal Theory and Applications of Categories (TAC)12\nusing NLP before we decided to focus on undergraduate rather than research mathematics.\n4\nUsing MathGloss\nThe table of terms in MathGloss can be found at https://mathgloss.github.io/MathGloss/database.\nFigure 2 shows the first few rows of the table of mappings. As an example of how to use MathGloss,\nlet\u2019s say we want to find out more about \u2018abelian groups\u2019. If we haven\u2019t already seen it in row three,\na simple page search (Ctrl+F) can help find it in the table.\nFigure 2: The main table of MathGloss\nClicking on the link labeled \u201cQ181296\u201d takes us to the Wikidata entry for abelian group, which con-\ntains much useful information about abelian groups and their relationships to other kinds of algebraic\nobjects. In particular, it has a link to the Wikipedia page for \u201cAbelian group.\u201d Clicking on the link\nlabeled \u201cabelian group\u201d under the header \u201cChicago\u201d takes us to a definition of abelian group, hosted\non the MathGloss website. This page contains links to other relevant definitions, also on MathGloss.\nThere is currently no Lean 4 link for abelian group in the provided list of undergraduate concepts, but\n10https://thosgood.com/maths-dictionary/\n11https://ncatlab.org/nlab/show/HomePage\n12http://www.tac.mta.ca/tac/\n4\n if there were, clicking on it would take us to the entry in the Lean 4 documentation defining abelian\ngroups. Since \u201cabelian group\u201d appears under the MuLiMa heading, going to the MuLiMa website will\nallow us to see its translation into several languages. Finally, if we click on the link under the nLab\nheading, we will see the nLab page for \u201cabelian group,\u201d which takes on a distinctly categorical point\nof view.\nThe lack of a link to an instance of \u201cabelian group\u201d in Lean 4 highlights the need to collect\ninformation from multiple resources. Certainly one can talk about abelian groups in Lean, but the list\nof undergraduate concepts we used just happens not to link there.\n5\nTools for NLP\nAt another stage in the project, we collected terms from the abstracts of articles published in Theory\nand Applications of Categories (TAC) using natural language processing (TAC). We did not include\nit in this iteration of MathGloss because as a corpus of research mathematics, it does not fit our goal\nof organizing undergraduate math. However, we hope to apply this technique to other corpora of\nundergraduate mathematics in the future. We describe the technique below.\nTo extract terms from TAC, we used the Python library spaCy13 to perform grammatical analysis\non the text of the 755 abstracts from the articles in TAC.14 SpaCy performs syntactic parsing of\nsentences using the Universal Dependencies (UD)[NdMG+16] framework. UD is an open-source project\nthat works towards standardizing grammatical annotation to make linguistics research more consistent.\nThe output of this analysis is in a UD-developed format called CoNLL-U,15 which we then inspect\nusing a script from UD.\nA CoNLL-U file is a plaintext file that displays sentence analysis in a particular structure. It allows\ncomment lines, which are indicated by \u201c#\u201d, and in the CoNLL-U files we created, the comments include\nthe text of the sentence, the number of its entry in the corpus, and the length (in \u201ctokens\u201d) of the\nsentence. Figure 3 shows an example of a CoNLL-U file containing a sentence analyzed in this way\nafter application of the \u201cdetextor\u201d pipeline component described below. Each word in the sentence is\nwritten on its own line following the text of the sentence along with information about the word, for\nexample its part of speech.\nFigure 3: Analysis of the definition of \u201cabelian group\u201d in CoNLL-U format\nAt its simplest, spaCy takes in a section of text (called a Doc, short for \u201cdocument\u201d), and then\nuses its models to split the text into sentences, split sentences into tokens, and then assigns parts\nof speech and assigns Universal Dependency relations to each token.\nA token can be thought of\nas a generalization of a word: it can be a punctuation mark, or especially in our case, a piece of\nmathematical notation. The process of splitting a Doc into sentences is called \u201csentencization,\u201d and\nthe process of splitting a sentence into tokens is called \u201ctokenization.\u201d We used the smallest model\nprovided by spaCy, called \u201cen core web sm,\u201d because of our limitations in computing power. Using\nlarger models made no difference in output, but took more time and resources to generate that output.\n13https://spacy.io/\n14https://github.com/ToposInstitute/tac-corpus\n15https://universaldependencies.org/format.html\n5\n It is possible to run spaCy on LATEX code, but without making specific modifications for mathemat-\nical notation, the results are very poor due to incorrect sentencization (including both sentences that\nare too long and sentences that are too short) and the over-tokenization of the sentence. LATEX code\nconsists of many punctuation marks and commands that often resemble English words, but none of\nthese things should be parsed as normal English. Without modifications to the default spaCy pipeline,\neach piece of the code that represents a mathematical expression is fragmented and treated separately.\nOur goal is to extract definitions and concepts from texts written in mathematical English, and LATEX\nis an integral part of that.\nFirst-year mathematics students are taught that mathematical expressions should always be situ-\nated within a grammatically correct sentence. They should never stand on their own, and the statement\n\u201cx = y\u201d should be read \u201cx equals y,\u201d and can therefore be considered an independent clause. Within\nabstracts and definitions, one does not usually make declarative statements like \u201cx = y,\u201d so most of\nthe LATEX we encounter in our corpora should be thought of as nouns or as names of instances of\nmathematical objects. Because code itself is not \u201cnatural language,\u201d one way to analyze it is to take\neach piece of LATEX code to represent a single \u201cword.\u201d\nThis makes up our main approach to solving the tokenization problem. It is as simple as telling\nspaCy to treat everything in between two dollar signs as one token. The implementation of this pipeline\ncomponent is done using the \u201cretokenizer\u201d context manager and requires that dollar signs (and for\nbest results, hyphens) be padded with spaces. This new pipeline component, called \u201cdetextor,\u201d is\nimplemented after the \u201ctagger\u201d component, which assigns parts of speech to tokens. The detextor\npipeline component represents only a partial solution to the problem of annotating LATEX code as it\nstruggles to capture the actual information conveyed by formulas. However, it greatly improves the\naccuracy of sentencization, which is crucial for annotating regular English words with their parts of\nspeech.\nThe part-of-speech annotation forms the foundation of our term extraction. We are able to pick\nout what we expect to be mathematical terms or concepts with a simple heuristic. From the CoNLL-\nU files, we are able to compile lists of \u201clemmas,\u201d or basic forms of words (e.g. \u201cbe\u201d for the word\n\u201cis,\u201d or \u201cgroup\u201d for the word \u201cgroups\u201d) according to their part of speech. We suppose that the most\nfrequently occurring nouns, adjective-noun phrases, and \u201ccompounds\u201d are math terms. \u201cCompound\u201d\nis a UD annotation for certain multi-word phrases that represent a single thing16. Adjective-noun\nphrases consist of any number of adjectives followed by a noun or by a compound. The upper ends of\nthe frequency tables for these types of terms are almost exclusively populated by math concepts, but\nwe do not yet know if the lists exclude some concepts.\nThis process is what we hope to use on other bodies of mathematical text when they do not\nthemselves provide ready-made lists of terms. Unfortunately, it does not produce any kind of link\nto an explanation or definition of a given term. In the future we hope to be able to pinpoint the\ndefinitions of these extracted terms and give pointers to their locations in text. In other words, we\nwant to perform entity linking on math text.\n6\nFuture Work\nFirst, we hope to develop a method to more easily collect and map terms from different math resources.\nIn particular, we want to include more theorem provers in our mappings. Presently we are looking\ninto adding links to Agda, Coq, and Isabelle. At the Dagstuhl seminar on automated mathematics,17\nthere was discussion on how best to find and compile instances of undergraduate math concepts from\nthese provers. Hopefully they will follow the example of Lean and produce such resources.\nOn the natural language side of collecting more terms from more resources, we want to use machine\nlearning techniques to extract definitions of the terms we already know how to find. Some work has\nbeen done in this direction, even specifically tailored to mathematical text [VLSR20], but results do\nnot look impressive [CdPS23].\nAnother future project is to come up with a way to verify that the mappings to Wikidata are indeed\ncorrect. The addition of subject parentheticals definitely reduces the number of disambiguation pages\noutput by Wikimapper, but some still slip through the cracks. We could potentially do this by verifying\n16https://universaldependencies.org/u/dep/compound.html\n17https://www.dagstuhl.de/23401\n6\n that the terms we map to a Wikidata item have the same relations to other Wikidata items\u2014this relies\non the linking that already exists within corpora like Chicago Notes.\nCurrently, the process for performing mappings is labor-intensive even though it is automated\nto some degree. We hope to further automate the process to take advantage of the fluid nature of\nWikidata, which is always adding more terms. Relatedly, there are some basic improvements that need\nto be made to the GitHub Pages website. As we add new resources, we want users to be able to select\nwhich ones they see on the webpage at any time and for there to be a better search function than\npressing Ctrl+F. Moreover, the website needs some support for LATEX in order to properly display the\ncontent in Chicago Notes that is also hosted there.\n7\nConclusion\nMathGloss aims to help people from different backgrounds make sense of the diverse resources for\nmathematics available online through organization by individual concept. Should one encyclopedia\u2019s\narticle for a particular construction be inscrutable, we would like it to be easy to find another article\nthat is more closely aligned with the reader\u2019s background and therefore easier to understand. Moreover,\nMathGloss represents a step in the direction of bridging the gap between natural math as done by\nhumans and formal math. By creating a knowledge graph of undergraduate mathematics, we hope\nto empower students, mathematicians, and those who use mathematics in their work to both better\nnavigate the intricate web of definitions and theorems and to embrace the use of formal systems.\nReferences\n[CdPFS22]\nJacob Collard, Valeria de Paiva, Brendan Fong, and Eswaran Subrahmanian. Extracting\nmathematical concepts from text, 2022.\n[CdPS23]\nJacob Collard, Valeria de Paiva, and Eswaran Subrahmanian. Parmesan: mathematical\nconcept extraction for education, 2023.\n[Har20]\nKevin Hartnett. Building the mathematical library of the future, October 2020.\n[Har21]\nKevin Hartnett. Proof assistant makes jump to big league math, July 2021.\n[KEdCG20] Jan-Christoph Klie, Richard Eckart de Castilho, and Iryna Gurevych.\nFrom Zero to\nHero: Human-In-The-Loop Entity Linking in Low Resource Domains. In Dan Jurafsky,\nJoyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 6982\u20136993, Online, July\n2020. Association for Computational Linguistics.\n[NdMG+16] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Haji\u02c7c,\nChristopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira,\nReut Tsarfaty, and Daniel Zeman. Universal Dependencies v1: A multilingual treebank\ncollection. In Proceedings of the Tenth International Conference on Language Resources\nand Evaluation (LREC\u201916), pages 1659\u20131666, Portoro\u02c7z, Slovenia, May 2016. European\nLanguage Resources Association (ELRA).\n[Rob23]\nSiobhan Roberts. A.I. is coming for mathematics, too, July 2023.\n[VK14]\nDenny Vrande\u02c7ci\u00b4c and Markus Kr\u00a8otzsch. Wikidata: A free collaborative knowledgebase.\nCommun. ACM, 57(10):78\u201385, Sep 2014.\n[VLSR20]\nNatalia Vanetik, Marina Litvak, Sergey Shevchuk, and Lior Reznik. Automated discov-\nery of mathematical definitions in text. In Nicoletta Calzolari, Fr\u00b4ed\u00b4eric B\u00b4echet, Philippe\nBlache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara,\nBente Maegaard, Joseph Mariani, H\u00b4el`ene Mazo, Asuncion Moreno, Jan Odijk, and Stelios\nPiperidis, editors, Proceedings of the Twelfth Language Resources and Evaluation Con-\nference, pages 2086\u20132094, Marseille, France, May 2020. European Language Resources\nAssociation.\n7\n"}}